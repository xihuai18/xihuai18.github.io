<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://xihuai18.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xihuai18.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-12-21T00:10:55+00:00</updated><id>https://xihuai18.github.io/feed.xml</id><title type="html">Xihuai Wang’s Page</title><subtitle>Xihuai&apos;s personal page.
</subtitle><entry xml:lang="en"><title type="html">Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html" rel="alternate" type="text/html" title="Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees" /><published>2025-12-17T00:00:00+00:00</published><updated>2025-12-17T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#introduction-why-should-we-care-about-off-policy" id="markdown-toc-introduction-why-should-we-care-about-off-policy">Introduction: Why Should We Care About “Off-Policy”?</a></li>
  <li><a href="#part-i-theoretical-foundations" id="markdown-toc-part-i-theoretical-foundations">Part I: Theoretical Foundations</a>    <ul>
      <li><a href="#11-basic-setup" id="markdown-toc-11-basic-setup">1.1 Basic Setup</a></li>
      <li><a href="#12-core-tool-policy-performance-difference-lemma" id="markdown-toc-12-core-tool-policy-performance-difference-lemma">1.2 Core Tool: Policy Performance Difference Lemma</a></li>
    </ul>
  </li>
  <li><a href="#part-ii-performance-improvement-bounds-for-single-policy-sampling" id="markdown-toc-part-ii-performance-improvement-bounds-for-single-policy-sampling">Part II: Performance Improvement Bounds for Single-Policy Sampling</a>    <ul>
      <li><a href="#21-the-distribution-mismatch-problem" id="markdown-toc-21-the-distribution-mismatch-problem">2.1 The Distribution Mismatch Problem</a></li>
      <li><a href="#22-controlling-state-distribution-differences" id="markdown-toc-22-controlling-state-distribution-differences">2.2 Controlling State Distribution Differences</a></li>
      <li><a href="#23-policy-performance-improvement-lower-bound" id="markdown-toc-23-policy-performance-improvement-lower-bound">2.3 Policy Performance Improvement Lower Bound</a></li>
    </ul>
  </li>
  <li><a href="#part-iii-multi-policy-static-mixture-sampling" id="markdown-toc-part-iii-multi-policy-static-mixture-sampling">Part III: Multi-Policy Static Mixture Sampling</a>    <ul>
      <li><a href="#31-practical-scenarios" id="markdown-toc-31-practical-scenarios">3.1 Practical Scenarios</a></li>
      <li><a href="#32-core-idea-augmented-state-space" id="markdown-toc-32-core-idea-augmented-state-space">3.2 Core Idea: Augmented State Space</a></li>
      <li><a href="#33-structural-simplification-for-trajectory-level-mixture" id="markdown-toc-33-structural-simplification-for-trajectory-level-mixture">3.3 Structural Simplification for Trajectory-Level Mixture</a></li>
      <li><a href="#34-performance-improvement-lower-bound-for-trajectory-level-mixture" id="markdown-toc-34-performance-improvement-lower-bound-for-trajectory-level-mixture">3.4 Performance Improvement Lower Bound for Trajectory-Level Mixture</a></li>
    </ul>
  </li>
  <li><a href="#part-iv-dynamic-mixture-sampling-and-monotonic-improvement-conditions" id="markdown-toc-part-iv-dynamic-mixture-sampling-and-monotonic-improvement-conditions">Part IV: Dynamic Mixture Sampling and Monotonic Improvement Conditions</a>    <ul>
      <li><a href="#41-the-core-challenge" id="markdown-toc-41-the-core-challenge">4.1 The Core Challenge</a></li>
      <li><a href="#42-unified-modeling-framework" id="markdown-toc-42-unified-modeling-framework">4.2 Unified Modeling Framework</a></li>
      <li><a href="#43-core-decomposition" id="markdown-toc-43-core-decomposition">4.3 Core Decomposition</a></li>
      <li><a href="#44-monotonic-improvement-lower-bound" id="markdown-toc-44-monotonic-improvement-lower-bound">4.4 Monotonic Improvement Lower Bound</a></li>
      <li><a href="#45-infeasibility-of-direct-constraints" id="markdown-toc-45-infeasibility-of-direct-constraints">4.5 Infeasibility of Direct Constraints</a></li>
      <li><a href="#46-triangle-inequality-decomposition" id="markdown-toc-46-triangle-inequality-decomposition">4.6 Triangle Inequality Decomposition</a></li>
    </ul>
  </li>
  <li><a href="#part-v-theoretical-foundations-of-clipping-mechanisms" id="markdown-toc-part-v-theoretical-foundations-of-clipping-mechanisms">Part V: Theoretical Foundations of Clipping Mechanisms</a>    <ul>
      <li><a href="#51-from-tv-distance-to-computable-quantities" id="markdown-toc-51-from-tv-distance-to-computable-quantities">5.1 From TV Distance to Computable Quantities</a></li>
      <li><a href="#52-sample-representation-of-u_k" id="markdown-toc-52-sample-representation-of-u_k">5.2 Sample Representation of $U_k$</a></li>
      <li><a href="#53-two-methods-for-constraining-u_k" id="markdown-toc-53-two-methods-for-constraining-u_k">5.3 Two Methods for Constraining $U_k$</a></li>
      <li><a href="#54-complete-objective-functions-for-three-clipping-mechanisms" id="markdown-toc-54-complete-objective-functions-for-three-clipping-mechanisms">5.4 Complete Objective Functions for Three Clipping Mechanisms</a></li>
      <li><a href="#55-comparison-of-three-methods" id="markdown-toc-55-comparison-of-three-methods">5.5 Comparison of Three Methods</a></li>
      <li><a href="#56-controlling-sampling-staleness" id="markdown-toc-56-controlling-sampling-staleness">5.6 Controlling Sampling Staleness</a></li>
      <li><a href="#57-operational-meaning-of-clipping" id="markdown-toc-57-operational-meaning-of-clipping">5.7 Operational Meaning of Clipping</a></li>
      <li><a href="#58-section-summary" id="markdown-toc-58-section-summary">5.8 Section Summary</a></li>
    </ul>
  </li>
  <li><a href="#part-vi-comparison-of-trajectory-level-and-stepsegment-level-mixture" id="markdown-toc-part-vi-comparison-of-trajectory-level-and-stepsegment-level-mixture">Part VI: Comparison of Trajectory-Level and Step/Segment-Level Mixture</a>    <ul>
      <li><a href="#61-core-differences-between-the-two-mechanisms" id="markdown-toc-61-core-differences-between-the-two-mechanisms">6.1 Core Differences Between the Two Mechanisms</a></li>
      <li><a href="#62-differences-in-sampling-staleness-s_k" id="markdown-toc-62-differences-in-sampling-staleness-s_k">6.2 Differences in Sampling Staleness $S_k$</a></li>
      <li><a href="#63-differences-in-surrogate-objective-estimation" id="markdown-toc-63-differences-in-surrogate-objective-estimation">6.3 Differences in Surrogate Objective Estimation</a></li>
      <li><a href="#64-variance-amplification-risk" id="markdown-toc-64-variance-amplification-risk">6.4 Variance Amplification Risk</a></li>
      <li><a href="#65-applicable-scenarios" id="markdown-toc-65-applicable-scenarios">6.5 Applicable Scenarios</a></li>
    </ul>
  </li>
  <li><a href="#part-vii-handling-training-inference-inconsistency" id="markdown-toc-part-vii-handling-training-inference-inconsistency">Part VII: Handling Training-Inference Inconsistency</a>    <ul>
      <li><a href="#71-background" id="markdown-toc-71-background">7.1 Background</a></li>
      <li><a href="#72-effective-staleness" id="markdown-toc-72-effective-staleness">7.2 Effective Staleness</a></li>
      <li><a href="#73-actionable-control" id="markdown-toc-73-actionable-control">7.3 Actionable Control</a></li>
    </ul>
  </li>
  <li><a href="#summary-practical-guidelines" id="markdown-toc-summary-practical-guidelines">Summary: Practical Guidelines</a>    <ul>
      <li><a href="#core-theoretical-framework" id="markdown-toc-core-theoretical-framework">Core Theoretical Framework</a></li>
      <li><a href="#separation-of-concerns-principle" id="markdown-toc-separation-of-concerns-principle">Separation of Concerns Principle</a></li>
      <li><a href="#clipping-method-selection" id="markdown-toc-clipping-method-selection">Clipping Method Selection</a></li>
      <li><a href="#handling-training-inference-inconsistency" id="markdown-toc-handling-training-inference-inconsistency">Handling Training-Inference Inconsistency</a></li>
    </ul>
  </li>
  <li><a href="#appendix-quick-reference-for-key-symbols" id="markdown-toc-appendix-quick-reference-for-key-symbols">Appendix: Quick Reference for Key Symbols</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<p><a href="/reinforcement-learning/2025/12/17/offpolicy-zh.html">中文版</a></p>

<h2 id="introduction-why-should-we-care-about-off-policy">Introduction: Why Should We Care About “Off-Policy”?</h2>

<p>Consider the following scenario: you are training a large language model with reinforcement learning to improve its question-answering capabilities. Ideally, each time the model generates a batch of responses, you would immediately update the model with this data, then use the updated model to generate new data, and so on. This approach of “updating with data from the same policy that generated it” is called <strong>on-policy</strong> training.</p>

<p>Reality, however, is not so simple. In large-scale distributed training, hundreds of GPUs generate data in parallel, while model updates take time. When a new model is deployed, much data generated by “older versions” of the model remains unused—discarding it seems wasteful, yet using it raises concerns about whether “stale data” might harm training effectiveness.</p>

<p>This is the core problem faced by <strong>off-policy</strong> training: <strong>Can we guarantee continued performance improvement when using data collected by older policies to update newer policies?</strong></p>

<p>This article systematically addresses this question. Starting from foundational theory, we progressively derive actionable conditions that specify when mixing data from multiple policy versions can still guarantee monotonic training improvement.</p>

<h2 id="part-i-theoretical-foundations">Part I: Theoretical Foundations</h2>

<h3 id="11-basic-setup">1.1 Basic Setup</h3>

<p>We consider a standard Markov Decision Process (MDP) comprising a state space $\mathcal{S}$, action space $\mathcal{A}$, transition probability $p(s’|s,a)$, reward function $r(s,a)$, initial distribution $\rho_0$, and discount factor $\gamma \in (0,1)$.</p>

<p>The <strong>expected cumulative discounted return</strong> of policy $\pi$ is:</p>

\[J(\pi) := \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid \pi\right]\]

<p>The <strong>discounted state visitation distribution</strong> represents the weighted frequency of visiting each state during long-term policy execution:</p>

\[d_\pi(s) := (1-\gamma) \sum_{t=0}^{\infty} \gamma^t \Pr(s_t = s \mid \pi)\]

<p>The <strong>advantage function</strong> measures how much better action $a$ is compared to the policy’s average:</p>

\[A^\pi(s,a) := Q^\pi(s,a) - V^\pi(s)\]

<p>The <strong>total variation distance</strong> (TV distance) measures the difference between two policies’ action distributions at state $s$:</p>

\[D_{\mathrm{TV}}(\pi, \pi'; s) := \frac{1}{2} \sum_{a \in \mathcal{A}} |\pi(a \mid s) - \pi'(a \mid s)|\]

<h3 id="12-core-tool-policy-performance-difference-lemma">1.2 Core Tool: Policy Performance Difference Lemma</h3>

<p>The cornerstone of the entire theory is this elegant result:</p>

<blockquote>
  <p><strong>Lemma 1.1 (Policy Performance Difference Lemma)</strong></p>

  <p>For any policies $\pi_k$ (old) and $\pi$ (new), the performance difference can be expressed as:</p>

\[J(\pi) - J(\pi_k) = \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_\pi}\left[ \mathbb{E}_{a \sim \pi(\cdot \mid s)}[A^{\pi_k}(s,a)] \right]\]
</blockquote>

<p><strong>Intuitive understanding</strong>: How much better the new policy is than the old equals the “average advantage” obtained by selecting actions according to the new policy under the state distribution visited by the new policy.</p>

<h2 id="part-ii-performance-improvement-bounds-for-single-policy-sampling">Part II: Performance Improvement Bounds for Single-Policy Sampling</h2>

<h3 id="21-the-distribution-mismatch-problem">2.1 The Distribution Mismatch Problem</h3>

<p>The Policy Performance Difference Lemma has a practical issue: the expectation on the right-hand side is computed under $d_\pi$ (the new policy’s state distribution), while we can only sample from $d_{\pi_k}$ (the old policy).</p>

<p>The solution is to decompose the expectation into “expectation under the old distribution + bias term,” then control the bias. The key question is: <strong>What is the quantitative relationship between the difference in state distributions and the difference in policies?</strong></p>

<h3 id="22-controlling-state-distribution-differences">2.2 Controlling State Distribution Differences</h3>

<blockquote>
  <p><strong>Lemma 1.2 (Relationship Between State Distribution Difference and Policy TV Distance)</strong></p>

\[\|d_\pi - d_{\pi_k}\|_1 \leq \frac{2\gamma}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]\]
</blockquote>

<p><strong>Physical interpretation</strong>: Small differences in policies in action space are “amplified” through environment dynamics into differences in state visitation distributions. The coefficient $\frac{\gamma}{1-\gamma}$ reflects the <strong>temporal accumulation effect</strong>—in long-horizon tasks ($\gamma$ close to 1), the amplification is stronger.</p>

<p><strong>Proof sketch</strong>: By deriving the fixed-point equation for discounted visitation distributions and exploiting the $\ell_1$ non-expansiveness of stochastic matrices, one can show that state distribution differences are amplified by policy differences through transition dynamics, with the amplification factor being precisely $\frac{\gamma}{1-\gamma}$.</p>

<h3 id="23-policy-performance-improvement-lower-bound">2.3 Policy Performance Improvement Lower Bound</h3>

<blockquote>
  <p><strong>Theorem 1.1 (Policy Performance Improvement Lower Bound)</strong></p>

  <p>Define the expected advantage upper bound constant $C_{\pi,\pi_k} := \max_{s} \lvert \mathbb{E}_{a \sim \pi}[A^{\pi_k}(s,a)] \rvert$. Then:</p>

\[J(\pi) - J(\pi_k) \geq L_{\pi_k}(\pi) - \frac{2\gamma C_{\pi,\pi_k}}{(1-\gamma)^2} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]\]

  <p>where the <strong>surrogate objective</strong> is:</p>

\[L_{\pi_k}(\pi) := \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}, a \sim \pi_k} \left[ \frac{\pi(a \mid s)}{\pi_k(a \mid s)} A^{\pi_k}(s,a) \right]\]
</blockquote>

<p>This lower bound consists of two parts:</p>

<ol>
  <li>
    <p><strong>Surrogate objective</strong> $L_{\pi_k}(\pi)$: Can be directly estimated from old policy data via importance sampling; this is the optimization objective of TRPO/PPO.</p>
  </li>
  <li>
    <p><strong>Policy shift penalty</strong>: Increases with the TV distance between new and old policies, explaining why PPO needs to constrain update magnitude.</p>
  </li>
</ol>

<p><strong>Core conclusion</strong>: Maximizing the surrogate objective while controlling policy shift guarantees performance improvement.</p>

<h2 id="part-iii-multi-policy-static-mixture-sampling">Part III: Multi-Policy Static Mixture Sampling</h2>

<h3 id="31-practical-scenarios">3.1 Practical Scenarios</h3>

<p>In practice, a batch of data may come from multiple policy versions ${\pi^{(1)}, \ldots, \pi^{(M)}}$, with respective proportions $\alpha_1, \ldots, \alpha_M$. How do we extend Theorem 1.1 to this setting?</p>

<h3 id="32-core-idea-augmented-state-space">3.2 Core Idea: Augmented State Space</h3>

<p>The solution is an elegant modeling technique: <strong>treat the policy version index as part of the state</strong>.</p>

<p>Define the augmented state space $\tilde{\mathcal{S}} := \mathcal{S} \times \mathcal{I}$, where $\mathcal{I} = {1, \ldots, M}$ is the policy index set. Under augmented state $(s, i)$, the <strong>mixture behavior policy</strong> is defined as $\beta(a \mid s, i) := \pi^{(i)}(a \mid s)$.</p>

<p>The evolution of indices is characterized by the <strong>index transition kernel</strong> $q(i’ \mid i)$. The augmented MDP inherits the original MDP’s rewards and environment transitions, with indices evolving independently according to $q(i’|i)$.</p>

<p>This technique works because the new policy $\pi$’s return in the augmented MDP equals its return in the original MDP, allowing direct application of Theorem 1.1.</p>

<h3 id="33-structural-simplification-for-trajectory-level-mixture">3.3 Structural Simplification for Trajectory-Level Mixture</h3>

<p>The most common scenario is <strong>using a single old policy per trajectory</strong>: at trajectory start, sample index $I_0 \sim \alpha$, and use $\pi^{(I_0)}$ throughout. In this case, the index transition kernel is the identity: $q(i’ \mid i) = \mathbf{1}_{i’=i}$.</p>

<p>From an engineering perspective, in many <strong>actor-learner asynchronous training</strong> setups (when sampling and training organize data by “entire trajectories/complete episodes belonging to a certain policy version”), this approximately corresponds to what we call <strong>trajectory-level mixture</strong>: actors use a fixed policy snapshot within a sampling unit to generate data, while learners mix trajectories from different versions for updates. We say “approximately” because different systems may not have identical boundaries for “trajectory/sampling unit.”</p>

<blockquote>
  <p><strong>Lemma 2.1 (Structural Simplification for Trajectory-Level Mixture)</strong></p>

  <p>(a) The augmented state visitation distribution decomposes as: $d_{\beta}(s, i) = \alpha_i \cdot d_{\pi^{(i)}}(s)$</p>

  <p>(b) The advantage function reduces to: $A^{\beta}((s, i), a) = A^{\pi^{(i)}}(s, a)$</p>
</blockquote>

<p><strong>Intuition for (b)</strong>: Since the index never changes, <strong>all future trajectories</strong> starting from augmented state $(s,i)$ are generated by the same policy $\pi^{(i)}$. Therefore, future cumulative returns are entirely determined by $\pi^{(i)}$, and value functions and advantage functions naturally reduce to their $\pi^{(i)}$ counterparts.</p>

<p>Consequently, the mixture policy’s return is the weighted average of individual old policies’ returns: $J_{\mathrm{mix}} = \sum_{i=1}^{M} \alpha_i J(\pi^{(i)})$.</p>

<h3 id="34-performance-improvement-lower-bound-for-trajectory-level-mixture">3.4 Performance Improvement Lower Bound for Trajectory-Level Mixture</h3>

<blockquote>
  <p><strong>Corollary 2.1 (Performance Improvement Lower Bound for Trajectory-Level Mixture)</strong></p>

\[J(\pi) - \sum_{i=1}^{M} \alpha_i J(\pi^{(i)}) \geq \sum_{i=1}^{M} \alpha_i L_{\pi^{(i)}}(\pi) - \frac{2\gamma \max_i C_{\pi, \pi^{(i)}}}{(1-\gamma)^2} \sum_{i=1}^{M} \alpha_i \mathbb{E}_{s \sim d_{\pi^{(i)}}} \big[ D_{\mathrm{TV}}(\pi, \pi^{(i)}; s) \big]\]
</blockquote>

<p>This result shows that when mixing trajectories from multiple old policy versions for training, if we construct the loss using importance ratios corresponding to each trajectory’s source policy while controlling the new policy’s deviation from each old policy, the new policy’s performance has a clear improvement lower bound.</p>

<h2 id="part-iv-dynamic-mixture-sampling-and-monotonic-improvement-conditions">Part IV: Dynamic Mixture Sampling and Monotonic Improvement Conditions</h2>

<h3 id="41-the-core-challenge">4.1 The Core Challenge</h3>

<p>Part III discussed <strong>static mixture</strong>—where mixture weights $\alpha_i$ remain fixed. This section considers the more general <strong>dynamic mixture</strong>—where sampling gradually transitions to the new policy after it is released.</p>

<p>The previous results characterize improvement of “the new policy relative to the mixture behavior policy.” However, in actual training, what we truly care about is: <strong>Does the latest policy $\pi_{k+1}$ after each update monotonically improve over the previous latest policy $\pi_k$?</strong></p>

\[J(\pi_{k+1}) \geq J(\pi_k)\]

<h3 id="42-unified-modeling-framework">4.2 Unified Modeling Framework</h3>

<p>Two typical forms of dynamic mixture sampling can be uniformly characterized by the index transition kernel $q(i’|i)$:</p>

<p><strong>Trajectory-level mixture</strong> (can be viewed as an abstraction of conventional asynchronous training; identity index transition): $q(i’|i) = \mathbf{1}{i’=i}$</p>

<p><strong>Step/segment-level mixture</strong> (an abstraction of partial rollout / segment-based sampling; allows switching): $q(i’|i) = (1-\sigma(i))\mathbf{1}{i’=i} + \sigma(i)\kappa(i’|i)$</p>

<p>where $\sigma(i)$ is the switching probability and $\kappa(\cdot|i)$ is the target index distribution.</p>

<h3 id="43-core-decomposition">4.3 Core Decomposition</h3>

<p>By introducing the mixture return $J_{\mathrm{mix}}^{(k)}$ as an intermediate bridge, the performance difference decomposes as:</p>

\[J(\pi_{k+1}) - J(\pi_k) = \underbrace{[J(\pi_{k+1}) - J_{\mathrm{mix}}^{(k)}]}_{\text{improvement over mixture policy}} + \underbrace{[J_{\mathrm{mix}}^{(k)} - J(\pi_k)]}_{\text{mixture bias term}}\]

<p>The first term can be handled using Theorem 1.1. The second term is the <strong>mixture bias term</strong>, which can be shown to satisfy:</p>

\[J_{\mathrm{mix}}^{(k)} - J(\pi_k) \geq -\frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]\]

<h3 id="44-monotonic-improvement-lower-bound">4.4 Monotonic Improvement Lower Bound</h3>

<p>Combining the above results yields the core theorem:</p>

<blockquote>
  <p><strong>Theorem 3.1 (Monotonic Improvement Lower Bound Under Dynamic Mixture Sampling)</strong></p>

\[\begin{aligned}
J(\pi_{k+1}) - J(\pi_k) \geq\;&amp; L_{\beta^{(k)}}(\pi_{k+1}) \\
&amp;- \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \big] \\
&amp;- \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
\end{aligned}\]
</blockquote>

<p>This lower bound reveals the necessity of <strong>dual control</strong>:</p>
<ul>
  <li><strong>Update shift penalty</strong>: Deviation of the new policy $\pi_{k+1}$ from the sampling source policy $\pi^{(i)}$</li>
  <li><strong>Sampling staleness penalty</strong>: Staleness of the sampling source policy $\pi^{(i)}$ relative to the current policy $\pi_k$</li>
</ul>

<h3 id="45-infeasibility-of-direct-constraints">4.5 Infeasibility of Direct Constraints</h3>

<p>The update shift penalty term in Theorem 3.1 might appear controllable by constraining $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s)$, but this is actually <strong>infeasible</strong>:</p>

<blockquote>
  <p><strong>Observation 3.1 (Infeasibility of Update Shift Constraints)</strong></p>

  <p>Suppose the mixture sampling includes two old policies $\pi^{(1)}$ and $\pi^{(2)}$. If there exists some state $s$ such that $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) &gt; 2\delta$, then no policy $\pi_{k+1}$ can simultaneously satisfy $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(1)}; s) \leq \delta$ and $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(2)}; s) \leq \delta$.</p>
</blockquote>

<p><strong>Proof</strong>: By the triangle inequality, if both constraints were satisfied, then $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) \leq 2\delta$, a contradiction.</p>

<p><strong>Root cause</strong>: The update shift penalty directly couples $\pi_{k+1}$ with the historical policy family ${\pi^{(i)}}$, whose internal structure is a product of historical training and not controllable by the current update.</p>

<h3 id="46-triangle-inequality-decomposition">4.6 Triangle Inequality Decomposition</h3>

<p>The solution leverages the triangle inequality of TV distance:</p>

\[D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \leq D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s) + D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\]

<p>This decomposes the coupled constraint into two independent parts:</p>

<ul>
  <li><strong>Update increment shift</strong> $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)$: Deviation of the new policy from the current policy, <strong>controllable by the optimization side</strong></li>
  <li><strong>Sampling staleness</strong> $D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)$: Deviation of the current policy from each old policy, <strong>must be controlled by the sampling side</strong></li>
</ul>

<p>Define:</p>

\[U_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)\big], \quad S_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\big]\]

<blockquote>
  <p><strong>Corollary 3.2 (Decomposed Monotonic Improvement Lower Bound)</strong></p>

\[J(\pi_{k+1}) - J(\pi_k) \geq L_{\beta^{(k)}}(\pi_{k+1}) - \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} U_k - \left( \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} + \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \right) S_k\]
</blockquote>

<p><strong>Why does decomposition solve the problem?</strong> The key is that after decomposition, $U_k$ only involves the new policy $\pi_{k+1}$ and the current policy $\pi_k$, <strong>completely independent of the structure of the old policy family ${\pi^{(i)}}$</strong>. Therefore, regardless of how different the old policies are from each other, constraining $U_k$ is always feasible—this is precisely the resolution to the infeasibility issue revealed in Observation 3.1.</p>

<p>This reveals an important engineering principle—<strong>separation of concerns</strong>:</p>

<div>
<table>
<thead>
<tr><th>Control Term</th><th>Responsible Party</th><th>Control Mechanism</th></tr>
</thead>
<tbody>
<tr><td>$U_k$ (update increment shift)</td><td>Optimization algorithm</td><td>Policy clipping</td></tr>
<tr><td>$S_k$ (sampling staleness)</td><td>Sampling system</td><td>Data filtering, version window</td></tr>
</tbody>
</table>
</div>

<h2 id="part-v-theoretical-foundations-of-clipping-mechanisms">Part V: Theoretical Foundations of Clipping Mechanisms</h2>

<h3 id="51-from-tv-distance-to-computable-quantities">5.1 From TV Distance to Computable Quantities</h3>

<p>Corollary 3.2 tells us that to guarantee monotonic improvement, we need to control the update increment shift $U_k = \mathbb{E}[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)]$. However, TV distance is a distribution-level quantity—how can we control it using samples?</p>

<p>The key bridge is the following identity:</p>

<blockquote>
  <p><strong>Lemma 3.3 (Ratio Difference Representation of TV Distance)</strong></p>

  <p>Suppose policy $\pi_1$’s support covers the supports of $\pi$ and $\pi_2$. Then for any state distribution $\mu$:</p>

\[\mathbb{E}_{s\sim \mu} \big[D_{\mathrm{TV}}(\pi, \pi_2; s)\big] = \frac{1}{2} \mathbb{E}_{s\sim \mu, a\sim\pi_1(\cdot|s)} \left| \frac{\pi(a|s)}{\pi_1(a|s)} - \frac{\pi_2(a|s)}{\pi_1(a|s)} \right|\]
</blockquote>

<p><strong>Intuitive understanding</strong>: The left side is the TV distance between two distributions (requiring enumeration over all actions), while the right side is the absolute difference of two importance ratios when sampling under $\pi_1$. This enables us to estimate and control TV distance using samples.</p>

<h3 id="52-sample-representation-of-u_k">5.2 Sample Representation of $U_k$</h3>

<p>Using Lemma 3.3, setting $\pi = \pi_{k+1}$, $\pi_2 = \pi_k$, $\pi_1 = \pi^{(i)}$ (the sampling source policy), we obtain:</p>

\[U_k = \frac{1}{2} \mathbb{E}_{(s,i) \sim d_{\beta^{(k)}}, a \sim \pi^{(i)}(\cdot|s)} \left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right|\]

<p>Denoting $\rho_{k+1} := \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)}$ and $\rho_k := \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)}$, we have:</p>

\[U_k = \frac{1}{2} \mathbb{E}_{(s,i,a) \sim \text{training data}} \big| \rho_{k+1} - \rho_k \big|\]

<p>This means: <strong>If we can ensure $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$ for each sample, we can guarantee $U_k \leq \epsilon/2$</strong>.</p>

<h3 id="53-two-methods-for-constraining-u_k">5.3 Two Methods for Constraining $U_k$</h3>

<p><strong>Method 1: Direct Constraint on Ratio Difference</strong></p>

<p>For each sample $(s, i, a)$, require:</p>

\[\left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right| \leq \epsilon\]

<p>The clipping interval is $\left[\frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} - \epsilon, \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} + \epsilon\right]$, with <strong>clipping center at $\rho_k$ rather than 1</strong>.</p>

<p><strong>Method 2: Constraint on Incremental Ratio</strong></p>

<p>Noting that $\rho_{k+1} - \rho_k = \rho_k \cdot \left(\frac{\pi_{k+1}}{\pi_k} - 1\right)$, we have:</p>

\[|\rho_{k+1} - \rho_k| = \rho_k \cdot \left|\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right|\]

<p>If we constrain $\left\lvert\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right\rvert \leq \epsilon$, since $\mathbb{E}_{a\sim\pi^{(i)}}[\rho_k] = 1$, one can show $U_k \leq \epsilon/2$.</p>

<p>This method clips $\pi_{k+1}/\pi_k$ with center at 1, <strong>completely independent of the old policy $\pi^{(i)}$</strong>.</p>

<h3 id="54-complete-objective-functions-for-three-clipping-mechanisms">5.4 Complete Objective Functions for Three Clipping Mechanisms</h3>

<p>For comparison, we present the complete objective functions for three clipping mechanisms. Suppose the current sample comes from old policy $\pi^{(i)}$, and denote:</p>
<ul>
  <li>$\rho_{k+1} = \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)}$ (new policy’s ratio relative to sampling policy)</li>
  <li>$\rho_k = \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)}$ (current policy’s ratio relative to sampling policy)</li>
  <li>$r = \frac{\pi_{k+1}(a|s)}{\pi_k(a|s)}$ (new policy’s incremental ratio relative to current policy)</li>
</ul>

<p><strong>Standard PPO</strong>: Clip $\rho_{k+1}$ with center at 1</p>

\[L^{\mathrm{PPO}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\pi^{(i)}}, \; \mathrm{clip}(\rho_{k+1}, 1-\epsilon, 1+\epsilon) \cdot A^{\pi^{(i)}} \right) \right]\]

<p><strong>Method 1</strong>: Clip $\rho_{k+1}$ with center at $\rho_k$</p>

\[L^{\mathrm{M1}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\beta^{(k)}}, \; \mathrm{clip}(\rho_{k+1}, \rho_k-\epsilon, \rho_k+\epsilon) \cdot A^{\beta^{(k)}} \right) \right]\]

<p><strong>Method 2</strong>: Clip incremental ratio $r$ with center at 1</p>

\[L^{\mathrm{M2}} = \mathbb{E} \left[ \min\left( r \cdot \hat{A}, \; \mathrm{clip}(r, 1-\epsilon, 1+\epsilon) \cdot \hat{A} \right) \right]\]

<p>where $\hat{A} = \rho_k \cdot A^{\beta^{(k)}}$ is the importance-weighted advantage estimate.</p>

<h3 id="55-comparison-of-three-methods">5.5 Comparison of Three Methods</h3>

<p><strong>Table 5.1　Comparison of Three Clipping Mechanisms</strong></p>

<div>
<table>
<thead>
<tr><th>Method</th><th>Clipped Variable</th><th>Clipping Center</th><th>Clipping Interval</th><th>Constrained TV Distance</th></tr>
</thead>
<tbody>
<tr><td>Standard PPO</td><td>$\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$</td><td>$1$</td><td>$[1-\epsilon, 1+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$</td></tr>
<tr><td>Method 1</td><td>$\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$</td><td>$\rho_k = \pi_k/\pi^{(i)}$</td><td>$[\rho_k-\epsilon, \rho_k+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$</td></tr>
<tr><td>Method 2</td><td>$r = \pi_{k+1}/\pi_k$</td><td>$1$</td><td>$[1-\epsilon, 1+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$</td></tr>
</tbody>
</table>
</div>

<p><strong>The Fundamental Problem with Standard PPO Under Multi-Policy Mixture</strong></p>

<p>Standard PPO constrains $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$, requiring the new policy to be simultaneously close to all sampling source policies. By Observation 3.1, when the old policies $\pi^{(1)}, \pi^{(2)}, \ldots$ differ significantly from each other, <strong>no $\pi_{k+1}$ can simultaneously satisfy all constraints</strong>. This causes the trust region intersection to shrink or even become empty, with updates being limited by the most stale policy.</p>

<p><strong>Common Advantages of Methods 1 and 2</strong></p>

<p>Both methods constrain $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$—the deviation of the new policy from the <strong>current policy</strong> (rather than the sampling policy). Since $\pi_k$ is uniquely determined, this constraint is consistent across all sample sources, completely avoiding the infeasibility problem.</p>

<p><strong>Method 1 vs Method 2</strong></p>

<div>
<table>
<thead>
<tr><th>Comparison Dimension</th><th>Method 1 (Adaptive Clipping)</th><th>Method 2 (Incremental Clipping)</th></tr>
</thead>
<tbody>
<tr><td>Stale samples ($\rho_k \gg 1$)</td><td>Automatically tightens constraints, more conservative</td><td>May produce large gradient variance</td></tr>
<tr><td>LLM large vocabulary low-probability tokens</td><td>Allows larger absolute changes (additive)</td><td>Absolute changes are limited (multiplicative)</td></tr>
<tr><td>Implementation complexity</td><td>Requires storing $\pi^{(i)}(a|s)$ and $\pi_k(a|s)$</td><td>Only requires $\pi_k(a|s)$</td></tr>
<tr><td>Advantage function</td><td>Uses $A^{\beta^{(k)}}$</td><td>Uses weighted advantage $\rho_k \cdot A^{\beta^{(k)}}$</td></tr>
</tbody>
</table>
</div>

<p><strong>Detailed Explanations</strong>:</p>

<p><strong>(1) Handling Stale Samples</strong></p>

<p>When samples come from very old policies, $\rho_k = \pi_k/\pi^{(i)}$ can be large.</p>

<ul>
  <li>Method 2’s integrand is $\rho_k \cdot \lvert r - 1\rvert$; even if $\lvert r-1\rvert \leq \epsilon$, the integrand can reach $\epsilon \cdot \rho_k$, producing spikes.</li>
  <li>Method 1 directly constrains $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$; the integrand’s upper bound is always $\epsilon$, unaffected by $\rho_k$ amplification.</li>
</ul>

<p><strong>(2) LLM Large Vocabulary Issue</strong></p>

<p>Large language models have many tokens having very small probabilities.</p>

<ul>
  <li>Method 2 constrains $\pi_{k+1} \in [(1-\epsilon)\pi_k, (1+\epsilon)\pi_k]$, which is a <strong>multiplicative constraint</strong>: if $\pi_k(a|s) = 10^{-6}$, the allowed absolute change is only $\epsilon \times 10^{-6}$.</li>
  <li>Method 1 constrains $\lvert\pi_{k+1} - \pi_k\rvert \leq \epsilon \cdot \pi^{(i)}$, which is an <strong>additive constraint</strong>: if that token has higher probability under the old policy (e.g., $\pi^{(i)}(a|s) = 0.1$), even if the current probability is very low, faster improvement is allowed.</li>
</ul>

<h3 id="56-controlling-sampling-staleness">5.6 Controlling Sampling Staleness</h3>

<p>Corollary 3.2 shows that $S_k$ also affects the monotonic improvement lower bound, but it <strong>cannot be controlled through optimization-side clipping</strong> and must be implemented by the sampling system:</p>

<p><strong>(1) Discarding Stale Data</strong></p>

<p>Set a threshold $\epsilon_{\mathrm{stale}}$. For each sample, compute $\lvert\rho_k - 1\rvert = \lvert\pi_k(a|s)/\pi^{(i)}(a|s) - 1\rvert$, and discard samples exceeding the threshold.</p>

<p><strong>(2) Controlling Policy Version Window</strong></p>

<p>Limit the number of old policy versions in the mixture sampling, e.g., using only data from the most recent $W$ versions.</p>

<h3 id="57-operational-meaning-of-clipping">5.7 Operational Meaning of Clipping</h3>

<p>Finally, we clarify the relationship between clipping and the theoretical lower bound.</p>

<p>In Corollary 3.2, the coefficient of $U_k$, namely $C_{\pi_{k+1},\beta^{(k)}}$, depends on the new policy $\pi_{k+1}$, so the penalty term <strong>cannot be simply replaced by a constant</strong>. The correct operational meaning is:</p>

<blockquote>
  <p><strong>Maximize the surrogate objective $L_{\beta^{(k)}}(\pi_{k+1})$ subject to the constraint $U_k \leq \epsilon/2$</strong></p>
</blockquote>

<p>The clipping objective function is precisely an implementation of this constrained optimization—clipping <strong>hard limits</strong> the update magnitude to ensure $U_k$ is controllable; under this premise, gradient ascent improves the surrogate objective, thereby providing guarantees for monotonic policy improvement.</p>

<h3 id="58-section-summary">5.8 Section Summary</h3>

<p>This section established the theoretical foundations of clipping mechanisms:</p>

<ol>
  <li><strong>Lemma 3.3</strong> converts TV distance to sample-level ratio differences, serving as the bridge between theory and implementation</li>
  <li><strong>Two constraint methods</strong>: Method 1 (adaptive clipping center) and Method 2 (fixed incremental clipping), both guaranteeing $U_k \leq \epsilon/2$</li>
  <li><strong>Comparison with standard PPO</strong>: Standard PPO constrains $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$, which is infeasible under multi-policy mixture; Methods 1/2 constrain $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$, avoiding this issue</li>
  <li><strong>Method selection</strong>: Method 1 (adaptive) is recommended for high staleness or LLM large vocabulary scenarios; Method 2 (incremental) is recommended when implementation simplicity is prioritized</li>
  <li><strong>$S_k$ control</strong> is the sampling side’s responsibility, implemented through data filtering and version windows</li>
  <li><strong>Clipping is constrained optimization</strong>: Maximize the surrogate objective subject to $U_k$ constraints</li>
</ol>

<h2 id="part-vi-comparison-of-trajectory-level-and-stepsegment-level-mixture">Part VI: Comparison of Trajectory-Level and Step/Segment-Level Mixture</h2>

<h3 id="61-core-differences-between-the-two-mechanisms">6.1 Core Differences Between the Two Mechanisms</h3>

<p>The essential difference between the two mixture mechanisms lies in the structure of the index transition kernel:</p>

<ul>
  <li><strong>Trajectory-level mixture</strong>: $q(i’|i) = \mathbf{1}{i’=i}$, index never changes</li>
  <li><strong>Step/segment-level mixture</strong>: $\sigma(i) &gt; 0$, allows within-trajectory switching</li>
</ul>

<p>The correspondence with common engineering terminology is:</p>

<ul>
  <li><strong>Trajectory-level mixture</strong> here can be roughly understood as an idealized abstraction of “<strong>conventional asynchronous training</strong>”: data is organized by entire trajectories/episodes belonging to a certain policy version;</li>
  <li><strong>Step/segment-level mixture</strong> here can be roughly understood as an abstraction of “<strong>partial rollout</strong>”: due to asynchrony between actors and learners, and possible refresh to new policy versions at segment boundaries, using an index transition kernel that allows “within-trajectory version switching” can better approximate this phenomenon.</li>
</ul>

<p>The key watershed is <strong>whether Lemma 2.1’s structural simplification holds</strong>: trajectory-level mixture satisfies advantage function reduction; step/segment-level mixture generally does not, because future returns are affected by the index transition kernel.</p>

<h3 id="62-differences-in-sampling-staleness-s_k">6.2 Differences in Sampling Staleness $S_k$</h3>

<p><strong>Trajectory-level mixture</strong>’s staleness arises from: mixture weights $\alpha_i^{(k)}$ retaining probability mass on old policies after new policy release.</p>

<p><strong>Step/segment-level mixture</strong> has an <strong>exponential compression effect</strong>: Consider a simplified model with switching probability $\sigma$ from old to new. The marginal probability mass on old indices under the discounted visitation distribution is $\frac{1-\gamma}{1-\gamma(1-\sigma)}$. As long as $\sigma \gg 1-\gamma$, the old policy weight can be significantly compressed.</p>

<h3 id="63-differences-in-surrogate-objective-estimation">6.3 Differences in Surrogate Objective Estimation</h3>

<p><strong>Trajectory-level mixture</strong>: The advantage function reduces to $A^{\pi^{(i)}}(s,a)$, with a clear estimation path.</p>

<p><strong>Advantage substitution bias in step/segment-level mixture</strong>: If single-policy advantage estimates are used, systematic bias will arise. The reason is that $A^{\beta^{(k)}}((s,i),a)$ requires taking expectations over future index switching, while $A^{\pi^{(i)}}(s,a)$ implicitly assumes “the future always follows $\pi^{(i)}$.”</p>

<p><strong>Unification under bandit setting</strong>: In single-step episode LLM training, with no subsequent state transitions, the estimation problems of both mechanisms unify, with no such bias.</p>

<h3 id="64-variance-amplification-risk">6.4 Variance Amplification Risk</h3>

<p>Step/segment-level mixture has another hidden concern: even if single-step importance ratios are clipped, multi-step noise accumulation over long trajectories can still amplify gradient estimation variance. When policy changes per update are large, “behavioral discontinuities” within trajectories may induce heavier-tailed ratio distributions. This is why trajectory-level mixture is recommended for “large policy change per update” scenarios in the table below.</p>

<h3 id="65-applicable-scenarios">6.5 Applicable Scenarios</h3>

<p><strong>Table 6.1　Applicable Scenarios for Two Mixture Mechanisms</strong></p>

<div>
<table>
<thead>
<tr><th>Scenario Characteristics</th><th>Recommended Mechanism</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>Long trajectories, high-frequency updates, strong asynchrony</td><td>Step/segment-level</td><td>Can significantly compress $S_k$</td></tr>
<tr><td>Short trajectories (non-bandit)</td><td>Trajectory-level</td><td>$S_k$ is naturally low</td></tr>
<tr><td>Large policy change per update</td><td>Trajectory-level</td><td>Avoids variance amplification</td></tr>
<tr><td>Single-step episode (bandit)</td><td>Either</td><td>Choose based on implementation convenience</td></tr>
<tr><td>Need for compromise</td><td>Segment-level</td><td>Switch at natural boundaries</td></tr>
</tbody>
</table>
</div>

<p><strong>Core trade-off</strong>: Step/segment-level mixture is stronger on the sampling side (fast staleness removal), while trajectory-level mixture is more stable on the estimation side (easier surrogate objective estimation).</p>

<h2 id="part-vii-handling-training-inference-inconsistency">Part VII: Handling Training-Inference Inconsistency</h2>

<h3 id="71-background">7.1 Background</h3>

<p>In large-scale distributed training, policies on the inference side and training side may be inconsistent:</p>

<ul>
  <li><strong>Numerical implementation differences</strong>: softmax normalization, quantization, kernel fusion</li>
  <li><strong>Decoding rule differences</strong>: temperature scaling, top-p/top-k sampling</li>
</ul>

<p>Let the behavior policy modeled on the training side be $\pi^{(i)}$, while the policy actually sampling on the inference side is $\hat{\pi}^{(i)}$.</p>

<h3 id="72-effective-staleness">7.2 Effective Staleness</h3>

<p>Define <strong>effective staleness</strong>:</p>

\[\hat{S}_k := \mathbb{E}_{(s,i) \sim d_{\hat{\beta}^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_k, \hat{\pi}^{(i)}; s) \big]\]

<p>This definition simultaneously covers version staleness and training-inference implementation differences.</p>

<h3 id="73-actionable-control">7.3 Actionable Control</h3>

<p>By Lemma 3.3, $\hat{S}_k$ can be expressed in sample-level computable form. Given threshold $\epsilon_{\mathrm{stale}}$, if training only uses samples satisfying $\lvert\pi_k(a|s)/\hat{\pi}^{(i)}(a|s) - 1\rvert \leq \epsilon_{\mathrm{stale}}$, then $\hat{S}_k \leq \epsilon_{\mathrm{stale}}/2$.</p>

<p><strong>Key implementation points</strong>:</p>

<ol>
  <li><strong>Behavior denominator alignment</strong>: The behavior probability in the loss should use the inference-side recorded $\hat{\pi}^{(i)}(a|s)$</li>
  <li><strong>Probability smoothing</strong>: If the inference side has truncation (e.g., top-k), ensure ratios are valid</li>
</ol>

<h2 id="summary-practical-guidelines">Summary: Practical Guidelines</h2>

<h3 id="core-theoretical-framework">Core Theoretical Framework</h3>

<p>The structure of the monotonic improvement lower bound is:</p>

\[J(\pi_{k+1}) - J(\pi_k) \geq \underbrace{L_{\beta^{(k)}}(\pi_{k+1})}_{\text{surrogate objective}} - \underbrace{C_1 \cdot U_k}_{\text{update shift penalty}} - \underbrace{C_2 \cdot S_k}_{\text{sampling staleness penalty}}\]

<h3 id="separation-of-concerns-principle">Separation of Concerns Principle</h3>

<div>
<table>
<thead>
<tr><th>Control Term</th><th>Responsible Party</th><th>Control Mechanism</th><th>Specific Operation</th></tr>
</thead>
<tbody>
<tr><td>$U_k$</td><td>Optimization algorithm</td><td>Policy clipping</td><td>Clip $\pi_{k+1}/\pi_k$</td></tr>
<tr><td>$S_k$</td><td>Sampling system</td><td>Data filtering</td><td>Discard stale samples</td></tr>
<tr><td>$S_k$</td><td>Sampling system</td><td>Version window</td><td>Use only most recent $W$ versions</td></tr>
</tbody>
</table>
</div>

<h3 id="clipping-method-selection">Clipping Method Selection</h3>

<div>
<table>
<thead>
<tr><th>Scenario</th><th>Recommended Method</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>High staleness</td><td>Method 1 (adaptive)</td><td>Automatically tightens constraints for stale samples</td></tr>
<tr><td>Implementation simplicity prioritized</td><td>Method 2 (incremental)</td><td>No need to store old policy information</td></tr>
<tr><td>LLM large vocabulary</td><td>Method 1</td><td>Avoids slow updates for low-probability tokens</td></tr>
</tbody>
</table>
</div>

<h3 id="handling-training-inference-inconsistency">Handling Training-Inference Inconsistency</h3>

<ul>
  <li>Use inference-side recorded $\hat{\pi}^{(i)}$ as the behavior denominator</li>
  <li>Compress effective staleness through sample filtering</li>
</ul>

<h2 id="appendix-quick-reference-for-key-symbols">Appendix: Quick Reference for Key Symbols</h2>

<div>
<table>
<thead>
<tr><th>Symbol</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td>$\pi_k$, $\pi^{(i)}$</td><td>Latest policy at round $k$, $i$-th old policy</td></tr>
<tr><td>$d_\pi(s)$, $A^\pi(s,a)$</td><td>Discounted state visitation distribution, advantage function</td></tr>
<tr><td>$D_{\mathrm{TV}}(\pi, \pi'; s)$</td><td>TV distance between two policies at state $s$</td></tr>
<tr><td>$\beta^{(k)}(a \mid s, i) := \pi^{(i)}(a \mid s)$</td><td>Mixture behavior policy at round $k$</td></tr>
<tr><td>$q(i' \mid i)$, $\alpha_i^{(k)}$</td><td>Index transition kernel, initial index distribution</td></tr>
<tr><td>$U_k$, $S_k$</td><td>Update increment shift, sampling staleness</td></tr>
<tr><td>$\epsilon$, $\epsilon_{\mathrm{stale}}$, $W$</td><td>Clipping radius, staleness threshold, version window</td></tr>
<tr><td>$C_{\pi,\pi_k}$</td><td>Expected advantage upper bound constant</td></tr>
</tbody>
</table>
</div>

<h2 id="references">References</h2>

<ol>
  <li>
    <p>John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. “Trust Region Policy Optimization” (TRPO). arXiv:1502.05477. <a href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a></p>
  </li>
  <li>
    <p>Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel. “Constrained Policy Optimization” (CPO). arXiv:1705.10528. <a href="https://arxiv.org/abs/1705.10528">https://arxiv.org/abs/1705.10528</a></p>
  </li>
  <li>
    <p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. “Proximal Policy Optimization Algorithms” (PPO). arXiv:1707.06347. <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
  </li>
  <li>
    <p>James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras. “Generalized Proximal Policy Optimization with Sample Reuse” (GePPO). arXiv:2111.00072. <a href="https://arxiv.org/abs/2111.00072">https://arxiv.org/abs/2111.00072</a></p>
  </li>
  <li>
    <p>Yuzhen Zhou, Jiajun Li, Yusheng Su, et al. “APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation” (APRIL; partial rollout). arXiv:2509.18521. <a href="https://arxiv.org/abs/2509.18521">https://arxiv.org/abs/2509.18521</a></p>
  </li>
  <li>
    <p>Jacob Hilton, Karl Cobbe, John Schulman. “Batch size-invariance for policy optimization” (Decoupled PPO). arXiv:2110.00641. <a href="https://arxiv.org/abs/2110.00641">https://arxiv.org/abs/2110.00641</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025OffPolicyLLMRL</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Off-Policy Training in LLM Reinforcement Learning: From Theory to Practice}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html}</span><span class="p">,</span>
	<span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-12-17}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[A systematic derivation of off-policy training theory for LLM reinforcement learning: starting from single-policy sampling performance improvement bounds, extending to multi-policy static/dynamic mixture sampling, establishing sufficient conditions for monotonic improvement, decomposing constraints via the triangle inequality into update increment shift (controllable by optimization) and sampling staleness (controllable by sampling), and ultimately translating these into actionable clipping mechanisms and data filtering strategies.]]></summary></entry><entry xml:lang="zh"><title type="html">驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh.html" rel="alternate" type="text/html" title="驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证" /><published>2025-12-17T00:00:00+00:00</published><updated>2025-12-17T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#引言为什么我们需要关心异策略" id="markdown-toc-引言为什么我们需要关心异策略">引言：为什么我们需要关心”异策略”？</a></li>
  <li><a href="#第一部分理论基础" id="markdown-toc-第一部分理论基础">第一部分：理论基础</a>    <ul>
      <li><a href="#11-基本设定" id="markdown-toc-11-基本设定">1.1 基本设定</a></li>
      <li><a href="#12-核心工具策略性能差异引理" id="markdown-toc-12-核心工具策略性能差异引理">1.2 核心工具：策略性能差异引理</a></li>
    </ul>
  </li>
  <li><a href="#第二部分单策略采样的性能改进下界" id="markdown-toc-第二部分单策略采样的性能改进下界">第二部分：单策略采样的性能改进下界</a>    <ul>
      <li><a href="#21-分布不匹配问题" id="markdown-toc-21-分布不匹配问题">2.1 分布不匹配问题</a></li>
      <li><a href="#22-状态分布差异的控制" id="markdown-toc-22-状态分布差异的控制">2.2 状态分布差异的控制</a></li>
      <li><a href="#23-策略性能改进下界" id="markdown-toc-23-策略性能改进下界">2.3 策略性能改进下界</a></li>
    </ul>
  </li>
  <li><a href="#第三部分多策略静态混合采样" id="markdown-toc-第三部分多策略静态混合采样">第三部分：多策略静态混合采样</a>    <ul>
      <li><a href="#31-实际场景" id="markdown-toc-31-实际场景">3.1 实际场景</a></li>
      <li><a href="#32-核心思想扩展状态空间" id="markdown-toc-32-核心思想扩展状态空间">3.2 核心思想：扩展状态空间</a></li>
      <li><a href="#33-轨迹级混合的结构简化" id="markdown-toc-33-轨迹级混合的结构简化">3.3 轨迹级混合的结构简化</a></li>
      <li><a href="#34-轨迹级混合的性能改进下界" id="markdown-toc-34-轨迹级混合的性能改进下界">3.4 轨迹级混合的性能改进下界</a></li>
    </ul>
  </li>
  <li><a href="#第四部分动态混合采样与单调提升条件" id="markdown-toc-第四部分动态混合采样与单调提升条件">第四部分：动态混合采样与单调提升条件</a>    <ul>
      <li><a href="#41-问题的核心挑战" id="markdown-toc-41-问题的核心挑战">4.1 问题的核心挑战</a></li>
      <li><a href="#42-统一建模框架" id="markdown-toc-42-统一建模框架">4.2 统一建模框架</a></li>
      <li><a href="#43-核心分解" id="markdown-toc-43-核心分解">4.3 核心分解</a></li>
      <li><a href="#44-单调提升下界" id="markdown-toc-44-单调提升下界">4.4 单调提升下界</a></li>
      <li><a href="#45-直接约束的不可行性" id="markdown-toc-45-直接约束的不可行性">4.5 直接约束的不可行性</a></li>
      <li><a href="#46-三角不等式分解" id="markdown-toc-46-三角不等式分解">4.6 三角不等式分解</a></li>
    </ul>
  </li>
  <li><a href="#第五部分裁剪机制的理论基础" id="markdown-toc-第五部分裁剪机制的理论基础">第五部分：裁剪机制的理论基础</a>    <ul>
      <li><a href="#51-从tv距离到可计算量" id="markdown-toc-51-从tv距离到可计算量">5.1 从TV距离到可计算量</a></li>
      <li><a href="#52-u_k-的样本表示" id="markdown-toc-52-u_k-的样本表示">5.2 $U_k$ 的样本表示</a></li>
      <li><a href="#53-两种约束-u_k-的方法" id="markdown-toc-53-两种约束-u_k-的方法">5.3 两种约束 $U_k$ 的方法</a></li>
      <li><a href="#55-三种方法的对比" id="markdown-toc-55-三种方法的对比">5.5 三种方法的对比</a></li>
      <li><a href="#56-采样陈旧性的控制" id="markdown-toc-56-采样陈旧性的控制">5.6 采样陈旧性的控制</a></li>
      <li><a href="#57-裁剪的操作含义" id="markdown-toc-57-裁剪的操作含义">5.7 裁剪的操作含义</a></li>
      <li><a href="#58-本节小结" id="markdown-toc-58-本节小结">5.8 本节小结</a></li>
    </ul>
  </li>
  <li><a href="#第六部分轨迹级与步段级混合的比较" id="markdown-toc-第六部分轨迹级与步段级混合的比较">第六部分：轨迹级与步/段级混合的比较</a>    <ul>
      <li><a href="#61-两类机制的核心差异" id="markdown-toc-61-两类机制的核心差异">6.1 两类机制的核心差异</a></li>
      <li><a href="#62-采样陈旧性-s_k-的差异" id="markdown-toc-62-采样陈旧性-s_k-的差异">6.2 采样陈旧性 $S_k$ 的差异</a></li>
      <li><a href="#63-代理目标估计的差异" id="markdown-toc-63-代理目标估计的差异">6.3 代理目标估计的差异</a></li>
      <li><a href="#64-方差放大风险" id="markdown-toc-64-方差放大风险">6.4 方差放大风险</a></li>
      <li><a href="#65-适用场景" id="markdown-toc-65-适用场景">6.5 适用场景</a></li>
    </ul>
  </li>
  <li><a href="#第七部分训推不一致的处理" id="markdown-toc-第七部分训推不一致的处理">第七部分：训推不一致的处理</a>    <ul>
      <li><a href="#71-问题背景" id="markdown-toc-71-问题背景">7.1 问题背景</a></li>
      <li><a href="#72-有效陈旧性" id="markdown-toc-72-有效陈旧性">7.2 有效陈旧性</a></li>
      <li><a href="#73-可操作控制" id="markdown-toc-73-可操作控制">7.3 可操作控制</a></li>
    </ul>
  </li>
  <li><a href="#总结实践指南" id="markdown-toc-总结实践指南">总结：实践指南</a>    <ul>
      <li><a href="#核心理论框架" id="markdown-toc-核心理论框架">核心理论框架</a></li>
      <li><a href="#职责分离原则" id="markdown-toc-职责分离原则">职责分离原则</a></li>
      <li><a href="#裁剪方法选择" id="markdown-toc-裁剪方法选择">裁剪方法选择</a></li>
      <li><a href="#训推不一致处理" id="markdown-toc-训推不一致处理">训推不一致处理</a></li>
    </ul>
  </li>
  <li><a href="#附录关键符号速查表" id="markdown-toc-附录关键符号速查表">附录：关键符号速查表</a></li>
  <li><a href="#参考文献" id="markdown-toc-参考文献">参考文献</a></li>
</ul>

<p><a href="/reinforcement-learning/2025/12/17/offpolicy-en.html">English Version</a></p>

<h2 id="引言为什么我们需要关心异策略">引言：为什么我们需要关心”异策略”？</h2>

<p>想象这样一个场景：你正在用强化学习训练一个大语言模型，让它学会更好地回答问题。理想情况下，每次模型生成一批回答后，你会立即用这些数据更新模型，然后用更新后的模型生成新数据，如此循环往复。这种”用谁的数据就更新谁”的方式叫做<strong>同策略</strong>（on-policy）训练。</p>

<p>但现实没这么简单。在大规模分布式训练中，数百个GPU并行生成数据，而模型更新需要时间。当新模型发布时，很多”旧版本”模型生成的数据还没用完——扔掉太浪费，用起来又担心”数据过时”会影响训练效果。</p>

<p>这就是<strong>异策略</strong>（off-policy）训练面临的核心问题：<strong>用旧策略采集的数据来更新新策略，能保证性能持续提升吗？</strong></p>

<p>本文将系统回答这个问题。我们从基础理论出发，逐步推导出可操作的条件，告诉你：在什么情况下，混合使用多个版本策略的数据仍然能保证训练单调改进。</p>

<h2 id="第一部分理论基础">第一部分：理论基础</h2>

<h3 id="11-基本设定">1.1 基本设定</h3>

<p>我们考虑标准的马尔可夫决策过程（MDP），包含状态空间 $\mathcal{S}$、动作空间 $\mathcal{A}$、转移概率 $p(s’|s,a)$、奖励函数 $r(s,a)$、初始分布 $\rho_0$ 和折扣因子 $\gamma \in (0,1)$。</p>

<p>策略 $\pi$ 的<strong>期望累计折扣回报</strong>为：</p>

\[J(\pi) := \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid \pi\right]\]

<p><strong>折扣状态访问分布</strong>定义为策略长期运行中访问各状态的加权频率：</p>

\[d_\pi(s) := (1-\gamma) \sum_{t=0}^{\infty} \gamma^t \Pr(s_t = s \mid \pi)\]

<p><strong>优势函数</strong>衡量动作 $a$ 相对于策略平均水平的优劣：</p>

\[A^\pi(s,a) := Q^\pi(s,a) - V^\pi(s)\]

<p><strong>全变差距离</strong>（TV距离）衡量两个策略在状态 $s$ 上动作分布的差异：</p>

\[D_{\mathrm{TV}}(\pi, \pi'; s) := \frac{1}{2} \sum_{a \in \mathcal{A}} |\pi(a \mid s) - \pi'(a \mid s)|\]

<h3 id="12-核心工具策略性能差异引理">1.2 核心工具：策略性能差异引理</h3>

<p>整个理论的基石是这个简洁的结论：</p>

<blockquote>
  <p><strong>引理1.1（策略性能差异引理）</strong></p>

  <p>对任意策略 $\pi_k$（旧）和 $\pi$（新），性能差异可表示为：</p>

\[J(\pi) - J(\pi_k) = \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_\pi}\left[ \mathbb{E}_{a \sim \pi(\cdot \mid s)}[A^{\pi_k}(s,a)] \right]\]
</blockquote>

<p><strong>直观理解</strong>：新策略比旧策略好多少，等于在新策略访问的状态分布下，用新策略选动作能获得的”平均优势”。</p>

<h2 id="第二部分单策略采样的性能改进下界">第二部分：单策略采样的性能改进下界</h2>

<h3 id="21-分布不匹配问题">2.1 分布不匹配问题</h3>

<p>策略性能差异引理有个实际问题：右侧期望在 $d_\pi$（新策略的状态分布）下计算，而我们只能从 $d_{\pi_k}$（旧策略）采样。</p>

<p>解决思路是：把期望拆成”旧分布下的期望 + 偏差项”，然后控制偏差。关键问题是：<strong>状态分布的差异与策略的差异有什么定量关系？</strong></p>

<h3 id="22-状态分布差异的控制">2.2 状态分布差异的控制</h3>

<blockquote>
  <p><strong>引理1.2（状态分布差异与策略TV距离的关系）</strong></p>

\[\|d_\pi - d_{\pi_k}\|_1 \leq \frac{2\gamma}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]\]
</blockquote>

<p><strong>物理意义</strong>：策略在动作空间上的小差异，会通过环境动力学”放大”成状态访问分布的差异。系数 $\frac{\gamma}{1-\gamma}$ 反映了<strong>时间累积效应</strong>——长时域任务（$\gamma$ 接近1）中，放大效应更强。</p>

<p><strong>证明思路</strong>：推导折扣访问分布的不动点方程，利用随机矩阵的 $\ell_1$ 非扩张性，可以证明状态分布差异被策略差异通过转移动力学放大，放大系数正是 $\frac{\gamma}{1-\gamma}$。</p>

<h3 id="23-策略性能改进下界">2.3 策略性能改进下界</h3>

<blockquote>
  <p><strong>定理1.1（策略性能改进下界）</strong></p>

  <p>定义期望优势上界常数 $C_{\pi,\pi_k} := \max_{s} \lvert \mathbb{E}_{a \sim \pi}[A^{\pi_k}(s,a)] \rvert$，则：</p>

\[J(\pi) - J(\pi_k) \geq L_{\pi_k}(\pi) - \frac{2\gamma C_{\pi,\pi_k}}{(1-\gamma)^2} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]\]

  <p>其中<strong>代理目标</strong>为：</p>

\[L_{\pi_k}(\pi) := \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}, a \sim \pi_k} \left[ \frac{\pi(a \mid s)}{\pi_k(a \mid s)} A^{\pi_k}(s,a) \right]\]
</blockquote>

<p>这个下界由两部分组成：</p>

<ol>
  <li>
    <p><strong>代理目标</strong> $L_{\pi_k}(\pi)$：可用旧策略数据通过重要性采样直接估计，是TRPO/PPO的优化目标。</p>
  </li>
  <li>
    <p><strong>策略偏移惩罚</strong>：随新旧策略的TV距离增大而增大，这解释了为何PPO需要限制更新幅度。</p>
  </li>
</ol>

<p><strong>核心结论</strong>：最大化代理目标的同时控制策略偏移，即可保证性能改进。</p>

<h2 id="第三部分多策略静态混合采样">第三部分：多策略静态混合采样</h2>

<h3 id="31-实际场景">3.1 实际场景</h3>

<p>在实际训练中，一个batch的数据可能来自多个策略版本 ${\pi^{(1)}, \ldots, \pi^{(M)}}$，各版本占比为 $\alpha_1, \ldots, \alpha_M$。如何将定理1.1扩展到这种情形？</p>

<h3 id="32-核心思想扩展状态空间">3.2 核心思想：扩展状态空间</h3>

<p>解决方案是一个优雅的建模技巧：<strong>把策略版本索引当作状态的一部分</strong>。</p>

<p>定义扩展状态空间 $\tilde{\mathcal{S}} := \mathcal{S} \times \mathcal{I}$，其中 $\mathcal{I} = {1, \ldots, M}$ 是策略索引集合。在扩展状态 $(s, i)$ 下，<strong>混合行为策略</strong>定义为 $\beta(a \mid s, i) := \pi^{(i)}(a \mid s)$。</p>

<p>索引的演化由<strong>索引转移核</strong> $q(i’ \mid i)$ 刻画。扩展MDP继承原始MDP的奖励和环境转移，索引按 $q(i’|i)$ 独立演化。</p>

<p>这个技巧之所以有效，是因为新策略 $\pi$ 在扩展MDP上的回报等于原始MDP中的回报，从而可以直接应用定理1.1。</p>

<h3 id="33-轨迹级混合的结构简化">3.3 轨迹级混合的结构简化</h3>

<p>最常见的情形是<strong>每条轨迹只用一个旧策略</strong>：轨迹开始时采样索引 $I_0 \sim \alpha$，整条轨迹使用 $\pi^{(I_0)}$。此时索引转移核为恒等转移：$q(i’ \mid i) = \mathbf{1}_{i’=i}$。</p>

<p>从工程实现角度看，在很多 <strong>actor-learner 的异步训练</strong>里（如果采样与训练侧把数据按”整条轨迹/完整 episode 归属某个策略版本”来组织），这可以近似对应这里的<strong>轨迹级混合</strong>：actor 在一个采样单元内固定使用某个策略快照生成数据，learner 再混合使用来自不同版本的整轨迹数据做更新。这里用”近似”是因为不同系统对”轨迹/采样单元”的切分边界并不完全一致。</p>

<blockquote>
  <p><strong>引理2.1（轨迹级混合的结构简化）</strong></p>

  <p>(a) 扩展状态访问分布分解为：$d_{\beta}(s, i) = \alpha_i \cdot d_{\pi^{(i)}}(s)$</p>

  <p>(b) 优势函数还原为：$A^{\beta}((s, i), a) = A^{\pi^{(i)}}(s, a)$</p>
</blockquote>

<p><strong>(b)的直觉</strong>：由于索引永不改变，从扩展状态 $(s,i)$ 出发的<strong>所有未来轨迹</strong>都由同一个策略 $\pi^{(i)}$ 生成。因此，未来的累计回报完全由 $\pi^{(i)}$ 决定，价值函数和优势函数自然还原为 $\pi^{(i)}$ 的对应量。</p>

<p>由此，混合策略的回报为各旧策略回报的加权平均：$J_{\mathrm{mix}} = \sum_{i=1}^{M} \alpha_i J(\pi^{(i)})$。</p>

<h3 id="34-轨迹级混合的性能改进下界">3.4 轨迹级混合的性能改进下界</h3>

<blockquote>
  <p><strong>推论2.1（轨迹级混合的性能改进下界）</strong></p>

\[J(\pi) - \sum_{i=1}^{M} \alpha_i J(\pi^{(i)}) \geq \sum_{i=1}^{M} \alpha_i L_{\pi^{(i)}}(\pi) - \frac{2\gamma \max_i C_{\pi, \pi^{(i)}}}{(1-\gamma)^2} \sum_{i=1}^{M} \alpha_i \mathbb{E}_{s \sim d_{\pi^{(i)}}} \big[ D_{\mathrm{TV}}(\pi, \pi^{(i)}; s) \big]\]
</blockquote>

<p>该结论表明：将多个旧策略版本的轨迹混合训练时，若对每条轨迹用对应旧策略的重要性比率构造损失，同时控制新策略与各旧策略的偏移，则新策略性能有明确的改进下界。</p>

<h2 id="第四部分动态混合采样与单调提升条件">第四部分：动态混合采样与单调提升条件</h2>

<h3 id="41-问题的核心挑战">4.1 问题的核心挑战</h3>

<p>第三部分讨论的是<strong>静态混合</strong>——混合权重 $\alpha_i$ 固定不变。本节考虑更一般的<strong>动态混合</strong>——新策略发布后，采样逐步由新策略接管。</p>

<p>前面的结论刻画了”新策略相对于混合行为策略”的改进。但在实际训练中，我们真正关心的是：<strong>每轮更新后的最新策略 $\pi_{k+1}$ 相对于上一轮最新策略 $\pi_k$ 是否单调提升？</strong></p>

\[J(\pi_{k+1}) \geq J(\pi_k)\]

<h3 id="42-统一建模框架">4.2 统一建模框架</h3>

<p>动态混合采样的两种典型形式都可以用索引转移核 $q(i’|i)$ 统一刻画：</p>

<p><strong>轨迹级混合</strong>（可类比为常规异步训练的一个抽象；索引恒等转移）：$q(i’|i) = \mathbf{1}{i’=i}$</p>

<p><strong>步/段级混合</strong>（partial rollout / 段式采样的一个抽象；允许切换）：$q(i’|i) = (1-\sigma(i))\mathbf{1}{i’=i} + \sigma(i)\kappa(i’|i)$</p>

<p>其中 $\sigma(i)$ 为切换概率，$\kappa(\cdot|i)$ 为目标索引分布。</p>

<h3 id="43-核心分解">4.3 核心分解</h3>

<p>通过引入混合回报 $J_{\mathrm{mix}}^{(k)}$ 作为中间桥梁，性能差异分解为：</p>

\[J(\pi_{k+1}) - J(\pi_k) = \underbrace{[J(\pi_{k+1}) - J_{\mathrm{mix}}^{(k)}]}_{\text{相对混合策略的改进}} + \underbrace{[J_{\mathrm{mix}}^{(k)} - J(\pi_k)]}_{\text{混合偏差项}}\]

<p>第一项可用定理1.1处理。第二项是<strong>混合偏差项</strong>，可以证明它满足：</p>

\[J_{\mathrm{mix}}^{(k)} - J(\pi_k) \geq -\frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]\]

<h3 id="44-单调提升下界">4.4 单调提升下界</h3>

<p>合并上述结果，得到核心定理：</p>

<blockquote>
  <p><strong>定理3.1（动态混合采样下的单调提升下界）</strong></p>

\[\begin{aligned}
J(\pi_{k+1}) - J(\pi_k) \geq\;&amp; L_{\beta^{(k)}}(\pi_{k+1}) \\
&amp;- \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \big] \\
&amp;- \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
\end{aligned}\]
</blockquote>

<p>该下界揭示了<strong>双重控制</strong>的必要性：</p>
<ul>
  <li><strong>更新偏移惩罚</strong>：新策略 $\pi_{k+1}$ 相对采样来源策略 $\pi^{(i)}$ 的偏移</li>
  <li><strong>采样陈旧性惩罚</strong>：采样来源策略 $\pi^{(i)}$ 相对当前策略 $\pi_k$ 的陈旧性</li>
</ul>

<h3 id="45-直接约束的不可行性">4.5 直接约束的不可行性</h3>

<p>定理3.1中的更新偏移惩罚项看似可以通过约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s)$ 来控制，但这实际上<strong>不可行</strong>：</p>

<blockquote>
  <p><strong>观察3.1（更新偏移约束的不可行性）</strong></p>

  <p>设混合采样包含两个旧策略 $\pi^{(1)}$ 和 $\pi^{(2)}$，若存在某状态 $s$ 使 $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) &gt; 2\delta$，则不存在策略 $\pi_{k+1}$ 同时满足 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(1)}; s) \leq \delta$ 与 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(2)}; s) \leq \delta$。</p>
</blockquote>

<p><strong>证明</strong>：由三角不等式，若同时满足两约束，则 $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) \leq 2\delta$，矛盾。</p>

<p><strong>问题根源</strong>：更新偏移惩罚项将 $\pi_{k+1}$ 与历史策略族 ${\pi^{(i)}}$ 直接耦合，而后者的内部结构是历史训练的产物，不受当前更新控制。</p>

<h3 id="46-三角不等式分解">4.6 三角不等式分解</h3>

<p>解决方案是利用TV距离的三角不等式：</p>

\[D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \leq D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s) + D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\]

<p>这将耦合约束拆分为两个独立部分：</p>

<ul>
  <li><strong>更新增量偏移</strong> $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)$：新策略相对当前策略的偏离，<strong>可由优化侧控制</strong></li>
  <li><strong>采样陈旧性</strong> $D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)$：当前策略相对各旧策略的偏离，<strong>需由采样侧控制</strong></li>
</ul>

<p>定义：</p>

\[U_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)\big], \quad S_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\big]\]

<blockquote>
  <p><strong>推论3.2（分解后的单调提升下界）</strong></p>

\[J(\pi_{k+1}) - J(\pi_k) \geq L_{\beta^{(k)}}(\pi_{k+1}) - \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} U_k - \left( \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} + \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \right) S_k\]
</blockquote>

<p><strong>为何分解能解决问题？</strong> 关键在于：分解后的 $U_k$ 只涉及新策略 $\pi_{k+1}$ 和当前策略 $\pi_k$，<strong>与旧策略族 ${\pi^{(i)}}$ 的结构完全无关</strong>。因此，无论旧策略之间差异多大，约束 $U_k$ 都是可行的——这正是观察3.1揭示的不可行性问题的解决之道。</p>

<p>这揭示了重要的工程原则——<strong>职责分离</strong>：</p>

<div>
<table>
<thead>
<tr><th>控制项</th><th>负责方</th><th>控制手段</th></tr>
</thead>
<tbody>
<tr><td>$U_k$（更新增量偏移）</td><td>优化算法</td><td>策略裁剪</td></tr>
<tr><td>$S_k$（采样陈旧性）</td><td>采样系统</td><td>数据过滤、版本窗口</td></tr>
</tbody>
</table>
</div>

<h2 id="第五部分裁剪机制的理论基础">第五部分：裁剪机制的理论基础</h2>

<h3 id="51-从tv距离到可计算量">5.1 从TV距离到可计算量</h3>

<p>推论3.2告诉我们，要保证单调提升，需要控制更新增量偏移 $U_k = \mathbb{E}[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)]$。但TV距离是分布层面的量，如何用样本来控制它？</p>

<p>关键桥梁是下面这个恒等式：</p>

<blockquote>
  <p><strong>引理3.3（TV距离的比值差表示）</strong></p>

  <p>设策略 $\pi_1$ 的支撑覆盖 $\pi$ 和 $\pi_2$ 的支撑，则对任意状态分布 $\mu$：</p>

\[\mathbb{E}_{s\sim \mu} \big[D_{\mathrm{TV}}(\pi, \pi_2; s)\big] = \frac{1}{2} \mathbb{E}_{s\sim \mu, a\sim\pi_1(\cdot|s)} \left| \frac{\pi(a|s)}{\pi_1(a|s)} - \frac{\pi_2(a|s)}{\pi_1(a|s)} \right|\]
</blockquote>

<p><strong>直观理解</strong>：左边是两个分布的TV距离（需要遍历所有动作），右边是在 $\pi_1$ 下采样时两个重要性比值的差的绝对值。这使得我们可以用样本来估计和控制TV距离。</p>

<h3 id="52-u_k-的样本表示">5.2 $U_k$ 的样本表示</h3>

<p>利用引理3.3，取 $\pi = \pi_{k+1}$，$\pi_2 = \pi_k$，$\pi_1 = \pi^{(i)}$（采样来源策略），可得：</p>

\[U_k = \frac{1}{2} \mathbb{E}_{(s,i) \sim d_{\beta^{(k)}}, a \sim \pi^{(i)}(\cdot|s)} \left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right|\]

<p>记 $\rho_{k+1} := \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)}$ 和 $\rho_k := \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)}$，则：</p>

\[U_k = \frac{1}{2} \mathbb{E}_{(s,i,a) \sim \text{训练数据}} \big| \rho_{k+1} - \rho_k \big|\]

<p>这意味着：<strong>如果我们能让每个样本上 $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$，就能保证 $U_k \leq \epsilon/2$</strong>。</p>

<h3 id="53-两种约束-u_k-的方法">5.3 两种约束 $U_k$ 的方法</h3>

<p><strong>方法一：直接约束比值差</strong></p>

<p>对每个样本 $(s, i, a)$，要求：</p>

\[\left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right| \leq \epsilon\]

<p>即裁剪区间为 $\left[\frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} - \epsilon, \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} + \epsilon\right]$，<strong>裁剪中心是 $\rho_k$ 而非 1</strong>。</p>

<p><strong>方法二：约束增量比值</strong></p>

<p>注意到 $\rho_{k+1} - \rho_k = \rho_k \cdot \left(\frac{\pi_{k+1}}{\pi_k} - 1\right)$，因此：</p>

\[|\rho_{k+1} - \rho_k| = \rho_k \cdot \left|\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right|\]

<p>如果约束 $\left\lvert\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right\rvert \leq \epsilon$，由于 $\mathbb{E}_{a\sim\pi^{(i)}}[\rho_k] = 1$，可证 $U_k \leq \epsilon/2$。</p>

<p>这种方法直接对 $\pi_{k+1}/\pi_k$ 以 1 为中心裁剪，**完全不涉及旧策略 $\pi^{(i)}$比，我们给出三种裁剪机制的完整目标函数。设当前样本来自旧策略 $\pi^{(i)}$，记：</p>
<ul>
  <li>$\rho_{k+1} = \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)}$（新策略相对采样策略的比值）</li>
  <li>$\rho_k = \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)}$（当前策略相对采样策略的比值）</li>
  <li>$r = \frac{\pi_{k+1}(a|s)}{\pi_k(a|s)}$（新策略相对当前策略的增量比值）</li>
</ul>

<p><strong>标准PPO</strong>：以 1 为中心裁剪 $\rho_{k+1}$</p>

\[L^{\mathrm{PPO}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\pi^{(i)}}, \; \mathrm{clip}(\rho_{k+1}, 1-\epsilon, 1+\epsilon) \cdot A^{\pi^{(i)}} \right) \right]\]

<p><strong>方法一</strong>：以 $\rho_k$ 为中心裁剪 $\rho_{k+1}$</p>

\[L^{\mathrm{M1}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\beta^{(k)}}, \; \mathrm{clip}(\rho_{k+1}, \rho_k-\epsilon, \rho_k+\epsilon) \cdot A^{\beta^{(k)}} \right) \right]\]

<p><strong>方法二</strong>：以 1 为中心裁剪增量比值 $r$</p>

\[L^{\mathrm{M2}} = \mathbb{E} \left[ \min\left( r \cdot \hat{A}, \; \mathrm{clip}(r, 1-\epsilon, 1+\epsilon) \cdot \hat{A} \right) \right]\]

<p>其中 $\hat{A} = \rho_k \cdot A^{\beta^{(k)}}$ 是经过重要性加权的优势估计。</p>

<h3 id="55-三种方法的对比">5.5 三种方法的对比</h3>

<p><strong>表5.1　三种裁剪机制的对比</strong></p>

<div>
<table>
<thead>
<tr><th>方法</th><th>裁剪变量</th><th>裁剪中心</th><th>裁剪区间</th><th>约束的TV距离</th></tr>
</thead>
<tbody>
<tr><td>标准PPO</td><td>$\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$</td><td>$1$</td><td>$[1-\epsilon, 1+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$</td></tr>
<tr><td>方法一</td><td>$\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$</td><td>$\rho_k = \pi_k/\pi^{(i)}$</td><td>$[\rho_k-\epsilon, \rho_k+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$</td></tr>
<tr><td>方法二</td><td>$r = \pi_{k+1}/\pi_k$</td><td>$1$</td><td>$[1-\epsilon, 1+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$</td></tr>
</tbody>
</table>
</div>

<p><strong>标准PPO在多策略混合下的根本问题</strong></p>

<p>标准PPO约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$，要求新策略同时接近所有采样来源策略。由观察3.1，当各旧策略 $\pi^{(1)}, \pi^{(2)}, \ldots$ 之间差异显著时，<strong>不存在能同时满足所有约束的 $\pi_{k+1}$</strong>。这导致信赖域交集收缩甚至为空，更新被最陈旧的策略所限制。</p>

<p><strong>方法一与方法二的共同优势</strong></p>

<p>两者都约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$——新策略相对<strong>当前策略</strong>（而非采样策略）的偏离。由于 $\pi_k$ 是唯一确定的，这个约束对所有来源的样本一致，完全规避了不可行性问题。</p>

<p><strong>方法一 vs 方法二</strong></p>

<div>
<table>
<thead>
<tr><th>比较维度</th><th>方法一（自适应裁剪）</th><th>方法二（增量裁剪）</th></tr>
</thead>
<tbody>
<tr><td>陈旧样本（$\rho_k \gg 1$）</td><td>自动收紧约束，更保守</td><td>可能产生大梯度方差</td></tr>
<tr><td>LLM大词表低概率token</td><td>允许较大绝对变化（加法型）</td><td>绝对变化受限（乘法型）</td></tr>
<tr><td>实现复杂度</td><td>需存储 $\pi^{(i)}(a|s)$ 和 $\pi_k(a|s)$</td><td>仅需 $\pi_k(a|s)$</td></tr>
<tr><td>优势函数</td><td>使用 $A^{\beta^{(k)}}$</td><td>使用加权优势 $\rho_k \cdot A^{\beta^{(k)}}$</td></tr>
</tbody>
</table>
</div>

<p><strong>详细解释</strong>：</p>

<p><strong>(一) 陈旧样本处理</strong></p>

<p>当样本来自很旧的策略时，$\rho_k = \pi_k/\pi^{(i)}$ 可能很大。</p>

<ul>
  <li>方法二的被积函数为 $\rho_k \cdot \lvert r - 1\rvert$，即便 $\lvert r-1\rvert \leq \epsilon$，被积函数仍可达 $\epsilon \cdot \rho_k$，产生尖峰。</li>
  <li>方法一直接约束 $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$，被积函数上界恒为 $\epsilon$，不受 $\rho_k$ 放大。</li>
</ul>

<p><strong>(二) LLM大词表问题</strong></p>

<p>大语言模型词表规模巨大，大量token概率极小。</p>

<ul>
  <li>方法二约束 $\pi_{k+1} \in [(1-\epsilon)\pi_k, (1+\epsilon)\pi_k]$，这是<strong>乘法型约束</strong>：若 $\pi_k(a|s) = 10^{-6}$，允许的绝对变化仅为 $\epsilon \times 10^{-6}$。</li>
  <li>方法一约束 $\lvert\pi_{k+1} - \pi_k\rvert \leq \epsilon \cdot \pi^{(i)}$，这是<strong>加法型约束</strong>：若该token在旧策略下概率较高（如 $\pi^{(i)}(a|s) = 0.1$），即便当前概率很低，也允许较快提升。</li>
</ul>

<h3 id="56-采样陈旧性的控制">5.6 采样陈旧性的控制</h3>

<p>推论3.2表明，$S_k$ 同样影响单调提升下界，但它<strong>无法通过优化侧裁剪控制</strong>，需由采样系统实现：</p>

<p><strong>(一) 丢弃陈旧数据</strong></p>

<p>设定阈值 $\epsilon_{\mathrm{stale}}$，对每个样本计算 $\lvert\rho_k - 1\rvert = \lvert\pi_k(a|s)/\pi^{(i)}(a|s) - 1\rvert$，丢弃超过阈值者。</p>

<p><strong>(二) 控制策略版本窗口</strong></p>

<p>限制混合采样的旧策略版本数量，如仅用最近 $W$ 个版本的数据。</p>

<h3 id="57-裁剪的操作含义">5.7 裁剪的操作含义</h3>

<p>最后，需要澄清裁剪与理论下界的关系。</p>

<p>推论3.2中，$U_k$ 的系数 $C_{\pi_{k+1},\beta^{(k)}}$ 依赖于新策略 $\pi_{k+1}$，因此惩罚项<strong>不能简单替换为常数</strong>。正确的操作含义是：</p>

<blockquote>
  <p><strong>在 $U_k \leq \epsilon/2$ 的约束下，最大化代理目标 $L_{\beta^{(k)}}(\pi_{k+1})$</strong></p>
</blockquote>

<p>裁剪目标函数正是这一约束优化的实现——通过裁剪<strong>硬性限制</strong>更新幅度，确保 $U_k$ 可控；在此前提下，梯度上升提升代理目标，从而为策略单调改进提供保障。</p>

<h3 id="58-本节小结">5.8 本节小结</h3>

<p>本节建立了裁剪机制的理论基础：</p>

<ol>
  <li><strong>引理3.3</strong>将TV距离转化为样本层面的比值差，是连接理论与实现的桥梁</li>
  <li><strong>两种约束方法</strong>：方法一（自适应裁剪中心）和方法二（固定增量裁剪），均保证 $U_k \leq \epsilon/2$</li>
  <li><strong>与标准PPO对比</strong>：标准PPO约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$，在多策略混合下不可行；方法一/二约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$，规避了该问题</li>
  <li><strong>方法选择</strong>：陈旧性高或LLM大词表场景推荐方法一；实现简洁优先推荐方法二</li>
  <li><strong>$S_k$ 的控制</strong>由采样侧负责，通过数据过滤和版本窗口实现</li>
  <li><strong>裁剪是约束优化</strong>：在 $U_k$ 约束下最大化代理目标</li>
</ol>

<h2 id="第六部分轨迹级与步段级混合的比较">第六部分：轨迹级与步/段级混合的比较</h2>

<h3 id="61-两类机制的核心差异">6.1 两类机制的核心差异</h3>

<p>两类混合机制的本质区别在于索引转移核的结构：</p>

<ul>
  <li><strong>轨迹级混合</strong>：$q(i’|i) = \mathbf{1}{i’=i}$，索引永不改变</li>
  <li><strong>步/段级混合</strong>：$\sigma(i) &gt; 0$，允许轨迹内切换</li>
</ul>

<p>与常见工程术语的对应关系是：</p>

<ul>
  <li>这里的<strong>轨迹级混合</strong>可以大致理解为”<strong>常规异步训练</strong>“的一个理想化抽象：数据按整条轨迹/episode 归属到某个策略版本；</li>
  <li>这里的<strong>步/段级混合</strong>可以大致理解为”<strong>partial rollout</strong>“的一个抽象：由于 actor 与 learner 异步、且 segment 边界处可能刷新到新策略版本，用索引转移核允许”轨迹内部版本切换”可以更好地近似刻画这种现象。</li>
</ul>

<p>关键分水岭是<strong>引理2.1的结构简化是否成立</strong>：轨迹级混合满足优势函数还原；步/段级混合一般不满足，因为未来回报受索引转移核影响。</p>

<h3 id="62-采样陈旧性-s_k-的差异">6.2 采样陈旧性 $S_k$ 的差异</h3>

<p><strong>轨迹级混合</strong>的陈旧性来源于：混合权重 $\alpha_i^{(k)}$ 在新策略发布后仍对旧策略保留质量。</p>

<p><strong>步/段级混合</strong>具有<strong>指数压缩效应</strong>：考虑从旧到新以概率 $\sigma$ 切换的简化模型，折扣访问分布下旧索引的边缘质量为 $\frac{1-\gamma}{1-\gamma(1-\sigma)}$。只要 $\sigma \gg 1-\gamma$，旧策略权重即可被显著压缩。</p>

<h3 id="63-代理目标估计的差异">6.3 代理目标估计的差异</h3>

<p><strong>轨迹级混合</strong>：优势函数还原为 $A^{\pi^{(i)}}(s,a)$，估计路径清晰。</p>

<p><strong>步/段级混合的优势替代偏差</strong>：若沿用单策略优势估计，将产生系统性偏差。原因是 $A^{\beta^{(k)}}((s,i),a)$ 需要对未来索引切换取期望，而 $A^{\pi^{(i)}}(s,a)$ 隐含”未来始终沿用 $\pi^{(i)}$”的假设。</p>

<p><strong>Bandit设定下的统一</strong>：在单步episode的LLM训练中，无后续状态转移，两类机制的估计问题统一，无上述偏差。</p>

<h3 id="64-方差放大风险">6.4 方差放大风险</h3>

<p>步/段级混合还有一个隐患：即便单步重要性比值被裁剪，长轨迹下多步噪声叠加仍会放大梯度估计方差。当每次更新的策略变化幅度较大时，轨迹内部的”行为突变”可能引发更重尾的比值分布。这也是下表中”策略变化幅度大”场景推荐轨迹级混合的原因。</p>

<h3 id="65-适用场景">6.5 适用场景</h3>

<p><strong>表6.1　两类混合机制的适用场景</strong></p>

<div>
<table>
<thead>
<tr><th>场景特征</th><th>推荐机制</th><th>理由</th></tr>
</thead>
<tbody>
<tr><td>长轨迹、高频更新、强异步</td><td>步/段级</td><td>可显著压缩 $S_k$</td></tr>
<tr><td>短轨迹（非Bandit）</td><td>轨迹级</td><td>$S_k$ 自然较低</td></tr>
<tr><td>每次更新策略变化幅度大</td><td>轨迹级</td><td>避免方差放大</td></tr>
<tr><td>单步episode（Bandit）</td><td>均可</td><td>按实现便利选择</td></tr>
<tr><td>需要折中方案</td><td>段级</td><td>在自然边界切换</td></tr>
</tbody>
</table>
</div>

<p><strong>核心权衡</strong>：步/段级混合在采样侧更强（快速去陈旧），轨迹级混合在估计侧更稳（代理目标易估计）。</p>

<h2 id="第七部分训推不一致的处理">第七部分：训推不一致的处理</h2>

<h3 id="71-问题背景">7.1 问题背景</h3>

<p>在大规模分布式训练中，推理端和训练端的策略可能不一致：</p>

<ul>
  <li><strong>数值实现差异</strong>：softmax归一化、量化、核融合</li>
  <li><strong>解码规则差异</strong>：温度缩放、top-p/top-k采样</li>
</ul>

<p>设训练侧建模的行为策略为 $\pi^{(i)}$，而推理端实际采样的策略为 $\hat{\pi}^{(i)}$。</p>

<h3 id="72-有效陈旧性">7.2 有效陈旧性</h3>

<p>定义<strong>有效陈旧性</strong>：</p>

\[\hat{S}_k := \mathbb{E}_{(s,i) \sim d_{\hat{\beta}^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_k, \hat{\pi}^{(i)}; s) \big]\]

<p>该定义同时覆盖版本陈旧性与训推实现差异。</p>

<h3 id="73-可操作控制">7.3 可操作控制</h3>

<p>由引理3.3，$\hat{S}_k$ 可表示为样本级可计算形式。给定阈值 $\epsilon_{\mathrm{stale}}$，若训练仅使用满足 $\lvert\pi_k(a|s)/\hat{\pi}^{(i)}(a|s) - 1\rvert \leq \epsilon_{\mathrm{stale}}$ 的样本，则 $\hat{S}_k \leq \epsilon_{\mathrm{stale}}/2$。</p>

<p><strong>关键实现要点</strong>：</p>

<ol>
  <li><strong>行为分母对齐</strong>：损失中的行为概率应使用推理端记录的 $\hat{\pi}^{(i)}(a|s)$</li>
  <li><strong>概率平滑</strong>：若推理端有截断（如top-k），需确保比值合法</li>
</ol>

<h2 id="总结实践指南">总结：实践指南</h2>

<h3 id="核心理论框架">核心理论框架</h3>

<p>单调提升下界的结构为：</p>

\[J(\pi_{k+1}) - J(\pi_k) \geq \underbrace{L_{\beta^{(k)}}(\pi_{k+1})}_{\text{代理目标}} - \underbrace{C_1 \cdot U_k}_{\text{更新偏移惩罚}} - \underbrace{C_2 \cdot S_k}_{\text{采样陈旧性惩罚}}\]

<h3 id="职责分离原则">职责分离原则</h3>

<div>
<table>
<thead>
<tr><th>控制项</th><th>负责方</th><th>控制手段</th><th>具体操作</th></tr>
</thead>
<tbody>
<tr><td>$U_k$</td><td>优化算法</td><td>策略裁剪</td><td>对 $\pi_{k+1}/\pi_k$ 裁剪</td></tr>
<tr><td>$S_k$</td><td>采样系统</td><td>数据过滤</td><td>丢弃陈旧样本</td></tr>
<tr><td>$S_k$</td><td>采样系统</td><td>版本窗口</td><td>仅用最近 $W$ 个版本</td></tr>
</tbody>
</table>
</div>

<h3 id="裁剪方法选择">裁剪方法选择</h3>

<div>
<table>
<thead>
<tr><th>场景</th><th>推荐方法</th><th>理由</th></tr>
</thead>
<tbody>
<tr><td>陈旧性较高</td><td>方法一（自适应）</td><td>自动对陈旧样本收紧约束</td></tr>
<tr><td>实现简洁优先</td><td>方法二（增量）</td><td>无需存储旧策略信息</td></tr>
<tr><td>LLM大词表</td><td>方法一</td><td>避免低概率token更新过慢</td></tr>
</tbody>
</table>
</div>

<h3 id="训推不一致处理">训推不一致处理</h3>

<ul>
  <li>使用推理端记录的 $\hat{\pi}^{(i)}$ 作为行为分母</li>
  <li>通过样本过滤压缩有效陈旧性</li>
</ul>

<h2 id="附录关键符号速查表">附录：关键符号速查表</h2>

<div>
<table>
<thead>
<tr><th>符号</th><th>含义</th></tr>
</thead>
<tbody>
<tr><td>$\pi_k$, $\pi^{(i)}$</td><td>第 $k$ 轮最新策略，第 $i$ 个旧策略</td></tr>
<tr><td>$d_\pi(s)$, $A^\pi(s,a)$</td><td>折扣状态访问分布，优势函数</td></tr>
<tr><td>$D_{\mathrm{TV}}(\pi, \pi'; s)$</td><td>两策略在状态 $s$ 上的TV距离</td></tr>
<tr><td>$\beta^{(k)}(a \mid s, i) := \pi^{(i)}(a \mid s)$</td><td>第 $k$ 轮混合行为策略</td></tr>
<tr><td>$q(i' \mid i)$, $\alpha_i^{(k)}$</td><td>索引转移核，索引初始分布</td></tr>
<tr><td>$U_k$, $S_k$</td><td>更新增量偏移，采样陈旧性</td></tr>
<tr><td>$\epsilon$, $\epsilon_{\mathrm{stale}}$, $W$</td><td>裁剪半径，陈旧性阈值，版本窗口</td></tr>
<tr><td>$C_{\pi,\pi_k}$</td><td>期望优势上界常数</td></tr>
</tbody>
</table>
</div>

<h2 id="参考文献">参考文献</h2>

<ol>
  <li>
    <p>John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. “Trust Region Policy Optimization” (TRPO). arXiv:1502.05477. <a href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a></p>
  </li>
  <li>
    <p>Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel. “Constrained Policy Optimization” (CPO). arXiv:1705.10528. <a href="https://arxiv.org/abs/1705.10528">https://arxiv.org/abs/1705.10528</a></p>
  </li>
  <li>
    <p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. “Proximal Policy Optimization Algorithms” (PPO). arXiv:1707.06347. <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
  </li>
  <li>
    <p>James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras. “Generalized Proximal Policy Optimization with Sample Reuse” (GePPO). arXiv:2111.00072. <a href="https://arxiv.org/abs/2111.00072">https://arxiv.org/abs/2111.00072</a></p>
  </li>
  <li>
    <p>Yuzhen Zhou, Jiajun Li, Yusheng Su, et al. “APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation” (APRIL; partial rollout). arXiv:2509.18521. <a href="https://arxiv.org/abs/2509.18521">https://arxiv.org/abs/2509.18521</a></p>
  </li>
  <li>
    <p>Jacob Hilton, Karl Cobbe, John Schulman. “Batch size-invariance for policy optimization” (Decoupled PPO). arXiv:2110.00641. <a href="https://arxiv.org/abs/2110.00641">https://arxiv.org/abs/2110.00641</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025OffPolicyLLMRL</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Off-Policy Training in LLM Reinforcement Learning: From Theory to Practice}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html}</span><span class="p">,</span>
	<span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-12-17}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。]]></summary></entry><entry xml:lang="en"><title type="html">Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html" rel="alternate" type="text/html" title="Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" /><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#introduction-what-kl-does-in-rl" id="markdown-toc-introduction-what-kl-does-in-rl">Introduction: What KL Does in RL</a>    <ul>
      <li><a href="#forward-vs-reverse-kl" id="markdown-toc-forward-vs-reverse-kl">Forward vs. reverse KL</a></li>
    </ul>
  </li>
  <li><a href="#three-estimators-definitions-and-design" id="markdown-toc-three-estimators-definitions-and-design">Three estimators: definitions and design</a>    <ul>
      <li><a href="#k_1-the-naive-estimator" id="markdown-toc-k_1-the-naive-estimator">$k_1$: the naive estimator</a></li>
      <li><a href="#k_2-an-f-divergence-lower-variance" id="markdown-toc-k_2-an-f-divergence-lower-variance">$k_2$: an f-divergence, lower variance</a></li>
      <li><a href="#k_3-control-variate-optimal-shape" id="markdown-toc-k_3-control-variate-optimal-shape">$k_3$: control variate, “optimal” shape</a></li>
      <li><a href="#quick-comparison" id="markdown-toc-quick-comparison">Quick comparison</a></li>
    </ul>
  </li>
  <li><a href="#core-analysis" id="markdown-toc-core-analysis">Core analysis</a>    <ul>
      <li><a href="#bias-and-variance-for-kl-values" id="markdown-toc-bias-and-variance-for-kl-values">Bias and variance for KL values</a></li>
      <li><a href="#gradient-estimation-the-crucial-distinction" id="markdown-toc-gradient-estimation-the-crucial-distinction">Gradient estimation: the crucial distinction</a>        <ul>
          <li><a href="#true-gradients-for-reference" id="markdown-toc-true-gradients-for-reference">True gradients for reference</a></li>
          <li><a href="#two-differentiation-orders" id="markdown-toc-two-differentiation-orders">Two differentiation orders</a></li>
          <li><a href="#gradients-of-the-three-estimators-on-policy" id="markdown-toc-gradients-of-the-three-estimators-on-policy">Gradients of the three estimators (on-policy)</a></li>
          <li><a href="#expectation-then-grad-vs-grad-then-expectation" id="markdown-toc-expectation-then-grad-vs-grad-then-expectation">Expectation-then-grad vs. grad-then-expectation</a></li>
        </ul>
      </li>
      <li><a href="#off-policy-gradients-with-importance-sampling" id="markdown-toc-off-policy-gradients-with-importance-sampling">Off-policy gradients with importance sampling</a>        <ul>
          <li><a href="#setup" id="markdown-toc-setup">Setup</a></li>
          <li><a href="#crucial-observation-the-two-orders-coincide" id="markdown-toc-crucial-observation-the-two-orders-coincide">Crucial observation: the two orders coincide</a></li>
          <li><a href="#value-unbiasedness-remains" id="markdown-toc-value-unbiasedness-remains">Value unbiasedness remains</a></li>
          <li><a href="#gradients-with-weights" id="markdown-toc-gradients-with-weights">Gradients with weights</a></li>
          <li><a href="#variance-of-the-three-unbiased-off-policy-gradient-estimators" id="markdown-toc-variance-of-the-three-unbiased-off-policy-gradient-estimators">Variance of the three unbiased off-policy gradient estimators</a></li>
        </ul>
      </li>
      <li><a href="#gradient-cheat-sheet" id="markdown-toc-gradient-cheat-sheet">Gradient cheat sheet</a></li>
    </ul>
  </li>
  <li><a href="#two-ways-to-use-kl-as-reward-vs-as-loss" id="markdown-toc-two-ways-to-use-kl-as-reward-vs-as-loss">Two Ways to Use KL: As Reward vs. As Loss</a>    <ul>
      <li><a href="#definitions" id="markdown-toc-definitions">Definitions</a></li>
      <li><a href="#key-difference-1-optimization-target" id="markdown-toc-key-difference-1-optimization-target">Key Difference 1: Optimization Target</a></li>
      <li><a href="#key-difference-2-actor-gradient" id="markdown-toc-key-difference-2-actor-gradient">Key Difference 2: Actor Gradient</a></li>
      <li><a href="#key-difference-3-critic-learning-target" id="markdown-toc-key-difference-3-critic-learning-target">Key Difference 3: Critic Learning Target</a></li>
      <li><a href="#key-difference-4-credit-assignment" id="markdown-toc-key-difference-4-credit-assignment">Key Difference 4: Credit Assignment</a></li>
      <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
    </ul>
  </li>
  <li><a href="#rl-practice-guide" id="markdown-toc-rl-practice-guide">RL practice guide</a>    <ul>
      <li><a href="#kl-as-reward-penalty-no-gradient-needed" id="markdown-toc-kl-as-reward-penalty-no-gradient-needed">KL as reward penalty (no gradient needed)</a></li>
      <li><a href="#kl-as-loss-needs-gradients" id="markdown-toc-kl-as-loss-needs-gradients">KL as loss (needs gradients)</a>        <ul>
          <li><a href="#on-policy-optimize-reverse-kl-most-common" id="markdown-toc-on-policy-optimize-reverse-kl-most-common">On-policy: optimize reverse KL (most common)</a></li>
          <li><a href="#on-policy-optimize-forward-kl-coverage" id="markdown-toc-on-policy-optimize-forward-kl-coverage">On-policy: optimize forward KL (coverage)</a></li>
          <li><a href="#off-policy-optimize-reverse-kl" id="markdown-toc-off-policy-optimize-reverse-kl">Off-policy: optimize reverse KL</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#grab-and-use-crib-sheet" id="markdown-toc-grab-and-use-crib-sheet">“Grab-and-use” crib sheet</a></li>
  <li><a href="#common-implementation-traps" id="markdown-toc-common-implementation-traps">Common implementation traps</a></li>
  <li><a href="#summary-1" id="markdown-toc-summary-1">Summary</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<p><img src="/assets/img/kl-estimators/kl-estimator-en.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>How we approximate KL divergence directly affects training stability. This post systematically analyzes three estimators $k_1, k_2, k_3$ in both on-policy and off-policy scenarios, and gives practical guidelines for choosing them when KL is used as a reward penalty versus when it is used as a loss for backpropagation.</p>
</blockquote>

<p><a href="/reinforcement-learning/2025/12/01/kl-estimators-zh.html">中文版</a> | <a href="https://zhuanlan.zhihu.com/p/1978993413425763764">知乎版本 <img src="https://static.zhihu.com/heifetz/favicon.ico" alt="Zhihu" /></a></p>

<h2 id="introduction-what-kl-does-in-rl">Introduction: What KL Does in RL</h2>

<p>In policy optimization (PPO, GRPO, etc.) and alignment training (RLHF/RLAIF), <strong>KL penalty</strong> keeps the new policy from drifting too far from a reference policy, preventing instability or collapse. However, implementing KL penalty involves multiple layers of choices: <strong>which estimator</strong> ($k_1$, $k_2$, $k_3$), <strong>who to sample from</strong> (on-policy vs off-policy), and <strong>how to use it</strong> (as reward shaping or as a loss for backpropagation). This post systematically dissects these choices and their interrelationships.</p>

<h3 id="forward-vs-reverse-kl">Forward vs. reverse KL</h3>

<p>Let $q_\theta$ be the current actor, $p$ the reference policy. The two directions are:</p>

<p><strong>Reverse KL:</strong>
\(D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{x \sim q_\theta}\left[\log \frac{q_\theta(x)}{p(x)}\right]\)</p>

<figure style="text-align:center;">
	<img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:95%;max-width:100%;" />
	<figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Forward KL:</strong>
\(D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q_\theta(x)}\right]\)</p>

<figure style="text-align:center;">
	<img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:95%;max-width:100%;" />
	<figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Intuition:</strong></p>
<ul>
  <li><strong>Reverse KL</strong> is mode-seeking: policy concentrates on high-probability regions of $p$, possibly sacrificing diversity.</li>
  <li><strong>Forward KL</strong> is mass-covering: policy tries to cover the support of $p$.</li>
</ul>

<p>RLHF typically uses <strong>reverse KL</strong> because we want the actor not to move too far from the reference, not necessarily to cover every mode.</p>

<h2 id="three-estimators-definitions-and-design">Three estimators: definitions and design</h2>

<p>Let $r(x) = \dfrac{p(x)}{q_\theta(x)}$. John Schulman defined three single-sample estimators:</p>

<h3 id="k_1-the-naive-estimator">$k_1$: the naive estimator</h3>

\[k_1(x) = -\log r = \log q_\theta(x) - \log p(x)\]

<p>Direct log-ratio. It is unbiased for reverse KL, but <strong>can be negative</strong> while KL is always nonnegative, giving huge variance because positive and negative samples cancel.</p>

<h3 id="k_2-an-f-divergence-lower-variance">$k_2$: an f-divergence, lower variance</h3>

\[k_2(x) = \frac{1}{2}(\log r)^2\]

<p><strong>Motivation:</strong> $k_1$ can be positive or negative; $k_2$ squares it so <strong>every sample is positive</strong>, each telling you how far $p$ and $q$ differ.</p>

<p><strong>Why tiny bias?</strong> $k_2$ is an <strong>f-divergence</strong> with $f(x) = \tfrac{1}{2}(\log x)^2$. All smooth f-divergences have the same second-order expansion near $q \approx p$:</p>

\[D_f(p, q_\theta) = \frac{f^{\prime\prime}(1)}{2} \theta^T F \theta + O(\theta^3)\]

<p>KL corresponds to $f(x) = -\log x$, so $f^{\prime\prime}(1) = 1$. For $k_2$, $f^{\prime\prime}(1) = 1$ as well. <strong>When policies are close, $k_2$ tracks true KL almost identically</strong>, bias only appears in higher-order terms.</p>

<h3 id="k_3-control-variate-optimal-shape">$k_3$: control variate, “optimal” shape</h3>

\[k_3(x) = r - 1 - \log r\]

<p><strong>Motivation:</strong> we want <strong>unbiased and low variance</strong>. Add a <strong>control variate</strong> to $k_1$: something zero-mean and negatively correlated.</p>

<p>Because $\mathbb{E}_q[r - 1] = 1 - 1 = 0$, for any $\lambda$:</p>

\[k_1 + \lambda(r - 1) = -\log r + \lambda(r - 1)\]

<p>is still unbiased.</p>

<p><strong>Why $\lambda = 1$?</strong> By concavity of $\log$, $\log x \le x - 1$, so</p>

\[k_3 = (r - 1) - \log r \ge 0\]

<p>It is <strong>always nonnegative</strong>, avoiding the cancelation problem.</p>

<p><strong>Geometric view:</strong> $k_3$ is a <strong>Bregman divergence</strong> for $\phi(x) = -\log x$. Its tangent at $x=1$ is $y = 1 - x$, so</p>

\[\begin{aligned}
D_\phi(r, 1) &amp;= \phi(r) - \phi(1) - \phi'(1)(r - 1) \\
&amp;= -\log r - 0 - (-1)(r - 1) \\
&amp;= r - 1 - \log r = k_3.
\end{aligned}\]

<p>Convexity keeps $\phi$ above its tangent, so this gap is <strong>nonnegative</strong>. As $r \to 1$, the gap shrinks quadratically $(r-1)^2$, explaining the low variance when policies are close.</p>

<h3 id="quick-comparison">Quick comparison</h3>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
	<thead>
		<tr style="background-color: var(--global-bg-color);">
			<th style="text-align: center;">Estimator</th>
			<th style="text-align: center;">Definition</th>
			<th style="text-align: center;">Design idea</th>
			<th style="text-align: center;">Bias (value)</th>
			<th style="text-align: center;">Variance</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style="text-align: center;">$k_1$</td>
			<td style="text-align: center;">$\log r$</td>
			<td style="text-align: center;">Naive log-ratio</td>
			<td style="text-align: center;">Unbiased</td>
			<td style="text-align: center;">High (can be negative)</td>
		</tr>
		<tr>
			<td style="text-align: center;">$k_2$</td>
			<td style="text-align: center;">$\tfrac{1}{2}(\log r)^2$</td>
			<td style="text-align: center;">f-divergence, KL-matching 2nd order</td>
			<td style="text-align: center;">Biased (very small)</td>
			<td style="text-align: center;">Low (always positive)</td>
		</tr>
		<tr>
			<td style="text-align: center;">$k_3$</td>
			<td style="text-align: center;">$r - 1 - \log r$</td>
			<td style="text-align: center;">Control variate + Bregman</td>
			<td style="text-align: center;">Unbiased</td>
			<td style="text-align: center;">Low (always positive)</td>
		</tr>
	</tbody>
</table>
</div>

<p>For estimating the KL <strong>value</strong>, $k_3$ is “unbiased + low variance”; but as we’ll analyze, <strong>the gradient story is completely different</strong> — different estimators’ gradients may correspond to different optimization objectives. Moreover, whether KL is added to the reward for shaping or used as a loss for direct gradient backpropagation will fundamentally affect training behavior.</p>

<h2 id="core-analysis">Core analysis</h2>

<h3 id="bias-and-variance-for-kl-values">Bias and variance for KL values</h3>

<p>Assume samples from $q_\theta$ to estimate reverse KL $D_{\mathrm{KL}}(q_\theta | p)$.</p>

<p><strong>Unbiasedness:</strong></p>

\[\begin{aligned}
\mathbb{E}_{q}[k_1] &amp;= \mathbb{E}_{q}\left[\log \tfrac{q}{p}\right] = D_{\mathrm{KL}}(q \| p) \quad \textbf{(unbiased)}\\
\mathbb{E}_{q}[k_3] &amp;= \mathbb{E}_{q}[r - 1 - \log r] = 1 - 1 + D_{\mathrm{KL}}(q \| p) = D_{\mathrm{KL}}(q \| p) \quad \textbf{(unbiased)}\\
\mathbb{E}_{q}[k_2] &amp;= \tfrac{1}{2}\mathbb{E}_{q}[(\log r)^2] \neq D_{\mathrm{KL}}(q \| p) \quad \textbf{(biased)}
\end{aligned}\]

<p><strong>Variance trade-off:</strong></p>

<p>John Schulman’s toy experiments ($q = \mathcal{N}(0,1)$, $p = \mathcal{N}(0.1,1)$, true KL = 0.005):</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
	<thead>
		<tr style="background-color: var(--global-bg-color);">
			<th style="text-align: center;">Estimator</th>
			<th style="text-align: center;">bias/true</th>
			<th style="text-align: center;">stdev/true</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style="text-align: center;">$k_1$</td>
			<td style="text-align: center;">0</td>
			<td style="text-align: center;">20</td>
		</tr>
		<tr>
			<td style="text-align: center;">$k_2$</td>
			<td style="text-align: center;">0.002</td>
			<td style="text-align: center;">1.42</td>
		</tr>
		<tr>
			<td style="text-align: center;">$k_3$</td>
			<td style="text-align: center;">0</td>
			<td style="text-align: center;">1.42</td>
		</tr>
	</tbody>
</table>
</div>

<p>When KL is large ($p = \mathcal{N}(1,1)$, true KL = 0.5):</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
	<thead>
		<tr style="background-color: var(--global-bg-color);">
			<th style="text-align: center;">Estimator</th>
			<th style="text-align: center;">bias/true</th>
			<th style="text-align: center;">stdev/true</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style="text-align: center;">$k_1$</td>
			<td style="text-align: center;">0</td>
			<td style="text-align: center;">2</td>
		</tr>
		<tr>
			<td style="text-align: center;">$k_2$</td>
			<td style="text-align: center;">0.25</td>
			<td style="text-align: center;">1.73</td>
		</tr>
		<tr>
			<td style="text-align: center;">$k_3$</td>
			<td style="text-align: center;">0</td>
			<td style="text-align: center;">1.7</td>
		</tr>
	</tbody>
</table>
</div>

<p><strong>Intuition:</strong></p>
<ul>
  <li>$k_1 = -\log r$ is first-order around $r=1$, can be negative, so variance explodes when close.</li>
  <li>$k_3 = r - 1 - \log r$ is second-order near $r=1$ and always positive, so lower variance when close.</li>
  <li>When coverage is poor (heavy tails in $r$), $k_3$ can explode; then $k_1$ can be more stable.</li>
</ul>

<blockquote>
  <p><strong>Note:</strong> To estimate <strong>forward KL value</strong> $D_{\mathrm{KL}}(p | q) = \mathbb{E}_p[\log r]$ but only sample from $q$, use importance sampling $\mathbb{E}_q[r \log r]$.</p>
</blockquote>

<h3 id="gradient-estimation-the-crucial-distinction">Gradient estimation: the crucial distinction</h3>

<p>This is the easiest part to get wrong. First analyze <strong>on-policy</strong> (samples from $q_\theta$), then extend to <strong>off-policy</strong> (samples from behavior $\mu$).</p>

<h4 id="true-gradients-for-reference">True gradients for reference</h4>

<p>Let score function $s_\theta(x) = \nabla_\theta \log q_\theta(x)$, with key property $\mathbb{E}_{q_\theta}[s_\theta] = 0$.</p>

<p><strong>Reverse KL gradient:</strong></p>

\[D_{\mathrm{KL}}(q_\theta \| p) = \int q_\theta(x) \log \frac{q_\theta(x)}{p(x)} dx\]

<p>Product rule and $\nabla_\theta q_\theta = q_\theta s_\theta$, $\nabla_\theta \log p = 0$ give</p>

\[\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_q\left[s_\theta \log \tfrac{q_\theta}{p}\right] = -\mathbb{E}_q[s_\theta \log r].\]

<p><strong>Forward KL gradient:</strong></p>

\[D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \log \frac{p(x)}{q_\theta(x)} dx\]

<p>Since $p$ is $\theta$-independent,</p>

\[\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = -\mathbb{E}_p[s_\theta] = -\mathbb{E}_q[r s_\theta] = \mathbb{E}_q[(1-r) s_\theta].\]

<p>These baselines tell us what each estimator’s expected gradient really targets.</p>

<h4 id="two-differentiation-orders">Two differentiation orders</h4>

<p>1) <strong>Grad then expectation:</strong> autograd on each sample, then batch average (what DL code actually does).
2) <strong>Expectation then grad:</strong> treat $\mathbb{E}_q[k_i]$ as a function of $\theta$ and differentiate analytically.</p>

<p>Typical code does (1).</p>

<h4 id="gradients-of-the-three-estimators-on-policy">Gradients of the three estimators (on-policy)</h4>

\[\nabla_\theta k_1 = s_\theta\]

\[\nabla_\theta k_2 = (\log r) \nabla_\theta(\log r) = (\log r)(-s_\theta) = - (\log r) s_\theta\]

\[\nabla_\theta k_3 = (1 - r) s_\theta\]

<p>Taking expectation under $q_\theta$:</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
	<thead>
		<tr style="background-color: var(--global-bg-color);">
			<th style="text-align: center;">Estimator</th>
			<th style="text-align: center;">$\mathbb{E}_{q}[\nabla_\theta k_i]$</th>
			<th style="text-align: center;">Equals</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style="text-align: center;">$k_1$</td>
			<td style="text-align: center;">$\mathbb{E}_{q}[s_\theta] = 0$</td>
			<td style="text-align: center;"><strong>Zero (useless as loss)</strong></td>
		</tr>
		<tr>
			<td style="text-align: center;">$k_2$</td>
			<td style="text-align: center;">$-\mathbb{E}_{q}[(\log r) s_\theta] = \nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
			<td style="text-align: center;"><strong>Gradient of reverse KL</strong></td>
		</tr>
		<tr>
			<td style="text-align: center;">$k_3$</td>
			<td style="text-align: center;">$\mathbb{E}_{q}[(1-r) s_\theta] = \nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
			<td style="text-align: center;"><strong>Gradient of forward KL</strong></td>
		</tr>
	</tbody>
</table>
</div>

<p><strong>Key takeaways:</strong></p>
<ul>
  <li><strong>$k_2$ gradient</strong> matches reverse KL gradient (the usual “stay near ref” objective).</li>
  <li><strong>$k_3$ gradient</strong> matches forward KL gradient (coverage objective).</li>
  <li><strong>$k_1$ gradient expectation is zero</strong> — useless as a loss.</li>
</ul>

<h4 id="expectation-then-grad-vs-grad-then-expectation">Expectation-then-grad vs. grad-then-expectation</h4>

<p>If you first form $\mathbb{E}_q[k_i]$ and then differentiate (expectation-then-grad):</p>

\[\nabla_\theta \mathbb{E}_q[k_1] = \nabla_\theta D_{\mathrm{KL}}(q \| p), \quad \nabla_\theta \mathbb{E}_q[k_3] = \nabla_\theta D_{\mathrm{KL}}(q \| p).\]

<p>Both give reverse KL. But autograd on per-sample $k_3$ averages (grad-then-expectation) yields <strong>forward KL gradient</strong>. Same estimator, different order, different result.</p>

<h3 id="off-policy-gradients-with-importance-sampling">Off-policy gradients with importance sampling</h3>

<p>Real RL often samples from a behavior policy $\mu$ (old or mixed policy, replay buffer). To optimize <strong>reverse KL</strong> you need <strong>importance weights</strong>.</p>

<p>See also my earlier post: <a href="/reinforcement-learning/2025/11/15/three-policy-zh.html">Three-policy TRPO extension for LLM RL</a>.</p>

<h4 id="setup">Setup</h4>

<p>Define importance weight</p>

\[w(x) = \frac{q_\theta(x)}{\mu(x)}.\]

<p>Using batch loss $w(x) k_i(x)$ with autograd, what gradients do we get?</p>

<p>A key difference:</p>
<ul>
  <li>Previously expectations were under $q_\theta$ (depends on $\theta$).</li>
  <li>Now expectations are under $\mu$ (independent of $\theta$).</li>
</ul>

<h4 id="crucial-observation-the-two-orders-coincide">Crucial observation: the two orders coincide</h4>

<p>Because $\mu$ is $\theta$-independent,</p>

\[\nabla_\theta \mathbb{E}_{\mu}[f_\theta] = \mathbb{E}_{\mu}[\nabla_\theta f_\theta].\]

<p>So autograd on sample means (grad-then-expectation) equals expectation-then-grad. For $k_1$ and $k_3$, both value-unbiased for reverse KL, their gradient expectations also match reverse KL.</p>

<h4 id="value-unbiasedness-remains">Value unbiasedness remains</h4>

<p>By $\mathbb{E}_\mu[w f] = \mathbb{E}_q[f]$:</p>

\[\mathbb{E}_\mu[w k_1] = D_{\mathrm{KL}}(q_\theta \| p), \quad \mathbb{E}_\mu[w k_3] = D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{(unbiased)}\]

\[\mathbb{E}_\mu[w k_2] = \mathbb{E}_{q_\theta}[k_2] \neq D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{(biased)}\]

<h4 id="gradients-with-weights">Gradients with weights</h4>

<p>Gradient of weight: $\nabla_\theta w = w s_\theta$. Using product rule:</p>

<p>\(\nabla_\theta(w k_1) = w s_\theta (k_1 + 1)\)
\(\nabla_\theta(w k_2) = w s_\theta (k_2 - \log r)\)
\(\nabla_\theta(w k_3) = w s_\theta (k_3 + 1 - r) = w s_\theta k_1\)</p>

<p>Which give expected gradients:</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
	<thead>
		<tr style="background-color: var(--global-bg-color);">
			<th style="text-align: center;">Weighted estimator</th>
			<th style="text-align: center;">Value target</th>
			<th style="text-align: center;">Expected gradient</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style="text-align: center;">$\tfrac{q_\theta}{\mu} k_1$</td>
			<td style="text-align: center;">$D_{\mathrm{KL}}(q_\theta \| p)$</td>
			<td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$ (reverse KL) ✓</td>
		</tr>
		<tr>
			<td style="text-align: center;">$\tfrac{q_\theta}{\mu} k_2$</td>
			<td style="text-align: center;">$\mathbb{E}_q[k_2]$ (f-divergence)</td>
			<td style="text-align: center;">$\nabla_\theta \mathbb{E}_q[k_2]$, <strong>not</strong> reverse KL ✗</td>
		</tr>
		<tr>
			<td style="text-align: center;">$\text{sg}\left(\tfrac{q_\theta}{\mu}\right) k_2$</td>
			<td style="text-align: center;">$\mathbb{E}_q[k_2]$ (f-divergence)</td>
			<td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$ (reverse KL) ✓</td>
		</tr>
		<tr>
			<td style="text-align: center;">$\tfrac{q_\theta}{\mu} k_3$</td>
			<td style="text-align: center;">$D_{\mathrm{KL}}(q_\theta \| p)$</td>
			<td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$ (reverse KL) ✓</td>
		</tr>
	</tbody>
</table>
</div>

<p><strong>Interesting reversal vs. on-policy:</strong></p>
<ul>
  <li>On-policy: $k_2$ as loss gives reverse KL gradient; $k_1$ gradient is zero.</li>
  <li>Off-policy + weights: $\tfrac{q}{\mu}k_1$ and $\tfrac{q}{\mu}k_3$ give reverse KL gradients; $\tfrac{q}{\mu}k_2$ (with weight in grad) fails.</li>
  <li>Detaching the weight makes $\text{sg}(\tfrac{q}{\mu}) k_2$ also give reverse KL gradient.</li>
</ul>

<h4 id="variance-of-the-three-unbiased-off-policy-gradient-estimators">Variance of the three unbiased off-policy gradient estimators</h4>

<p>Unbiased reverse-KL gradient estimators (off-policy + IS):</p>

\[L_1 = w k_1, \quad L_2 = \bar w k_2, \quad L_3 = w k_3,\]

<p>With $w = \tfrac{q_\theta}{\mu}$, $\bar w = \mathrm{sg}(w)$. Using $\nabla_\theta w = w s_\theta$, $\nabla_\theta k_1 = s_\theta$, $\nabla_\theta k_2 = k_1 s_\theta$, $\nabla_\theta k_3 = (1-r) s_\theta$:</p>

\[\begin{aligned}
g_1 &amp;= w s_\theta (k_1+1),\\
g_2 &amp;= w s_\theta k_1,\\
g_3 &amp;= w s_\theta k_1.
\end{aligned}\]

<p>So <strong>$g_2 \equiv g_3$</strong>. Only two distinct variance behaviors: $g_1$ vs. $g_\star := g_2 = g_3$.</p>

<p>Let $A = w s_\theta, B = k_1$. Then</p>

\[g_1 = A(B+1), \quad g_\star = A B.\]

<p>Variance difference:</p>

\[\boxed{\mathrm{Var}_\mu(g_1) - \mathrm{Var}_\mu(g_\star) = \mathbb{E}_\mu[A^2(2B+1)]} = \mathbb{E}_\mu\big[w^2 s_\theta^2 (2k_1+1)\big].\]

<p>In the typical KL-penalty regime $q_\theta \approx p \approx \mu$, write $r = 1 + \varepsilon$, $\lvert\varepsilon\rvert \ll 1$, so $k_1 \approx -\varepsilon$, $2k_1+1 \approx 1 - 2\varepsilon &gt; 0$. Thus $\mathrm{Var}(g_1) &gt; \mathrm{Var}(g_\star)$.</p>

<p>Intuition:</p>
<ul>
  <li>$g_1$ includes an $O(1)$ zero-mean noise term $w s_\theta$.</li>
  <li>$g_\star$ cancels that term; remaining magnitude is $O(\varepsilon)$, giving much lower variance.</li>
</ul>

<p>Table summary:</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
	<thead>
		<tr style="background-color: var(--global-bg-color);">
			<th style="text-align: center; white-space: nowrap;">Estimator</th>
			<th style="text-align: center; white-space: nowrap;">Gradient rv</th>
			<th style="text-align: center; white-space: nowrap;">Scale ($r\approx1$)</th>
			<th style="text-align: center;">Variance</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style="text-align: center;">$w k_1$</td>
			<td style="text-align: center;">$w s_\theta (k_1+1)$</td>
			<td style="text-align: center;">$O(1)$</td>
			<td style="text-align: center;">High</td>
		</tr>
		<tr>
			<td style="text-align: center;">$\mathrm{sg}(w) k_2$</td>
			<td style="text-align: center;">$w s_\theta k_1$</td>
			<td style="text-align: center;">$O(\varepsilon)$</td>
			<td style="text-align: center;">Low</td>
		</tr>
		<tr>
			<td style="text-align: center;">$w k_3$</td>
			<td style="text-align: center;">$w s_\theta k_1$</td>
			<td style="text-align: center;">$O(\varepsilon)$</td>
			<td style="text-align: center;">Low</td>
		</tr>
	</tbody>
</table>
</div>

<p>Conclusion: off-policy IS with reverse-KL gradients has three unbiased options: $w k_1$, $\bar w k_2$, $w k_3$. The latter two are identical in gradient and variance and are preferred; $w k_1$ is unbiased but noisier.</p>

<p><strong>When far off-policy:</strong> If $w$ explodes (little overlap), any $\tfrac{q}{\mu}$ method suffers. Then the variance advantage of $k_3$ over $k_1$ is not guaranteed; clipping/regularization becomes necessary.</p>

<h3 id="gradient-cheat-sheet">Gradient cheat sheet</h3>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
	<thead>
		<tr style="background-color: var(--global-bg-color);">
			<th style="text-align: center; white-space: nowrap;">Sampling</th>
			<th style="text-align: center;">Loss</th>
			<th style="text-align: center;">$\mathbb{E}[\nabla_\theta \text{Loss}]$</th>
			<th style="text-align: center;">Optimizes</th>
			<th style="text-align: center; white-space: nowrap;">Right for reverse KL?</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style="text-align: center;">$q$ (on)</td>
			<td style="text-align: center;">$k_1$</td>
			<td style="text-align: center;">$\mathbb{E}_q[s_\theta] = 0$</td>
			<td style="text-align: center;">None (zero grad)</td>
			<td style="text-align: center;">✗</td>
		</tr>
		<tr>
			<td style="text-align: center;">$q$ (on)</td>
			<td style="text-align: center;">$k_2$</td>
			<td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
			<td style="text-align: center;"><strong>Reverse KL</strong></td>
			<td style="text-align: center;">✓</td>
		</tr>
		<tr>
			<td style="text-align: center;">$q$ (on)</td>
			<td style="text-align: center;">$k_3$</td>
			<td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
			<td style="text-align: center;">Forward KL</td>
			<td style="text-align: center;">✗</td>
		</tr>
		<tr>
			<td style="text-align: center;">$\mu$ (off)</td>
			<td style="text-align: center;">$\tfrac{q}{\mu} k_1$</td>
			<td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
			<td style="text-align: center;"><strong>Reverse KL</strong></td>
			<td style="text-align: center;">✓ (higher var)</td>
		</tr>
		<tr>
			<td style="text-align: center;">$\mu$ (off)</td>
			<td style="text-align: center;">$\tfrac{q}{\mu} k_2$</td>
			<td style="text-align: center;">$\nabla_\theta \mathbb{E}_q[k_2]$</td>
			<td style="text-align: center;">f-divergence (not KL)</td>
			<td style="text-align: center;">✗</td>
		</tr>
		<tr>
			<td style="text-align: center;">$\mu$ (off)</td>
			<td style="text-align: center;">$\text{sg}\left(\tfrac{q}{\mu}\right) k_2$</td>
			<td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
			<td style="text-align: center;"><strong>Reverse KL</strong></td>
			<td style="text-align: center;">✓</td>
		</tr>
		<tr>
			<td style="text-align: center;">$\mu$ (off)</td>
			<td style="text-align: center;">$\tfrac{q}{\mu} k_3$</td>
			<td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
			<td style="text-align: center;"><strong>Reverse KL</strong></td>
			<td style="text-align: center;">✓ (recommended, low var)</td>
		</tr>
	</tbody>
</table>
</div>

<p><strong>Key conclusions:</strong>
1) <strong>On-policy reverse KL:</strong> use $k_2$ (only correct choice).
2) <strong>Off-policy reverse KL:</strong> three correct options: $\tfrac{q}{\mu} k_1$ (unbiased, higher var); $\text{sg}(\tfrac{q}{\mu}) k_2$ (unbiased, equals next); $\tfrac{q}{\mu} k_3$ (unbiased, lower var; equals previous).
3) <strong>$\tfrac{q}{\mu} k_2$ with weight in grad is wrong</strong> for reverse KL.</p>

<p>However, before choosing an estimator, there’s a more fundamental question to answer: <strong>should KL be added to rewards, or be part of the loss?</strong> This choice fundamentally affects optimization behavior and credit assignment.</p>

<h2 id="two-ways-to-use-kl-as-reward-vs-as-loss">Two Ways to Use KL: As Reward vs. As Loss</h2>

<p>In practice, KL penalty can be used in two fundamentally different ways: added to rewards for shaping (no gradient backpropagation needed), or as part of the loss for backpropagation (gradient needed).</p>

<p>These two approaches may seem like just a <code class="language-plaintext highlighter-rouge">detach</code> difference in code, but they correspond to completely different optimization behaviors.</p>

<h3 id="definitions">Definitions</h3>

<p><strong>KL as Reward (stop-gradient):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kl</span> <span class="o">=</span> <span class="nf">compute_kl</span><span class="p">(</span><span class="n">log_prob_q</span><span class="p">,</span> <span class="n">log_prob_p</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">shaped_reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>
</code></pre></div></div>

<p>Use shaped reward for standard actor-critic updates.</p>

<p><strong>KL as Loss (backprop):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantage</span> <span class="o">*</span> <span class="n">log_prob</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>  <span class="c1"># kl participates in gradient
</span></code></pre></div></div>

<p>Critic only learns environment value; KL is a regularization term for the actor that backpropagates gradients.</p>

<h3 id="key-difference-1-optimization-target">Key Difference 1: Optimization Target</h3>

<p><strong>KL as Reward:</strong> Optimizes a <strong>regularized new MDP</strong> where the reward function becomes $\tilde{r}(s,a) = r(s,a) - \beta \cdot \text{KL}(s)$.</p>

<p><strong>KL as Loss:</strong> Optimizes the <strong>original task + supervised regularization</strong>; KL doesn’t change the MDP definition, it’s just an external constraint term.</p>

<p><strong>Intuition:</strong> The former “changes the game rules”; the latter “adds constraints under the original rules”.</p>

<h3 id="key-difference-2-actor-gradient">Key Difference 2: Actor Gradient</h3>

<p><strong>KL as Reward:</strong> Single policy gradient, KL influence is <strong>reflected indirectly through advantage</strong>:</p>

\[g_{\text{reward}} = \mathbb{E}\left[s_\theta \cdot \tilde{A}_t\right], \quad \tilde{A}_t \text{ based on } (r_t - \beta \cdot \text{KL}_t)\]

<p><strong>KL as Loss:</strong> Gradient splits into two independent paths:</p>

\[g_{\text{loss}} = \underbrace{\mathbb{E}\left[s_\theta \cdot A_t^{\text{env}}\right]}_{\text{RL gradient}} + \underbrace{\beta \cdot \mathbb{E}\left[\nabla_\theta \text{KL}_t\right]}_{\text{KL explicit gradient}}\]

<p><strong>Key distinction:</strong> Is KL’s force “multiplied on advantage” or “a separate force”? The latter’s KL gradient is deterministic, unaffected by critic quality.</p>

<h3 id="key-difference-3-critic-learning-target">Key Difference 3: Critic Learning Target</h3>

<p><strong>KL as Reward:</strong> Critic learns mixed value</p>

\[V^{\text{reg}}(s) = \mathbb{E}\left[\sum_t \gamma^t (r_t - \beta \cdot \text{KL}_t)\right]\]

<p><strong>KL as Loss:</strong> Critic only learns environment value</p>

\[V^{\text{env}}(s) = \mathbb{E}\left[\sum_t \gamma^t r_t\right]\]

<p>The latter has cleaner separation, making it easier to monitor task return and KL divergence separately.</p>

<h3 id="key-difference-4-credit-assignment">Key Difference 4: Credit Assignment</h3>

<p>Consider a scenario: first few steps are routing behavior, final step has high reward but also high KL.</p>

<p><strong>KL as Reward:</strong> The large KL at the terminal state is <strong>propagated back to all previous steps</strong> through TD, so the policy tends to <strong>fundamentally avoid</strong> high-KL regions — this is “planning-based KL budget allocation”.</p>

<p><strong>KL as Loss:</strong> The terminal state’s KL only appears in that state’s gradient term; the policy is still willing to <strong>visit high-reward regions but locally correct</strong> behavior.</p>

<h3 id="summary">Summary</h3>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Dimension</th>
      <th style="text-align: center;">KL as Reward (stop-grad)</th>
      <th style="text-align: center;">KL as Loss (backprop)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">Optimization target</td>
      <td style="text-align: center;">Regularized new MDP</td>
      <td style="text-align: center;">Original task + supervised regularization</td>
    </tr>
    <tr>
      <td style="text-align: center;">Actor gradient</td>
      <td style="text-align: center;">Single PG, based on shaped advantage</td>
      <td style="text-align: center;">RL gradient + explicit KL gradient</td>
    </tr>
    <tr>
      <td style="text-align: center;">Critic</td>
      <td style="text-align: center;">Learns $V^{\text{reg}}$: reward + KL mixed</td>
      <td style="text-align: center;">Learns $V^{\text{env}}$: only environment reward</td>
    </tr>
    <tr>
      <td style="text-align: center;">Credit Assignment</td>
      <td style="text-align: center;">Multi-step backprop, planning-capable</td>
      <td style="text-align: center;">Local per-state, no planning</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>One-liner:</strong> KL as reward makes the agent “plan to avoid high-KL paths” — constraints are more global and thorough; KL as loss makes the agent “visit but locally correct” — constraints are more local and flexible. The choice depends on whether you need cross-timestep KL budget allocation capability, and whether you want constraints to be “preventive” or “corrective”.</p>

<h2 id="rl-practice-guide">RL practice guide</h2>

<p>Combining the preceding analysis of “estimator mathematical properties” and “usage modes”, this section provides practical recommendations for specific scenarios.</p>

<h3 id="kl-as-reward-penalty-no-gradient-needed">KL as reward penalty (no gradient needed)</h3>

<p>When KL is a scalar penalty in rewards, we only need accurate <strong>values</strong>, no backprop. Refer to the earlier section on “Bias and variance for KL values”.</p>

<p><strong>Recommend:</strong></p>
<ul>
  <li>Use <strong>$k_1$</strong> or <strong>$k_3$</strong> (both unbiased for reverse KL value).</li>
  <li>When policies are close, $k_3$ is typically lower variance.</li>
  <li>With poor coverage or heavy tails, $k_1$ is more robust.</li>
  <li>Off-policy: multiply by $\tfrac{q_\theta}{\mu}$.</li>
</ul>

<blockquote>
  <p>For a <strong>forward KL penalty</strong>, use $\mathbb{E}_q[r \log r]$ or (if sampling from $p$) $\mathbb{E}_p[\log r]$.</p>
</blockquote>

<h3 id="kl-as-loss-needs-gradients">KL as loss (needs gradients)</h3>

<h4 id="on-policy-optimize-reverse-kl-most-common">On-policy: optimize reverse KL (most common)</h4>

<p>Goal: keep actor near reference.</p>

<p><strong>Use $k_2$ as loss.</strong></p>

\[\mathcal{L}_{k_2} = \tfrac{1}{2}(\log r)^2\]

<p>Then $\mathbb{E}_q[\nabla k_2] = \nabla_\theta D_{\mathrm{KL}}(q | p)$.</p>

<h4 id="on-policy-optimize-forward-kl-coverage">On-policy: optimize forward KL (coverage)</h4>

<p>Goal: cover the reference distribution (offline RL, imitation, etc.).</p>

<p><strong>Use $k_3$ as loss.</strong> Autograd on sample means gives $\mathbb{E}_q[(1-r) s_\theta] = \nabla_\theta D_{\mathrm{KL}}(p | q)$.</p>

<h4 id="off-policy-optimize-reverse-kl">Off-policy: optimize reverse KL</h4>

<p>Goal: samples from behavior $\mu$, still optimize reverse KL.</p>

<p><strong>Recommended:</strong> $\dfrac{q_\theta}{\mu} k_3$ or $\mathrm{sg}\left(\dfrac{q_\theta}{\mu}\right) k_2$ (identical gradients).</p>

\[\mathcal{L} = \dfrac{q_\theta(x)}{\mu(x)} \Big( \dfrac{p(x)}{q_\theta(x)} - 1 - \log \dfrac{p(x)}{q_\theta(x)} \Big)\]

<p>or</p>

\[\mathcal{L} = \mathrm{sg}\left(\dfrac{q_\theta(x)}{\mu(x)}\right) \cdot \tfrac{1}{2}\left(\log \dfrac{p(x)}{q_\theta(x)}\right)^2.\]

<ul>
  <li>Gradients are unbiased.</li>
  <li>When $q_\theta \approx p$, both have much lower variance.</li>
</ul>

<p><strong>Fallback:</strong> $\dfrac{q_\theta}{\mu} k_1$ (unbiased but higher variance).</p>

<p><strong>Avoid:</strong> $\dfrac{q_\theta}{\mu} k_2$ with weight in gradient — biased for reverse KL.</p>

<h2 id="grab-and-use-crib-sheet">“Grab-and-use” crib sheet</h2>

<p>The table below provides recommended estimator choices along three dimensions: “target KL direction” × “sampling source” × “usage mode”. “For <strong>value</strong>” corresponds to KL as reward penalty (no gradient needed); “For <strong>gradient</strong>” corresponds to KL as loss (gradient backpropagation needed).</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
	<thead>
		<tr style="background-color: var(--global-bg-color);">
			<th style="text-align: center;">Target</th>
			<th style="text-align: center;">Sampling</th>
			<th style="text-align: center;">For <strong>value</strong> (KL as Reward)</th>
			<th style="text-align: center;">For <strong>gradient</strong> (KL as Loss)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td style="text-align: center;">Reverse KL $D_{\mathrm{KL}}(q \| p)$</td>
			<td style="text-align: center;">$q$ (on-policy)</td>
			<td style="text-align: center;">$k_1$ or $k_3$ (unbiased)</td>
			<td style="text-align: center;">$k_2$</td>
		</tr>
		<tr>
			<td style="text-align: center;">Reverse KL $D_{\mathrm{KL}}(q \| p)$</td>
			<td style="text-align: center;">$\mu$ (off-policy)</td>
			<td style="text-align: center;">$\tfrac{q}{\mu} k_1$ or $\tfrac{q}{\mu} k_3$ (unbiased)</td>
			<td style="text-align: center;">$\tfrac{q}{\mu} k_3$ (recommended) or $\text{sg}(\tfrac{q}{\mu}) k_2$</td>
		</tr>
		<tr>
			<td style="text-align: center;">Forward KL $D_{\mathrm{KL}}(p \| q)$</td>
			<td style="text-align: center;">$q$</td>
			<td style="text-align: center;">$\mathbb{E}_q[r\log r]$</td>
			<td style="text-align: center;">$k_3$</td>
		</tr>
	</tbody>
</table>
</div>

<h2 id="common-implementation-traps">Common implementation traps</h2>

<p><strong>Trap 1: Using $k_1$ directly as loss (on-policy)</strong></p>

<p>When KL is used as a loss, $k_1$ gradient expectation is zero ($\mathbb{E}_q[s_\theta]=0$); as a loss it does nothing.</p>

<blockquote>
  <p><strong>Fix:</strong> First clarify the KL usage mode. For reward shaping (no gradient needed), both $k_1$ and $k_3$ work; for losses (gradient needed), use $k_2$ (reverse KL) or $k_3$ (forward KL) on-policy.</p>
</blockquote>

<p><strong>Trap 2: Mixing up $k_3$ value-unbiasedness vs. gradient target</strong></p>

<p>$k_3$ is value-unbiased for reverse KL, but its <strong>gradient</strong> is <strong>forward KL</strong>. If you want reverse KL and backprop $k_3$, you are actually optimizing forward KL.</p>

<blockquote>
  <p><strong>Fix:</strong> be explicit: reverse KL -&gt; $k_2$; forward KL -&gt; $k_3$.</p>
</blockquote>

<p><strong>Trap 3: Heavy-tailed $r$ blows up variance</strong></p>

<p>If $r = p/q$ has extreme values, $k_3$ variance can explode.</p>

<blockquote>
  <p><strong>Fix:</strong> enforce KL constraint or clip $r$.</p>
</blockquote>

<p><strong>Trap 4: Off-policy but still using $k_2$ or $\tfrac{q_\theta}{\mu} k_2$ (with grad on weight)</strong></p>

<p>If $\mu \neq q_\theta$:</p>
<ul>
  <li>Plain $k_2$ (no weight): expectation is under $\mu$, estimator fails.</li>
  <li>$\tfrac{q_\theta}{\mu} k_2$ with weight in grad: gradient is biased (f-divergence), not reverse KL.</li>
</ul>

<blockquote>
  <p><strong>Fix:</strong> off-policy reverse KL -&gt; use $\tfrac{q_\theta}{\mu} k_3$ (recommended), $\text{sg}(\tfrac{q_\theta}{\mu}) k_2$, or $\tfrac{q_\theta}{\mu} k_1$.</p>
</blockquote>

<p><strong>Trap 5: Wrong detach on importance weights</strong></p>

<p>$w = q_\theta / \mu$ often comes from <code class="language-plaintext highlighter-rouge">log_prob_q - log_prob_mu</code> then <code class="language-plaintext highlighter-rouge">exp</code>. Detaching $w$ matters:</p>

<ul>
  <li><strong>Using $k_1$ or $k_3$:</strong> $w$ <strong>must participate in gradient</strong> (do not detach), otherwise you drop $\nabla_\theta w = w s_\theta$ and get wrong gradients.</li>
  <li><strong>Using $k_2$:</strong> <strong>detach $w$</strong> to get reverse KL gradient. If $w$ stays in the graph, you get f-divergence gradient instead.</li>
</ul>

<blockquote>
  <p><strong>Summary:</strong> match estimator with the right detach strategy.</p>
</blockquote>

<h2 id="summary-1">Summary</h2>

<p><strong>One-liners:</strong></p>

<ul>
  <li><strong>Only value (reward penalty):</strong> use $k_1$ or $k_3$ (both unbiased for reverse KL value); off-policy multiply by $\tfrac{q_\theta}{\mu}$.</li>
  <li><strong>Need gradients (loss):</strong>
    <ul>
      <li><strong>On-policy:</strong> reverse KL -&gt; $k_2$; forward KL -&gt; $k_3$.</li>
      <li><strong>Off-policy:</strong> reverse KL -&gt; $\tfrac{q_\theta}{\mu} k_3$ or $\text{sg}(\tfrac{q_\theta}{\mu}) k_2$ (same gradient, low variance); fallback $\tfrac{q_\theta}{\mu} k_1$ (unbiased but noisier).</li>
    </ul>
  </li>
</ul>

<p>Keep three questions clear: <strong>who do we sample from, whose value do we estimate, whose gradient do we need?</strong> Especially note: <strong>on-policy vs. off-policy choose different estimators for reverse KL</strong> — on-policy use $k_2$, off-policy use $\tfrac{q_\theta}{\mu} k_3$ or $\text{sg}(\tfrac{q_\theta}{\mu}) k_2$.</p>

<p>Additionally, don’t forget to determine <strong>the KL usage mode</strong> before choosing an estimator:</p>
<ul>
  <li><strong>KL as reward:</strong> Constraints act on the policy indirectly through shaped advantage, with cross-timestep credit assignment capability; agent will “plan to avoid high-KL paths”</li>
  <li><strong>KL as loss:</strong> Constraints act on the policy directly as an independent gradient term; agent will “visit but locally correct”</li>
</ul>

<p>This choice is more fundamental than the estimator itself, depending on whether you want constraints to be “preventive” or “corrective”.</p>

<h2 id="references">References</h2>

<ol>
  <li>Dibya Ghosh. “KL Divergence for Machine Learning”. <a href="https://dibyaghosh.com/blog/probability/kldivergence">https://dibyaghosh.com/blog/probability/kldivergence</a></li>
  <li>John Schulman. “Approximating KL Divergence”. <a href="https://joschu.net/blog/kl-approx.html">https://joschu.net/blog/kl-approx.html</a></li>
  <li>Verl Documentation. “Proximal Policy Optimization (PPO)”. <a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">https://verl.readthedocs.io/en/latest/algo/ppo.html</a></li>
  <li>初七123334. “RLHF/RLVR 训练中的 KL 近似方法浅析（k1 / k2 / k3)”. <a href="https://zhuanlan.zhihu.com/p/1966872846212010437">https://zhuanlan.zhihu.com/p/1966872846212010437</a></li>
  <li>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. “Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization”. <a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></li>
  <li>Yifan Zhang, Yiping Ji, Gavin Brown, et al. “On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning”. <a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding {KL} Divergence Estimators in {RL}: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate.]]></summary></entry><entry xml:lang="zh"><title type="html">简单理解 RL 中的 KL 散度估计器：从数值估计到梯度估计</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-zh.html" rel="alternate" type="text/html" title="简单理解 RL 中的 KL 散度估计器：从数值估计到梯度估计" /><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-zh</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-zh.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#引言kl-散度在强化学习中的角色" id="markdown-toc-引言kl-散度在强化学习中的角色">引言：KL 散度在强化学习中的角色</a>    <ul>
      <li><a href="#正向-kl-与反向-kl-的区别" id="markdown-toc-正向-kl-与反向-kl-的区别">正向 KL 与反向 KL 的区别</a></li>
    </ul>
  </li>
  <li><a href="#三种估计器的定义与设计原理" id="markdown-toc-三种估计器的定义与设计原理">三种估计器的定义与设计原理</a>    <ul>
      <li><a href="#k_1最朴素的估计器" id="markdown-toc-k_1最朴素的估计器">$k_1$：最朴素的估计器</a></li>
      <li><a href="#k_2基于-f-散度的低方差估计器" id="markdown-toc-k_2基于-f-散度的低方差估计器">$k_2$：基于 f-散度的低方差估计器</a></li>
      <li><a href="#k_3控制变量法构造的最优估计器" id="markdown-toc-k_3控制变量法构造的最优估计器">$k_3$：控制变量法构造的「最优」估计器</a></li>
      <li><a href="#三者对比总结" id="markdown-toc-三者对比总结">三者对比总结</a></li>
    </ul>
  </li>
  <li><a href="#核心分析" id="markdown-toc-核心分析">核心分析</a>    <ul>
      <li><a href="#估计-kl-数值时的偏差与方差" id="markdown-toc-估计-kl-数值时的偏差与方差">估计 KL 数值时的偏差与方差</a></li>
      <li><a href="#估计-kl-梯度时的关键区分" id="markdown-toc-估计-kl-梯度时的关键区分">估计 KL 梯度时的关键区分</a>        <ul>
          <li><a href="#正向与反向-kl-真梯度的推导" id="markdown-toc-正向与反向-kl-真梯度的推导">正向与反向 KL 真梯度的推导</a></li>
          <li><a href="#两种求导顺序" id="markdown-toc-两种求导顺序">两种求导顺序</a></li>
          <li><a href="#三种估计器的梯度推导" id="markdown-toc-三种估计器的梯度推导">三种估计器的梯度推导</a></li>
          <li><a href="#先期望后梯度vs先梯度后期望" id="markdown-toc-先期望后梯度vs先梯度后期望">「先期望后梯度」vs「先梯度后期望」</a></li>
        </ul>
      </li>
      <li><a href="#扩展从行为策略-mu-采样时的-kl-梯度估计" id="markdown-toc-扩展从行为策略-mu-采样时的-kl-梯度估计">扩展：从行为策略 $\mu$ 采样时的 KL 梯度估计</a>        <ul>
          <li><a href="#设置与记号" id="markdown-toc-设置与记号">设置与记号</a></li>
          <li><a href="#关键观察两种求导顺序的等价性" id="markdown-toc-关键观察两种求导顺序的等价性">关键观察：两种求导顺序的等价性</a></li>
          <li><a href="#数值层面无偏性仍然保持" id="markdown-toc-数值层面无偏性仍然保持">数值层面：无偏性仍然保持</a></li>
          <li><a href="#梯度推导" id="markdown-toc-梯度推导">梯度推导</a></li>
          <li><a href="#哪些给出无偏的反向-kl-梯度" id="markdown-toc-哪些给出无偏的反向-kl-梯度">哪些给出无偏的反向 KL 梯度？</a></li>
          <li><a href="#三个无偏梯度估计器的方差对比" id="markdown-toc-三个无偏梯度估计器的方差对比">三个无偏梯度估计器的方差对比</a></li>
          <li><a href="#小结" id="markdown-toc-小结">小结</a></li>
        </ul>
      </li>
      <li><a href="#梯度估计总览" id="markdown-toc-梯度估计总览">梯度估计总览</a></li>
    </ul>
  </li>
  <li><a href="#kl-的两种使用方式作为-reward-vs-作为-loss" id="markdown-toc-kl-的两种使用方式作为-reward-vs-作为-loss">KL 的两种使用方式：作为 Reward vs 作为 Loss</a>    <ul>
      <li><a href="#两种用法的定义" id="markdown-toc-两种用法的定义">两种用法的定义</a></li>
      <li><a href="#核心差异一更新目标" id="markdown-toc-核心差异一更新目标">核心差异一：更新目标</a></li>
      <li><a href="#核心差异二actor-梯度" id="markdown-toc-核心差异二actor-梯度">核心差异二：Actor 梯度</a></li>
      <li><a href="#核心差异三critic-学习目标" id="markdown-toc-核心差异三critic-学习目标">核心差异三：Critic 学习目标</a></li>
      <li><a href="#核心差异四credit-assignment" id="markdown-toc-核心差异四credit-assignment">核心差异四：Credit Assignment</a></li>
      <li><a href="#小结-1" id="markdown-toc-小结-1">小结</a></li>
    </ul>
  </li>
  <li><a href="#rl-实践指南" id="markdown-toc-rl-实践指南">RL 实践指南</a>    <ul>
      <li><a href="#kl-作为-reward-惩罚不需要梯度" id="markdown-toc-kl-作为-reward-惩罚不需要梯度">KL 作为 Reward 惩罚（不需要梯度）</a></li>
      <li><a href="#kl-作为-loss需要梯度回传" id="markdown-toc-kl-作为-loss需要梯度回传">KL 作为 Loss（需要梯度回传）</a>        <ul>
          <li><a href="#on-policy优化反向-kl最常见场景" id="markdown-toc-on-policy优化反向-kl最常见场景">On-policy：优化反向 KL（最常见场景）</a></li>
          <li><a href="#on-policy优化正向-kl覆盖型场景" id="markdown-toc-on-policy优化正向-kl覆盖型场景">On-policy：优化正向 KL（覆盖型场景）</a></li>
          <li><a href="#off-policy优化反向-kl" id="markdown-toc-off-policy优化反向-kl">Off-policy：优化反向 KL</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#一份拿来就用的对照表" id="markdown-toc-一份拿来就用的对照表">一份「拿来就用」的对照表</a></li>
  <li><a href="#常见实现陷阱" id="markdown-toc-常见实现陷阱">常见实现陷阱</a></li>
  <li><a href="#总结" id="markdown-toc-总结">总结</a></li>
  <li><a href="#参考文献" id="markdown-toc-参考文献">参考文献</a></li>
</ul>

<p><img src="/assets/img/kl-estimators/kl-estimator-zh.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>在强化学习中，KL 散度的估计方式直接影响训练稳定性。本文系统剖析三种经典估计器 $k_1, k_2, k_3$ 在 on-policy 和 off-policy 场景的性质差异，并给出「用于 reward 惩罚」与「用于 loss 回传」时的选型指南。</p>
</blockquote>

<p><a href="/reinforcement-learning/2025/12/01/kl-estimators-en.html">English Version</a> | <a href="https://zhuanlan.zhihu.com/p/1978993413425763764">知乎版本 <img src="https://static.zhihu.com/heifetz/favicon.ico" alt="Zhihu" /></a></p>

<h2 id="引言kl-散度在强化学习中的角色">引言：KL 散度在强化学习中的角色</h2>

<p>在策略优化（PPO、GRPO 等）或对齐训练（RLHF/RLAIF）中，<strong>KL 惩罚</strong>是约束新策略不偏离参考策略的核心手段，用以防止训练不稳定或策略崩溃。然而，KL 惩罚的实现涉及多个层次的选择：<strong>用哪个估计器</strong>（$k_1$, $k_2$, $k_3$）、<strong>从谁采样</strong>（on-policy vs off-policy）、以及<strong>如何使用</strong>（作为 reward shaping 还是作为 loss 回传）。本文将系统地拆解这些选择及其相互关系。</p>

<h3 id="正向-kl-与反向-kl-的区别">正向 KL 与反向 KL 的区别</h3>

<p>设 $q_\theta$ 为当前 actor 策略，$p$ 为参考策略，两种方向的 KL 散度分别为：</p>

<p><strong>反向 KL（Reverse KL）</strong>：
\(D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{x \sim q_\theta}\left[\log \frac{q_\theta(x)}{p(x)}\right]\)</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:95%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>正向 KL（Forward KL）</strong>：
\(D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q_\theta(x)}\right]\)</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:95%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>直觉理解</strong>：</p>
<ul>
  <li><strong>反向 KL</strong> 倾向于「模式寻优」（mode-seeking）——策略会集中在参考分布的高概率区域，可能牺牲多样性</li>
  <li><strong>正向 KL</strong> 倾向于「质量覆盖」（mass-covering）——策略会尽量覆盖参考分布的支撑集</li>
</ul>

<p>在 RLHF 的主流实现中，<strong>反向 KL</strong> 更为常见，因为我们希望 actor 不要偏离 reference policy 太远，而非要求完全覆盖所有模式。</p>

<h2 id="三种估计器的定义与设计原理">三种估计器的定义与设计原理</h2>

<p>设比值 $r(x) = \frac{p(x)}{q_\theta(x)}$，John Schulman 提出的三种单样本估计子定义如下：</p>

<h3 id="k_1最朴素的估计器">$k_1$：最朴素的估计器</h3>

\[k_1(x) = -\log r = \log q_\theta(x) - \log p(x)\]

<p>这是最直接的定义——直接取 log-ratio 的负值。它对反向 KL 无偏，但有一个致命缺陷：<strong>可能取负值</strong>，而 KL 散度始终非负。这导致其方差极高，因为正负样本会相互抵消。</p>

<h3 id="k_2基于-f-散度的低方差估计器">$k_2$：基于 f-散度的低方差估计器</h3>

\[k_2(x) = \frac{1}{2}(\log r)^2\]

<p><strong>设计动机</strong>：$k_1$ 的问题在于可正可负，而 $k_2$ 通过取平方保证<strong>每个样本都是正的</strong>，直观上每个样本都在告诉你 $p$ 和 $q$ 相差多远。</p>

<p><strong>为什么偏差很小？</strong> $k_2$ 本质上是一个 <strong>f-散度</strong>（f-divergence），其中 $f(x) = \frac{1}{2}(\log x)^2$。f-散度有一个优美的性质：<strong>所有可微的 f-散度在 $q \approx p$ 时，二阶展开都形如</strong></p>

\[D_f(p, q_\theta) = \frac{f^{\prime\prime}(1)}{2} \theta^T F \theta + O(\theta^3)\]

<p>其中 $F$ 是 Fisher 信息矩阵。KL 散度对应 $f(x) = -\log x$，有 $f^{\prime\prime}(1) = 1$；而 $k_2$ 对应的 $f(x) = \frac{1}{2}(\log x)^2$，同样有 $f^{\prime\prime}(1) = 1$。这意味着<strong>当策略接近时，$k_2$ 与真实 KL 的行为几乎一致</strong>，偏差仅体现在高阶项。</p>

<h3 id="k_3控制变量法构造的最优估计器">$k_3$：控制变量法构造的「最优」估计器</h3>

\[k_3(x) = r - 1 - \log r\]

<p><strong>设计动机</strong>：我们想要一个<strong>既无偏又低方差</strong>的估计器。标准做法是给 $k_1$ 加一个<strong>控制变量</strong>（control variate）——一个期望为零但与 $k_1$ 负相关的量。</p>

<p>注意到 $\mathbb{E}_q[r - 1] = \mathbb{E}_q\left[\frac{p}{q}\right] - 1 = 1 - 1 = 0$，所以对于任意 $\lambda$，</p>

\[k_1 + \lambda(r - 1) = -\log r + \lambda(r - 1)\]

<p>仍然是无偏估计。</p>

<p><strong>为什么选 $\lambda = 1$？</strong> 由于 $\log$ 是凹函数，有 $\log x \leq x - 1$，因此</p>

\[k_3 = (r - 1) - \log r \geq 0\]

<p><strong>始终非负</strong>！这保证了每个样本都在「正向」贡献信息，消除了 $k_1$ 正负抵消的问题。</p>

<p><strong>几何直觉</strong>：$k_3$ 实际上是一个 <strong>Bregman 散度</strong>。考虑凸函数 $\phi(x) = -\log x$，它在 $x=1$ 处的切线为 $y = 1 - x$。Bregman 散度定义为「函数值与切线值之差」：</p>

\[\begin{aligned}
D_\phi(r, 1) &amp;= \phi(r) - \phi(1) - \phi'(1)(r - 1) \\
&amp;= -\log r - 0 - (-1)(r - 1) \\
&amp;= r - 1 - \log r \\
&amp;= k_3.
\end{aligned}\]

<p>由于凸函数始终位于其切线上方，这个差值<strong>天然非负</strong>。更重要的是，在 $r \to 1$ 时，函数与切线「贴合」得越来越紧，差值以 $(r-1)^2$ 的二阶速度趋近于零——这正是 $k_3$ 在策略接近时方差小的根本原因。</p>

<h3 id="三者对比总结">三者对比总结</h3>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">估计器</th>
      <th style="text-align: center;">定义</th>
      <th style="text-align: center;">设计原理</th>
      <th style="text-align: center;">对数值的偏差</th>
      <th style="text-align: center;">方差特性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">$\log r$</td>
      <td style="text-align: center;">最朴素定义</td>
      <td style="text-align: center;">无偏</td>
      <td style="text-align: center;">高（可正可负）</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">$\frac{1}{2}(\log r)^2$</td>
      <td style="text-align: center;">f-散度，二阶行为与 KL 一致</td>
      <td style="text-align: center;">有偏（但极小）</td>
      <td style="text-align: center;">低（恒正）</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">$r - 1 - \log r$</td>
      <td style="text-align: center;">控制变量 + Bregman 散度</td>
      <td style="text-align: center;">无偏</td>
      <td style="text-align: center;">低（恒正）</td>
    </tr>
  </tbody>
</table>
</div>

<p>从数值估计的角度看，$k_3$ 是「无偏 + 低方差」的最优选择；但正如后文将分析的，<strong>梯度层面的故事完全不同</strong>——不同估计器的梯度可能对应不同的优化目标。此外，KL 是加入 reward 做 shaping，还是作为 loss 直接回传梯度，也会根本性地影响训练行为。</p>

<h2 id="核心分析">核心分析</h2>

<h3 id="估计-kl-数值时的偏差与方差">估计 KL 数值时的偏差与方差</h3>

<p>假设从 $q_\theta$ 采样来估计反向 KL $D_{\mathrm{KL}}(q_\theta | p)$：</p>

<p><strong>无偏性分析</strong>：</p>

\[\begin{aligned}
\mathbb{E}_{q}[k_1] &amp;= \mathbb{E}_{q}\left[\log \frac{q}{p}\right] = D_{\mathrm{KL}}(q \| p) \quad \textbf{（无偏）}\\
\mathbb{E}_{q}[k_3] &amp;= \mathbb{E}_{q}[r - 1 - \log r] \\
&amp;= 1 - 1 + D_{\mathrm{KL}}(q \| p) \\
&amp;= D_{\mathrm{KL}}(q \| p) \quad \textbf{（无偏）}\\
\mathbb{E}_{q}[k_2] &amp;= \frac{1}{2}\mathbb{E}_{q}[(\log r)^2] \neq D_{\mathrm{KL}}(q \| p) \quad \textbf{（有偏）}
\end{aligned}\]

<p><strong>结论</strong>：对于估计反向 KL 的<strong>数值</strong>，$k_1$ 和 $k_3$ 是无偏估计，而 $k_2$ 是有偏的。</p>

<p><strong>方差特性的 Trade-off</strong>：</p>

<p>John Schulman 的实验（$q = \mathcal{N}(0,1)$，$p = \mathcal{N}(0.1,1)$，真实 KL = 0.005）表明：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">估计器</th>
      <th style="text-align: center;">bias/true</th>
      <th style="text-align: center;">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">20</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">0.002</td>
      <td style="text-align: center;">1.42</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">1.42</td>
    </tr>
  </tbody>
</table>
</div>

<p>当 KL 较大时（$p = \mathcal{N}(1,1)$，真实 KL = 0.5）：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">估计器</th>
      <th style="text-align: center;">bias/true</th>
      <th style="text-align: center;">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">2</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">0.25</td>
      <td style="text-align: center;">1.73</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">1.7</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>核心直觉</strong>：</p>
<ul>
  <li>$k_1 = -\log r$ 以一阶项起步，当 $r$ 接近 1 时波动较大，且可能取负值</li>
  <li>$k_3 = r - 1 - \log r$ 在 $r=1$ 处是二阶小量，始终非负，因此在策略接近时方差更小</li>
  <li>但当覆盖严重不足（$r$ 可能爆炸）时，$k_3$ 的方差会被权重爆炸拖累；此时 $k_1$ 反而更稳定</li>
</ul>

<blockquote>
  <p><strong>注</strong>：若要估计<strong>正向 KL 的数值</strong> $D_{\mathrm{KL}}(p | q) = \mathbb{E}_p[\log r]$，而只能从 $q$ 采样，可用重要性采样 $\mathbb{E}_q[r \log r]$。</p>
</blockquote>

<h3 id="估计-kl-梯度时的关键区分">估计 KL 梯度时的关键区分</h3>

<p><strong>这是最容易混淆、也是实践中最关键的部分。</strong> 本节先分析<strong>从 $q_\theta$ 采样</strong>（on-policy）的情形，后文将进一步讨论从行为策略 $\mu$ 采样（off-policy）时的变化。</p>

<h4 id="正向与反向-kl-真梯度的推导">正向与反向 KL 真梯度的推导</h4>

<p>在分析估计器之前，我们先推导正向和反向 KL 散度对 $\theta$ 的<strong>真梯度</strong>作为参照。</p>

<p>记 score function $s_\theta(x) = \nabla_\theta \log q_\theta(x)$，它有一个重要性质：$\mathbb{E}_{q_\theta}[s_\theta] = 0$（因为 $\int \nabla_\theta q_\theta dx = \nabla_\theta \int q_\theta dx = \nabla_\theta 1 = 0$）。</p>

<p><strong>反向 KL 的梯度</strong>：</p>

\[D_{\mathrm{KL}}(q_\theta \| p) = \int q_\theta(x) \log \frac{q_\theta(x)}{p(x)} dx\]

<p>对 $\theta$ 求梯度（使用乘积法则）：</p>

\[\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \int \nabla_\theta q_\theta \cdot \log \frac{q_\theta}{p} dx + \int q_\theta \cdot \nabla_\theta \log \frac{q_\theta}{p} dx\]

<p>利用 $\nabla_\theta q_\theta = q_\theta \cdot s_\theta$ 以及 $\nabla_\theta \log q_\theta = s_\theta$、$\nabla_\theta \log p = 0$：</p>

\[= \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] + \mathbb{E}_q[s_\theta] = \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right]\]

<p>即：</p>

\[\boxed{\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] = -\mathbb{E}_q[s_\theta \cdot \log r]}\]

<p><strong>正向 KL 的梯度</strong>：</p>

\[D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \log \frac{p(x)}{q_\theta(x)} dx\]

<p>由于 $p(x)$ 不依赖于 $\theta$：</p>

\[\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \cdot \nabla_\theta \left(-\log q_\theta(x)\right) dx = -\mathbb{E}_p[s_\theta]\]

<p>为了用 $q$ 的样本估计这个量，进行重要性采样：</p>

\[-\mathbb{E}_p[s_\theta] = -\mathbb{E}_q\left[\frac{p}{q_\theta} \cdot s_\theta\right] = -\mathbb{E}_q[r \cdot s_\theta]\]

<p>利用 $\mathbb{E}_q[s_\theta] = 0$，可改写为：</p>

\[\boxed{\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_q[(1-r) \cdot s_\theta]}\]

<p>有了这两个结果，我们就能判断各估计器的梯度期望究竟对应哪个 KL 的真梯度。</p>

<h4 id="两种求导顺序">两种求导顺序</h4>

<p>在代码实现中，存在两条路径：</p>

<ol>
  <li><strong>先梯度、后期望</strong>：对每个样本的 $k_i(x)$ 求梯度，再对梯度求期望（Monte Carlo 估计）</li>
  <li><strong>先期望、后梯度</strong>：把 $\mathbb{E}_q[k_i]$ 当作损失函数，对解析表达式求梯度</li>
</ol>

<p><strong>在典型的深度学习代码中，我们实际执行的是「先梯度、后期望」</strong>——自动微分对每个样本计算梯度，然后在 batch 上取平均。</p>

<h4 id="三种估计器的梯度推导">三种估计器的梯度推导</h4>

<p>现在我们计算三种估计器的梯度，看它们的期望分别对应哪个 KL 的真梯度。</p>

<p><strong>推导 $\nabla_\theta k_1$</strong>：</p>

\[k_1 = -\log r = -\log \frac{p(x)}{q_\theta(x)} = \log q_\theta(x) - \log p(x)\]

\[\nabla_\theta k_1 = \nabla_\theta \log q_\theta(x) - \nabla_\theta \log p(x) = s_\theta - 0 = s_\theta\]

<p><strong>推导 $\nabla_\theta k_2$</strong>：</p>

\[k_2 = \frac{1}{2}(\log r)^2\]

<p>由链式法则：</p>

\[\begin{aligned}
\nabla_\theta k_2 
&amp;= (\log r) \cdot \nabla_\theta(\log r) \\
&amp;= (\log r) \cdot \nabla_\theta(\log p(x) - \log q_\theta(x)) \\
&amp;= (\log r)(-s_\theta) \\
&amp;= - (\log r) s_\theta.
\end{aligned}\]

<p><strong>推导 $\nabla_\theta k_3$</strong>：</p>

\[k_3 = r - 1 - \log r\]

<p>首先计算 $\nabla_\theta r$。由于 $r = p(x) \cdot q_\theta(x)^{-1}$：</p>

\[\nabla_\theta r = p(x) \cdot (-1) \cdot q_\theta(x)^{-2} \cdot \nabla_\theta q_\theta(x) = -\frac{p(x)}{q_\theta(x)} \cdot \frac{\nabla_\theta q_\theta(x)}{q_\theta(x)} = -r \cdot s_\theta\]

<p>再计算 $\nabla_\theta \log r$：</p>

\[\nabla_\theta \log r = \frac{1}{r} \nabla_\theta r = \frac{1}{r} \cdot (-r \cdot s_\theta) = -s_\theta\]

<p>因此：</p>

\[\nabla_\theta k_3 = \nabla_\theta r - 0 - \nabla_\theta \log r = -r \cdot s_\theta - (-s_\theta) = (1 - r) \cdot s_\theta\]

<p>对它们在 $q_\theta$ 下取期望：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Estimator</th>
      <th style="text-align: center;">$\mathbb{E}_{q}[\nabla_\theta k_i]$</th>
      <th style="text-align: center;">Equals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">$\mathbb{E}_{q}[s_\theta] = 0$</td>
      <td style="text-align: center;"><strong>Zero (useless as loss)</strong></td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">$-\mathbb{E}_{q}[(\log r) \cdot s_\theta] = \nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>Gradient of reverse KL</strong></td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">$\mathbb{E}_{q}[(1-r) \cdot s_\theta] = \nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;"><strong>Gradient of forward KL</strong></td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>关键洞察</strong>：</p>
<ul>
  <li><strong>$k_2$ 的梯度</strong>等价于反向 KL 的真梯度——这是优化「约束策略不偏离 ref」的正确选择</li>
  <li><strong>$k_3$ 的梯度</strong>等价于正向 KL 的真梯度——这对应「覆盖型」目标</li>
  <li><strong>$k_1$ 的梯度期望恒为零</strong>——作为 loss 反传毫无意义！</li>
</ul>

<h4 id="先期望后梯度vs先梯度后期望">「先期望后梯度」vs「先梯度后期望」</h4>

<p>如果从解析角度把 $\mathbb{E}_q[k_i]$ 当作一个关于 $\theta$ 的函数再求梯度（即「先期望后梯度」），那么：</p>

\[\nabla_\theta \mathbb{E}_q[k_1] = \nabla_\theta D_{\mathrm{KL}}(q \| p)\]

\[\nabla_\theta \mathbb{E}_q[k_3] = \nabla_\theta D_{\mathrm{KL}}(q \| p)\]

<p>两者都给出反向 KL 的梯度。但在代码中直接对 $k_3$ 的样本均值调用反传时，自动微分执行的是「先梯度后期望」，得到的是 $\mathbb{E}_q[\nabla_\theta k_3]$，即<strong>正向 KL 的梯度</strong>。</p>

<p>这个区分非常重要：<strong>同一个估计器，两种求导顺序可能给出完全不同的结果</strong>。</p>

<h3 id="扩展从行为策略-mu-采样时的-kl-梯度估计">扩展：从行为策略 $\mu$ 采样时的 KL 梯度估计</h3>

<p>前面的分析都默认<strong>样本来自当前策略 $q_\theta$</strong>。然而在实际 RL 训练中，我们常常遇到这样的 off-policy 场景：</p>

<ul>
  <li>用旧策略或混合策略生成数据，再更新当前 actor $q_\theta$</li>
  <li>离线 RL / 经验回放中，样本分布固定为 $\mu$，而不是当前的 $q_\theta$</li>
</ul>

<p>这时，如果我们仍然希望优化<strong>反向 KL</strong> $D_{\mathrm{KL}}(q_\theta | p)$，就必须引入<strong>重要性权重</strong>。</p>

<p>关于大模型 off-policy 场景的深入分析，可以参考我之前的博客：<a href="/reinforcement-learning/2025/11/15/three-policy-zh.html">从两策略到三策略：LLM RL 中行为策略–参考策略不一致下的 TRPO 扩展</a>。</p>

<h4 id="设置与记号">设置与记号</h4>

<p>仍然沿用前文的记号，现在加入采样分布 $\mu(x)$，并定义<strong>重要性权重</strong></p>

\[w(x) = \frac{q_\theta(x)}{\mu(x)}\]

<p>当从 $x \sim \mu$ 采样时，用 $w(x) k_i(x)$ 的 batch 均值作为 loss，然后调用自动微分。那么三种估计器分别给出什么梯度？</p>

<p>一个关键差异是：</p>

<blockquote>
  <p><strong>以前</strong>的期望是 $\mathbb{E}_{q_{\theta}}[\cdot]$，分布本身依赖 $\theta$；
<strong>现在</strong>的期望是 $\mathbb{E}_{\mu}[\cdot]$，而 $\mu$ 与 $\theta$ 无关。</p>
</blockquote>

<p>这会让「先期望后梯度」与「先梯度后期望」的关系发生根本变化。</p>

<h4 id="关键观察两种求导顺序的等价性">关键观察：两种求导顺序的等价性</h4>

<p>因为 $\mu$ 与 $\theta$ 无关，对任何关于 $\theta$ 可微的函数 $f_\theta(x)$，有</p>

\[\nabla_\theta \mathbb{E}_{\mu}[f_\theta(x)] = \mathbb{E}_{\mu}[\nabla_\theta f_\theta(x)]\]

<p>换句话说，<strong>代码中对样本均值反传（先梯度后期望）就等价于对解析形式求梯度（先期望后梯度）</strong>，不会再像 on-policy 时那样分裂成两个不同的结果。</p>

<p><strong>所以在 off-policy + 重要性加权 的情形下，对反向 KL 数值无偏的估计器 $k_1$ 和 $k_3$，它们的梯度期望都将对应于反向 KL 的真梯度。</strong></p>

<p>这是与 on-policy 情形的根本区别。</p>

<h4 id="数值层面无偏性仍然保持">数值层面：无偏性仍然保持</h4>

<p>由标准的重要性采样关系 $\mathbb{E}_\mu[w \cdot f] = \mathbb{E}_{q_\theta}[f]$，有</p>

\[\mathbb{E}_\mu[w k_1] = D_{\mathrm{KL}}(q_\theta \| p), \quad
\mathbb{E}_\mu[w k_3] = D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{（无偏）}\]

\[\mathbb{E}_\mu[w k_2] = \mathbb{E}_{q_\theta}[k_2] \neq D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{（有偏）}\]

<p>这与 on-policy 情形完全一致。</p>

<h4 id="梯度推导">梯度推导</h4>

<p>首先计算重要性权重的梯度。由 $w = q_\theta / \mu$ 且 $\mu$ 不依赖 $\theta$：</p>

\[\nabla_\theta w(x) = w(x) s_\theta(x)\]

<p>结合前文已推导的 $\nabla_\theta k_i$，用乘积法则：</p>

<p><strong>$\nabla_\theta(w k_1)$</strong>：</p>

\[\nabla_\theta(w k_1) = (\nabla_\theta w) k_1 + w (\nabla_\theta k_1) = w s_\theta k_1 + w s_\theta = w s_\theta (k_1 + 1)\]

<p><strong>$\nabla_\theta(w k_2)$</strong>：</p>

\[\nabla_\theta(w k_2) = w s_\theta k_2 + w (-\log r) s_\theta = w s_\theta (k_2 - \log r)\]

<p><strong>$\nabla_\theta(w k_3)$</strong>：</p>

\[\nabla_\theta(w k_3) = w s_\theta k_3 + w (1-r) s_\theta = w s_\theta (k_3 + 1 - r)\]

<p>代入 $k_3 = r - 1 - \log r$：</p>

\[k_3 + 1 - r = (r - 1 - \log r) + 1 - r = -\log r = k_1\]

<p>因此有一个漂亮的简化：</p>

\[\boxed{\nabla_\theta(w k_3) = w s_\theta k_1 = -w s_\theta \log r}\]

<h4 id="哪些给出无偏的反向-kl-梯度">哪些给出无偏的反向 KL 梯度？</h4>

<p>利用 $\mathbb{E}_\mu[w \cdot f] = \mathbb{E}_{q_\theta}[f]$ 和 $\mathbb{E}_{q_\theta}[s_\theta] = 0$：</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(w k_1)]$</strong>：</p>

\[\mathbb{E}_\mu[w s_\theta (k_1 + 1)] = \mathbb{E}_{q}[s_\theta k_1] + \underbrace{\mathbb{E}_{q}[s_\theta]}_{=0} = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark\]

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(w k_2)]$</strong>：</p>

\[\mathbb{E}_\mu[w s_\theta (k_2 - \log r)] = \mathbb{E}_{q}[s_\theta (k_2 - \log r)] = \nabla_\theta \mathbb{E}_{q}[k_2]\]

<p>这是 $\mathbb{E}_q[k_2]$ 这个 f-散度的真梯度，<strong>不是</strong>反向 KL 的梯度。</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(\bar{w} k_2)]$</strong>（$\bar{w} = \text{sg}(w)$ 表示 detach）：</p>

<p>如果把重要性权重视为常数（在代码中 detach 掉），则：</p>

\[\nabla_\theta(\bar{w} k_2) = \bar{w} \cdot \nabla_\theta k_2 = \bar{w} \cdot (-\log r) s_\theta\]

<p>取期望：</p>

\[\mathbb{E}_\mu[\bar{w} \cdot (-\log r) s_\theta] = \mathbb{E}_{q}[(-\log r) s_\theta] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark\]

<p>这正是反向 KL 的真梯度！</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(w k_3)]$</strong>：</p>

\[\mathbb{E}_\mu[w s_\theta k_1] = \mathbb{E}_{q}[s_\theta k_1] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark\]

<p><strong>总结表格</strong>：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">加权估计器</th>
      <th style="text-align: center;">期望对应的目标</th>
      <th style="text-align: center;">梯度期望对应的真梯度</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$\frac{q_\theta}{\mu} k_1$</td>
      <td style="text-align: center;">$D_{\mathrm{KL}}(q_\theta \| p)$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$（反向 KL） ✓</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\frac{q_\theta}{\mu} k_2$</td>
      <td style="text-align: center;">$\mathbb{E}_q[k_2]$（f-散度）</td>
      <td style="text-align: center;">$\nabla_\theta \mathbb{E}_q[k_2]$，<strong>不是</strong>反向 KL ✗</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$</td>
      <td style="text-align: center;">$\mathbb{E}_q[k_2]$（f-散度）</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$（反向 KL） ✓</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\frac{q_\theta}{\mu} k_3$</td>
      <td style="text-align: center;">$D_{\mathrm{KL}}(q_\theta \| p)$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$（反向 KL） ✓</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>与 on-policy 情形的对比——一个有趣的反转</strong>：</p>

<ul>
  <li>On-policy 时，用 $k_2$ 做 loss 的梯度是反向 KL，而 $k_1$ 的梯度期望恒为零</li>
  <li>Off-policy + 重要性加权时，$\frac{q_\theta}{\mu} k_1$ 和 $\frac{q_\theta}{\mu} k_3$ 给出反向 KL 的真梯度，而 $\frac{q_\theta}{\mu} k_2$（权重参与梯度计算）<strong>不再适用</strong></li>
  <li>但如果把重要性权重 <strong>detach</strong> 掉，$\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$ 的梯度也是反向 KL 的真梯度</li>
</ul>

<h4 id="三个无偏梯度估计器的方差对比">三个无偏梯度估计器的方差对比</h4>

<p>前一小节我们看到，在 off-policy + 重要性采样的设置下，下面三个 loss 都给出<strong>反向 KL</strong> 的无偏梯度估计：</p>

\[L_1(x) = w(x) k_1(x),\qquad
L_2(x) = \bar w(x) k_2(x),\qquad
L_3(x) = w(x) k_3(x),\]

<p>其中 $w = \dfrac{q_\theta}{\mu}$，$\bar w = \mathrm{sg}(w)$ 表示对权重做 stop-gradient。它们对应的梯度随机变量为：</p>

\[g_1(x) := \nabla_\theta L_1(x),\quad
g_2(x) := \nabla_\theta L_2(x),\quad
g_3(x) := \nabla_\theta L_3(x).\]

<p>利用前文已推导的结果：</p>

<ul>
  <li>$\nabla_\theta w = w s_\theta$;</li>
  <li>$\nabla_\theta k_1 = s_\theta$;</li>
  <li>$\nabla_\theta k_2 = - (\log r) s_\theta = k_1 s_\theta$;</li>
  <li>$\nabla_\theta k_3 = (1-r) s_\theta$.</li>
</ul>

<p>有：</p>

\[\begin{aligned}
g_1(x)
&amp;= \nabla_\theta(w k_1)
= w s_\theta k_1 + w s_\theta
= w(x) s_\theta(x)\big(k_1(x)+1\big),\\
g_2(x)
&amp;= \nabla_\theta(\bar w k_2)
= \bar w \,\nabla_\theta k_2
= w \, k_1 s_\theta
= w(x) s_\theta(x) k_1(x),\\
g_3(x)
&amp;= \nabla_\theta(w k_3)
= w s_\theta k_3 + w(1-r)s_\theta
= w s_\theta (k_3 + 1 - r)
= w(x) s_\theta(x) k_1(x).
\end{aligned}\]

<p>最后一步用到了 $k_3 + 1 - r = (r - 1 - \log r) + 1 - r = -\log r = k_1$。于是出现了一个非常关键的事实：</p>

<blockquote>
  <p>在 off-policy + detach 权重的情况下，$\bar w k_2$ 与 $w k_3$ 的梯度完全一样：$g_2(x) \equiv g_3(x)$。</p>
</blockquote>

<p>换言之，三个 loss 实际上只对应<strong>两种</strong>不同的梯度随机变量：$g_1$ 与 $g_\star := g_2 = g_3$。</p>

<p>下面就比较这两种随机变量的方差。</p>

<p>为简化记号，令</p>

\[A(x) := w(x) s_\theta(x), \quad B(x) := k_1(x),\]

<p>则</p>

\[g_1 = A(B+1),\qquad g_\star = A B.\]

<p>两者的期望都等于 $\nabla_\theta D_{\mathrm{KL}}(q_\theta|p)$，因此有相同的均值项。展开方差定义并相减得到：</p>

\[\boxed{
\mathrm{Var}_\mu(g_1) - \mathrm{Var}_\mu(g_\star)
= \mathbb{E}_\mu\big[A^2((B+1)^2 - B^2)\big]
= \mathbb{E}_\mu\big[A^2 (2B+1)\big]
}\]

<p>也就是</p>

\[\mathrm{Var}_\mu(g_1) - \mathrm{Var}_\mu(g_\star)
= \mathbb{E}_\mu\Big[w(x)^2 s_\theta(x)^2 \big(2k_1(x)+1\big)\Big].\]

<p>在常见的 KL 惩罚 regime 下，$q_\theta \approx p \approx \mu$，取 $r(x)=1+\varepsilon(x)$，$\lvert \varepsilon\rvert \ll1$。此时 $k_1 = -\log r \approx -\varepsilon$，因此 $2k_1+1 \approx 1 - 2\varepsilon$，主导项为正的 $O(1)$ 常数。这意味着上式右侧近似为 $\mathbb{E}_\mu[w^2 s_\theta^2] &gt; 0$，从而 $\mathrm{Var}_\mu(g_1) &gt; \mathrm{Var}_\mu(g_\star)$。</p>

<p>更具体地，一阶近似</p>

\[k_1 \approx -\varepsilon,\quad k_1+1 \approx 1-\varepsilon.\]

<p>于是</p>

\[g_1(x) \approx w(x) s_\theta(x)(1 - \varepsilon(x)),\quad g_\star(x) \approx w(x) s_\theta(x)(-\varepsilon(x)).\]

<p>核心直觉：</p>

<ul>
  <li>$g_1$ 包含一个量级为 $O(1)$ 的零均值噪声项 $w s_\theta$，导致单样本方差较大；</li>
  <li>$g_\star$ 已把该常数噪声项消去，剩下与 $\varepsilon$ 成正比的一阶小量，方差为 $O(\varepsilon^2)$，显著更小。</li>
</ul>

<p>小结表格：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center; white-space: nowrap;">估计器</th>
      <th style="text-align: center; white-space: nowrap;">梯度随机变量</th>
      <th style="text-align: center; white-space: nowrap;">系数量级（$r\approx1$）</th>
      <th style="text-align: center;">方差</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$w k_1$</td>
      <td style="text-align: center;">$w s_\theta (k_1+1)$</td>
      <td style="text-align: center;">$O(1)$</td>
      <td style="text-align: center;">高</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mathrm{sg}(w) k_2$</td>
      <td style="text-align: center;">$w s_\theta k_1$</td>
      <td style="text-align: center;">$O(\varepsilon)$</td>
      <td style="text-align: center;">低</td>
    </tr>
    <tr>
      <td style="text-align: center;">$w k_3$</td>
      <td style="text-align: center;">$w s_\theta k_1$</td>
      <td style="text-align: center;">$O(\varepsilon)$</td>
      <td style="text-align: center;">低</td>
    </tr>
  </tbody>
</table>
</div>

<p>结论：在 off-policy + 重要性采样的设置下，给出反向 KL 真梯度的无偏估计器有三个：$w k_1,\; \bar w k_2,\; w k_3$。其中 $\bar w k_2$ 与 $w k_3$ 在梯度层面完全等价——同均值、同方差、同高阶矩；相比之下，$w k_1$ 的梯度多了一个零均值的常数噪声项 $w s_\theta$，在典型的 KL 惩罚 regime 下其方差大约高一个量级。</p>

<blockquote>
  <p>实践建议：若在 off-policy 场景下优化反向 KL，首选 $w k_3$ 或 $\mathrm{sg}(w) k_2$（两者梯度等价且方差低）；$w k_1$ 虽无偏但方差高，可作为备选并需配合 clipping/正则化。</p>
</blockquote>

<p><strong>极度 off-policy 时的警示</strong>：</p>

<p>当 $\mu$ 与 $q_\theta$ 差异很大——比如 $\mu$ 在 $q_\theta$ 的高密度区域几乎没有采样，或 $w = q_\theta / \mu$ 在尾部爆炸——任何基于 $\frac{q_\theta}{\mu}$ 的方法都会遭遇严重的方差问题。此时 $\frac{q_\theta}{\mu} k_3$（或 $\mathrm{sg}\left(\frac{q_\theta}{\mu}\right) k_2$）相对 $\frac{q_\theta}{\mu} k_1$ 的优势不再有理论保证，需要结合 clipping、正则化等策略综合处理。</p>

<p>不过，在 RL 实践中我们通常会控制 KL 约束、限制 off-policy 程度（比如使用近邻策略 $\mu = q_{\theta_\text{old}}$），在这个常见的 regime 里，可以相当有信心地说：</p>

<blockquote>
  <p><strong>如果已经决定用 off-policy + 重要性采样来优化反向 KL，推荐使用 $\dfrac{q_\theta}{\mu} k_3$ 或 $\mathrm{sg}\left(\dfrac{q_\theta}{\mu}\right) k_2$（两者梯度等价且方差低）；相较之下，$\dfrac{q_\theta}{\mu} k_1$ 方差更高。</strong></p>
</blockquote>

<p>这就是为什么 DeepSeek v3.2 技术报告中使用的是 $\frac{q_\theta}{\mu} k_3$ 作为 off-policy KL 惩罚的估计器。</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/dpsk-3d2-k3.png" style="width:95%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://arxiv.org/pdf/2512.02556v1">DeepSeek v3.2 技术报告 3.1 章节</a></figcaption>
</figure>

<h4 id="小结">小结</h4>

<ul>
  <li>从行为策略 $\mu$ 采样时，自然的 off-policy KL 估计为 $\frac{q_\theta}{\mu} k_i$。</li>
  <li><strong>数值上</strong>，$\frac{q_\theta}{\mu} k_1$ 与 $\frac{q_\theta}{\mu} k_3$ 仍然是反向 KL 的无偏估计。</li>
  <li><strong>梯度上</strong>，因为 $\mu$ 与 $\theta$ 无关，「先期望后梯度」与「先梯度后期望」等价：
    <ul>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\frac{q_\theta}{\mu} k_1)] = \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\frac{q_\theta}{\mu} k_3)] = \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\frac{q_\theta}{\mu} k_2)] \neq \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
    </ul>
  </li>
  <li><strong>方差上</strong>，$\frac{q_\theta}{\mu} k_3$ 与 $\mathrm{sg}\left(\frac{q_\theta}{\mu}\right) k_2$ 的梯度<strong>完全相同</strong>（两者都是 $w s_\theta k_1$），在统计性质上等价。相比之下，$\frac{q_\theta}{\mu} k_1$ 的梯度多了一个零均值噪声项 $w s_\theta$，在 $q_\theta \approx p \approx \mu$ 的典型场景下方差显著更高。</li>
</ul>

<h3 id="梯度估计总览">梯度估计总览</h3>

<p>下表汇总了 on-policy 与 off-policy 两种场景下，各估计器的梯度期望及其对应的优化目标：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center; white-space: nowrap;">采样来源</th>
      <th style="text-align: center;">Loss</th>
      <th style="text-align: center;">$\nabla_\theta$ Loss 的期望</th>
      <th style="text-align: center;">对应的优化目标</th>
      <th style="text-align: center; white-space: nowrap;">能否用于优化反向 KL？</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">$\mathbb{E}_q[s_\theta] = 0$</td>
      <td style="text-align: center;">无（梯度恒为零）</td>
      <td style="text-align: center;">✗</td>
    </tr>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>反向 KL</strong></td>
      <td style="text-align: center;">✓</td>
    </tr>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;">正向 KL</td>
      <td style="text-align: center;">✗</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_1$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>反向 KL</strong></td>
      <td style="text-align: center;">✓（但方差较高）</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_2$</td>
      <td style="text-align: center;">$\nabla_\theta \mathbb{E}_q[k_2]$</td>
      <td style="text-align: center;">f-散度（非 KL）</td>
      <td style="text-align: center;">✗</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\text{sg}\left(\frac{q}{\mu}\right) k_2$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>反向 KL</strong></td>
      <td style="text-align: center;">✓</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_3$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>反向 KL</strong></td>
      <td style="text-align: center;">✓（推荐，低方差）</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>关键结论</strong>：</p>

<ol>
  <li><strong>On-policy 优化反向 KL</strong>：唯一正确选择是 $k_2$</li>
  <li><strong>Off-policy 优化反向 KL</strong>：有三个正确选项：
    <ul>
      <li>$\frac{q}{\mu} k_1$：无偏但方差较高</li>
      <li>$\text{sg}\left(\frac{q}{\mu}\right) k_2$：无偏，与 $\frac{q}{\mu} k_3$ <strong>梯度完全等价</strong></li>
      <li>$\frac{q}{\mu} k_3$：无偏且方差更低（与上一项等价，均为推荐选择）</li>
    </ul>
  </li>
  <li><strong>$\frac{q}{\mu} k_2$（权重参与梯度）在 off-policy 下失效</strong>：这是一个容易被忽视的陷阱</li>
</ol>

<p>然而，在选定估计器之前，还有一个更基础的问题需要回答：<strong>KL 应该加进 reward 里，还是作为 loss 的一部分？</strong> 这一选择会从根本上影响优化行为和 credit assignment。</p>

<h2 id="kl-的两种使用方式作为-reward-vs-作为-loss">KL 的两种使用方式：作为 Reward vs 作为 Loss</h2>

<p>在实际实现中，KL 惩罚有两种截然不同的使用方式：加入 reward 进行 shaping（不需要回传梯度），或作为 loss 的一部分参与反传（需要梯度）。</p>

<p>这两种做法看似只是代码里一个 <code class="language-plaintext highlighter-rouge">detach</code> 的区别，实际上对应着截然不同的优化行为。</p>

<h3 id="两种用法的定义">两种用法的定义</h3>

<p><strong>KL 作为 Reward（stop-gradient）</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kl</span> <span class="o">=</span> <span class="nf">compute_kl</span><span class="p">(</span><span class="n">log_prob_q</span><span class="p">,</span> <span class="n">log_prob_p</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">shaped_reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>
</code></pre></div></div>

<p>用 shaped reward 做标准 actor-critic 更新。</p>

<p><strong>KL 作为 Loss（backprop）</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantage</span> <span class="o">*</span> <span class="n">log_prob</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>  <span class="c1"># kl 参与梯度计算
</span></code></pre></div></div>

<p>Critic 只学环境价值，KL 作为 actor 的正则项回传梯度。</p>

<h3 id="核心差异一更新目标">核心差异一：更新目标</h3>

<p><strong>KL 作为 Reward</strong>：优化一个<strong>正则化后的新 MDP</strong>，奖励函数变为 $\tilde{r}(s,a) = r(s,a) - \beta \cdot \text{KL}(s)$。</p>

<p><strong>KL 作为 Loss</strong>：优化<strong>原任务 + 监督正则</strong>，KL 不改变 MDP 定义，只是外挂的约束项。</p>

<p><strong>直觉</strong>：前者是「改变游戏规则」，后者是「在原规则下加约束」。</p>

<h3 id="核心差异二actor-梯度">核心差异二：Actor 梯度</h3>

<p><strong>KL 作为 Reward</strong>：单一 policy gradient，KL 的影响<strong>通过 advantage 间接体现</strong>：</p>

\[g_{\text{reward}} = \mathbb{E}\left[s_\theta \cdot \tilde{A}_t\right], \quad \tilde{A}_t \text{ 基于 } (r_t - \beta \cdot \text{KL}_t)\]

<p><strong>KL 作为 Loss</strong>：梯度分成两条独立路径：</p>

\[g_{\text{loss}} = \underbrace{\mathbb{E}\left[s_\theta \cdot A_t^{\text{env}}\right]}*{\text{RL 梯度}} + \underbrace{\beta \cdot \mathbb{E}\left[\nabla*\theta \text{KL}*t\right]}*{\text{KL 显式梯度}}\]

<p><strong>关键区别</strong>：KL 的力量是「乘在 advantage 上」还是「单独一股力」。后者的 KL 梯度是确定性的，不受 critic 质量影响。</p>

<h3 id="核心差异三critic-学习目标">核心差异三：Critic 学习目标</h3>

<p><strong>KL 作为 Reward</strong>：Critic 学混合价值</p>

\[V^{\text{reg}}(s) = \mathbb{E}\left[\sum_t \gamma^t (r_t - \beta \cdot \text{KL}_t)\right]\]

<p><strong>KL 作为 Loss</strong>：Critic 只学环境价值</p>

\[V^{\text{env}}(s) = \mathbb{E}\left[\sum_t \gamma^t r_t\right]\]

<p>后者分工更清晰，便于分别监控任务回报和 KL 散度。</p>

<h3 id="核心差异四credit-assignment">核心差异四：Credit Assignment</h3>

<p>考虑场景：前几步是路由行为，最后一步 reward 高但 KL 也大。</p>

<p><strong>KL 作为 Reward</strong>：末状态的大 KL 通过 TD <strong>回传到前面所有步骤</strong>，策略倾向于<strong>从根本上避开</strong>高 KL 区域——这是「规划性的 KL 预算分配」。</p>

<p><strong>KL 作为 Loss</strong>：末状态的 KL 只在该状态的梯度项里体现，策略仍愿意<strong>访问高回报区域，但局部修正</strong>行为。</p>

<h3 id="小结-1">小结</h3>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">维度</th>
      <th style="text-align: center;">KL 作为 Reward（stop-grad）</th>
      <th style="text-align: center;">KL 作为 Loss（backprop）</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">更新目标</td>
      <td style="text-align: center;">正则化后的新 MDP</td>
      <td style="text-align: center;">原任务 + 监督正则</td>
    </tr>
    <tr>
      <td style="text-align: center;">Actor 梯度</td>
      <td style="text-align: center;">单一 PG，基于 shaped advantage</td>
      <td style="text-align: center;">RL 梯度 + 显式 KL 梯度</td>
    </tr>
    <tr>
      <td style="text-align: center;">Critic</td>
      <td style="text-align: center;">学 $V^{\text{reg}}$：reward + KL 混合</td>
      <td style="text-align: center;">学 $V^{\text{env}}$：只看环境 reward</td>
    </tr>
    <tr>
      <td style="text-align: center;">Credit Assignment</td>
      <td style="text-align: center;">多步回传，有规划性</td>
      <td style="text-align: center;">局部 per-state，无规划性</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>一句话总结</strong>：KL 作为 reward 让 agent「规划性地避开高 KL 路径」，约束更全局、更彻底；KL 作为 loss 让 agent「访问但局部修正」，约束更局部、更灵活。选择取决于你是否需要跨时间步的 KL 预算分配能力，以及希望约束是「预防性」还是「修正性」的。</p>

<h2 id="rl-实践指南">RL 实践指南</h2>

<p>结合前面对「估计器数学性质」和「使用方式」的分析，本节给出具体场景下的实践建议。</p>

<h3 id="kl-作为-reward-惩罚不需要梯度">KL 作为 Reward 惩罚（不需要梯度）</h3>

<p>当 KL 仅作为标量惩罚加入 reward shaping 时，我们只需要准确的<strong>数值估计</strong>，不需要反传梯度。此时应参考前文「估计 KL 数值时的偏差与方差」中的分析。</p>

<p><strong>推荐</strong>：</p>
<ul>
  <li>使用 <strong>$k_1$</strong> 或 <strong>$k_3$</strong>（两者对反向 KL 数值均无偏）</li>
  <li>当策略已接近参考策略时，$k_3$ 往往更低方差</li>
  <li>覆盖不足或尾部错配明显时，$k_1$ 更稳健</li>
  <li>Off-policy 时加重要性权重 $\frac{q_\theta}{\mu}$ 即可</li>
</ul>

<blockquote>
  <p><strong>注</strong>：若想施加<strong>正向 KL 惩罚</strong>（偏向覆盖行为分布），数值上可用 $\mathbb{E}_q[r \log r]$ 或（若可从 $p$ 采样）$\mathbb{E}_p[\log r]$。</p>
</blockquote>

<h3 id="kl-作为-loss需要梯度回传">KL 作为 Loss（需要梯度回传）</h3>

<p>当 KL 作为 loss 的一部分参与反传时，必须考虑梯度的正确性。</p>

<h4 id="on-policy优化反向-kl最常见场景">On-policy：优化反向 KL（最常见场景）</h4>

<p>目标：控制 actor 不偏离 reference policy。</p>

<p><strong>正确做法</strong>：使用 <strong>$k_2$</strong> 作为 loss。</p>

\[\mathcal{L}_{k_2} = \frac{1}{2}(\log r)^2\]

<p>其梯度期望 $\mathbb{E}_q[\nabla k_2] = \nabla_\theta D_{\mathrm{KL}}(q | p)$ 正是反向 KL 的真梯度。</p>

<h4 id="on-policy优化正向-kl覆盖型场景">On-policy：优化正向 KL（覆盖型场景）</h4>

<p>目标：让策略覆盖参考分布的支撑集（如离线 RL、模仿学习等）。</p>

<p><strong>正确做法</strong>：使用 <strong>$k_3$</strong> 作为 loss。</p>

\[\mathbb{E}_q[\nabla k_3] = \mathbb{E}_q[(1-r) \cdot s_\theta] = \nabla_\theta D_{\mathrm{KL}}(p \| q)\]

<p>直接对 $k_3$ 的样本均值调用反传，自动微分计算的就是 $\mathbb{E}_q[\nabla_\theta k_3]$，即正向 KL 的梯度，无需额外处理。</p>

<h4 id="off-policy优化反向-kl">Off-policy：优化反向 KL</h4>

<p>目标：数据来自行为策略 $\mu$，仍希望优化反向 KL。</p>

<p><strong>推荐做法</strong>：使用 <strong>$\dfrac{q_\theta}{\mu} k_3$</strong> 或 <strong>$\mathrm{sg}\left(\dfrac{q_\theta}{\mu}\right) k_2$</strong> 作为 loss（两者梯度完全等价）。</p>

\[\mathcal{L} = \dfrac{q_\theta(x)}{\mu(x)} \cdot \left(\dfrac{p(x)}{q_\theta(x)} - 1 - \log \dfrac{p(x)}{q_\theta(x)}\right)\]

<p>或</p>

\[\mathcal{L} = \mathrm{sg}\left(\dfrac{q_\theta(x)}{\mu(x)}\right) \cdot \dfrac{1}{2}\left(\log \dfrac{p(x)}{q_\theta(x)}\right)^2\]

<ul>
  <li>梯度无偏</li>
  <li>在 $q_\theta \approx p$ 时方差都显著更低</li>
</ul>

<p><strong>备选方案</strong>：使用 $\dfrac{q_\theta}{\mu} k_1$（梯度同样无偏，但方差更高）</p>

<p><strong>避免</strong>：使用 $\dfrac{q_\theta}{\mu} k_2$（权重参与梯度计算）——梯度有偏，不是反向 KL 的正确方向</p>

<h2 id="一份拿来就用的对照表">一份「拿来就用」的对照表</h2>

<p>下表按「目标 KL 方向」×「采样来源」×「使用方式」三个维度给出推荐的估计器选择。其中「用于<strong>数值</strong>」对应 KL 作为 reward 惩罚（不需要梯度），「用于<strong>梯度</strong>」对应 KL 作为 loss（需要反传梯度）。</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">目标</th>
      <th style="text-align: center;">采样来源</th>
      <th style="text-align: center;">用于<strong>数值</strong>（KL 作为 Reward）</th>
      <th style="text-align: center;">用于<strong>梯度</strong>（KL 作为 Loss）</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">反向 KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;">$q$（on-policy）</td>
      <td style="text-align: center;">$k_1$ 或 $k_3$（无偏）</td>
      <td style="text-align: center;">$k_2$</td>
    </tr>
    <tr>
      <td style="text-align: center;">反向 KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;">$\mu$（off-policy）</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_1$ 或 $\frac{q}{\mu} k_3$（无偏）</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_3$（推荐）或 $\text{sg}\left(\frac{q}{\mu}\right) k_2$</td>
    </tr>
    <tr>
      <td style="text-align: center;">正向 KL $D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;">$q$</td>
      <td style="text-align: center;">$\mathbb{E}_q[r\log r]$</td>
      <td style="text-align: center;">$k_3$</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="常见实现陷阱">常见实现陷阱</h2>

<p><strong>陷阱 1：把 $k_1$ 直接当 loss 反传（on-policy）</strong></p>

<p>当 KL 作为 loss 使用时，$k_1$ 的梯度期望恒为零（$\mathbb{E}_q[\nabla k_1] = \mathbb{E}_q[s_\theta] = 0$），作为 loss 完全无效。</p>

<blockquote>
  <p><strong>解决</strong>：首先明确 KL 的使用方式。如果是 reward shaping（不需要梯度），用 $k_1$ 或 $k_3$ 均可；如果是 loss（需要梯度），on-policy 下用 $k_2$（反向 KL）或 $k_3$（正向 KL）。</p>
</blockquote>

<p><strong>陷阱 2：混淆 $k_3$ 的「数值无偏性」与「梯度对应的目标」</strong></p>

<p>$k_3$ 对<strong>反向 KL 的数值</strong>是无偏估计，但它的<strong>梯度</strong>对应的是<strong>正向 KL</strong>。如果你的目标是优化反向 KL，却用 $k_3$ 作为 loss，实际上在优化正向 KL。</p>

<blockquote>
  <p><strong>解决</strong>：明确你的优化目标。优化反向 KL 用 $k_2$；优化正向 KL 才用 $k_3$。</p>
</blockquote>

<p><strong>陷阱 3：$r$ 重尾导致方差爆炸</strong></p>

<p>当策略与参考分布差异过大时，$r = p/q$ 可能出现极端值，导致 $k_3$ 的方差爆炸。</p>

<blockquote>
  <p><strong>解决</strong>：控制 KL 约束，或对 $r$ 进行 clipping。</p>
</blockquote>

<p><strong>陷阱 4：离策略下仍用 $k_2$ 或 $\frac{q_\theta}{\mu} k_2$（权重参与梯度）</strong></p>

<p>在 on-policy 下，$k_2$ 是优化反向 KL 的正确选择。但如果数据来自 $\mu \neq q_\theta$：</p>
<ul>
  <li>直接用 $k_2$（不加权）：期望不是在 $q_\theta$ 下取的，估计器完全失效</li>
  <li>用 $\frac{q_\theta}{\mu} k_2$（权重参与梯度计算）：梯度有偏，不是反向 KL 的真梯度</li>
</ul>

<blockquote>
  <p><strong>解决</strong>：离策略场景下，改用 $\frac{q_\theta}{\mu} k_3$（推荐）、$\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$（将权重 detach）或 $\frac{q_\theta}{\mu} k_1$。</p>
</blockquote>

<p><strong>陷阱 5：对重要性权重的 detach 处理不当</strong></p>

<p>代码实现中，$w = q_\theta / \mu$ 通常通过 <code class="language-plaintext highlighter-rouge">log_prob_q - log_prob_mu</code> 再取 <code class="language-plaintext highlighter-rouge">exp</code> 计算得到。是否对 $w$ 进行 detach 会导致完全不同的结果：</p>

<ul>
  <li><strong>使用 $k_1$ 或 $k_3$ 时</strong>：$w$ <strong>应该参与梯度计算</strong>（不要 detach），否则会丢失 $\nabla_\theta w = w s_\theta$ 这一项，导致梯度错误</li>
  <li><strong>使用 $k_2$ 时</strong>：$w$ <strong>应该被 detach</strong>，这样才能得到反向 KL 的真梯度。如果让 $w$ 参与梯度计算，得到的是 f-散度的梯度，不是反向 KL</li>
</ul>

<blockquote>
  <p><strong>总结</strong>：选择不同的估计器时，要注意匹配正确的 detach 策略。</p>
</blockquote>

<h2 id="总结">总结</h2>

<p><strong>一句话记忆</strong>：</p>

<ul>
  <li><strong>只要数值（KL 作为 reward 惩罚）</strong>：选 $k_1$ 或 $k_3$（均对反向 KL 无偏）；off-policy 时加重要性权重即可</li>
  <li><strong>需要梯度（KL 作为 loss）</strong>：
    <ul>
      <li><strong>On-policy</strong>：优化反向 KL → 用 $k_2$；优化正向 KL → 用 $k_3$</li>
      <li><strong>Off-policy</strong>：优化反向 KL → 用 $\frac{q_\theta}{\mu} k_3$ 或 $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$（两者梯度等价，均为无偏 + 低方差的推荐选择）</li>
    </ul>
  </li>
</ul>

<p>把「<strong>从谁采样</strong>」、「<strong>估计谁的值</strong>」、「<strong>对谁求梯度</strong>」这三个问题捋清楚，三种估计器就不再让人混淆了。特别注意：<strong>on-policy 和 off-policy 下，优化反向 KL 的正确选择是不同的</strong>——前者用 $k_2$，后者用 $\frac{q_\theta}{\mu} k_3$ 或 $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$。</p>

<p>此外，别忘了在选择估计器之前先确定 <strong>KL 的使用方式</strong>：</p>
<ul>
  <li><strong>KL 作为 reward</strong>：约束通过 shaped advantage 间接作用于策略，具有跨时间步的 credit assignment 能力，agent 会「规划性地避开高 KL 路径」</li>
  <li><strong>KL 作为 loss</strong>：约束作为独立梯度项直接作用于策略，agent 会「访问但局部修正」</li>
</ul>

<p>这一选择比估计器本身更根本，取决于你希望约束是「预防性」还是「修正性」的。</p>

<h2 id="参考文献">参考文献</h2>

<ol>
  <li>
    <p>Dibya Ghosh. “KL Divergence for Machine Learning”. <a href="https://dibyaghosh.com/blog/probability/kldivergence">https://dibyaghosh.com/blog/probability/kldivergence</a></p>
  </li>
  <li>
    <p>John Schulman. “Approximating KL Divergence”. <a href="https://joschu.net/blog/kl-approx.html">https://joschu.net/blog/kl-approx.html</a></p>
  </li>
  <li>
    <p>Verl Documentation. “Proximal Policy Optimization (PPO)”. <a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">https://verl.readthedocs.io/en/latest/algo/ppo.html</a></p>
  </li>
  <li>
    <p>初七123334. RLHF/RLVR 训练中的 KL 近似方法浅析（k1 / k2 / k3）. <a href="https://zhuanlan.zhihu.com/p/1966872846212010437">https://zhuanlan.zhihu.com/p/1966872846212010437</a></p>
  </li>
  <li>
    <p>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. “Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization”. <a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></p>
  </li>
  <li>
    <p>Yifan Zhang, Yiping Ji, Gavin Brown, et al. “On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning”. <a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding {KL} Divergence Estimators in {RL}: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[在强化学习中，KL 散度的估计方式直接影响训练稳定性。本文系统剖析三种经典估计器 k1, k2, k3 的性质差异，涵盖 on-policy 与 off-policy 两种场景，并给出「用于 reward 惩罚」与「用于 loss 回传」时的选型指南。]]></summary></entry><entry xml:lang="en"><title type="html">From Two Policies to Three: Extending TRPO under Behavior–Reference Policy Mismatch in LLM RL</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html" rel="alternate" type="text/html" title="From Two Policies to Three: Extending TRPO under Behavior–Reference Policy Mismatch in LLM RL" /><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#traininginference-mismatch-and-asynchronous-frameworks" id="markdown-toc-traininginference-mismatch-and-asynchronous-frameworks">Training–Inference Mismatch and Asynchronous Frameworks</a></li>
  <li><a href="#related-work" id="markdown-toc-related-work">Related Work</a></li>
  <li><a href="#a-minimally-unified-view-from-a-three-policy-trpo-perspective" id="markdown-toc-a-minimally-unified-view-from-a-three-policy-trpo-perspective">A Minimally Unified View from a Three-Policy TRPO Perspective</a>    <ul>
      <li><a href="#three-policies" id="markdown-toc-three-policies">Three Policies</a></li>
      <li><a href="#two-policy-trpo" id="markdown-toc-two-policy-trpo">Two-Policy TRPO</a></li>
      <li><a href="#three-policy-trpo" id="markdown-toc-three-policy-trpo">Three-Policy TRPO</a></li>
      <li><a href="#how-to-control-these-two-deviations-in-practice" id="markdown-toc-how-to-control-these-two-deviations-in-practice">How to Control These Two Deviations in Practice?</a></li>
    </ul>
  </li>
  <li><a href="#importance-sampling-and-masking-four-implementations-of-constraint-2" id="markdown-toc-importance-sampling-and-masking-four-implementations-of-constraint-2">Importance Sampling and Masking: Four Implementations of Constraint 2</a>    <ul>
      <li><a href="#1-tis-token-level-truncated-importance-sampling" id="markdown-toc-1-tis-token-level-truncated-importance-sampling">1. TIS: Token-Level Truncated Importance Sampling</a></li>
      <li><a href="#2-icepop-token-level-two-sided-masking-in-moe" id="markdown-toc-2-icepop-token-level-two-sided-masking-in-moe">2. IcePop: Token-Level Two-Sided Masking in MoE</a></li>
      <li><a href="#3-sequence-level-mis-masked-importance-sampling-over-entire-sequences" id="markdown-toc-3-sequence-level-mis-masked-importance-sampling-over-entire-sequences">3. Sequence-Level MIS: Masked Importance Sampling Over Entire Sequences</a></li>
      <li><a href="#4-worst-token-reject-sampling-rejecting-entire-sequences-based-on-the-worst-token" id="markdown-toc-4-worst-token-reject-sampling-rejecting-entire-sequences-based-on-the-worst-token">4. Worst Token Reject Sampling: Rejecting Entire Sequences Based on the Worst Token</a></li>
    </ul>
  </li>
  <li><a href="#moe-routing-replay-what-does-it-actually-do-in-three-policy-trpo" id="markdown-toc-moe-routing-replay-what-does-it-actually-do-in-three-policy-trpo">MoE Routing Replay: What Does It Actually Do in Three-Policy TRPO?</a>    <ul>
      <li><a href="#surrogate-objective-in-moe-separating-routing-and-token-generation" id="markdown-toc-surrogate-objective-in-moe-separating-routing-and-token-generation">Surrogate Objective in MoE: Separating Routing and Token Generation</a></li>
      <li><a href="#1-replaying-behavior-policy-routing-behavior-router-replay--r3-style" id="markdown-toc-1-replaying-behavior-policy-routing-behavior-router-replay--r3-style">(1) Replaying Behavior-Policy Routing (Behavior-Router Replay / R3-Style)</a></li>
      <li><a href="#2-replaying-reference-policy-routing-reference-router-replay" id="markdown-toc-2-replaying-reference-policy-routing-reference-router-replay">(2) Replaying Reference-Policy Routing (Reference-Router Replay)</a></li>
      <li><a href="#routing-replay-as-a-change-of-surrogate-objective" id="markdown-toc-routing-replay-as-a-change-of-surrogate-objective">Routing Replay as a Change of Surrogate Objective</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

<p><a href="/reinforcement-learning/2025/11/15/three-policy-zh.html">中文版本</a> | <a href="https://zhuanlan.zhihu.com/p/1973206684907365344">知乎版本 <img src="https://static.zhihu.com/heifetz/favicon.ico" alt="Zhihu" /></a></p>

<p><img src="/assets/img/three-policy/three-policy-mini-class-en.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<h2 id="traininginference-mismatch-and-asynchronous-frameworks">Training–Inference Mismatch and Asynchronous Frameworks</h2>

<p>Recently I’ve seen quite a lot of discussion around <em>training–inference mismatch</em> and <em>asynchronous RL frameworks</em> for large language models. My intuition is that many of these seemingly diverse and complicated issues are, in fact, manifestations of a more fundamental tension: a mismatch between the <strong>behavior policy</strong> and the <strong>reference policy</strong>.</p>

<p>In this post, I’ll first briefly summarize the related work I’ve come across, and then try to connect them through the lens of “behavior policy vs. reference policy,” as a complementary way to look at the problem.</p>

<p>Throughout the post I’ll use:</p>

<ul>
  <li>
    <p><strong>Behavior policy</strong> $\mu$: the policy that <em>actually</em> generates rollouts, i.e., “under which distribution your data are sampled.” In modern LLM RL systems this typically corresponds to the implementation inside the inference engine (vLLM, SGLang, etc.), and under asynchronous frameworks it is often a <strong>mixture distribution over multiple worker policies</strong>.</p>
  </li>
  <li>
    <p><strong>Reference policy</strong> $\pi_{\theta_{\text{old}}}$: the policy used in the training objective for importance sampling, clipping, or KL constraints — typically the “old policy” in PPO / GRPO.</p>
  </li>
  <li>
    <p><strong>Target policy</strong> $\pi_\theta$: the policy we optimize in the training objective, i.e., “what we want the model to become” — typically the “new policy” in PPO / GRPO.</p>
  </li>
</ul>

<p>In the classical idealized setup, we usually <strong>implicitly assume</strong> $\mu = \pi_{\theta_{\text{old}}}$. In real systems, however, asynchronous updates, different inference / training backends, MoE routing fluctuations, and even hardware-level numerical differences cause these two policies to deviate to varying degrees.</p>

<h2 id="related-work">Related Work</h2>

<p>Below is a rough timeline of the works that left a strong impression on me (this is only a partial and biased subset of the literature I’ve seen):</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2110.00641">Decoupled PPO</a> was among the first to point out that in trust-region policy optimization methods (TRPO and PPO), the “old policy” actually plays two distinct roles:
    <ol>
      <li>
        <p>It is used for importance sampling to perform off-policy correction. In this sense, the “old policy” is meant to represent the <strong>behavior policy</strong> that generated the training data.</p>
      </li>
      <li>
        <p>It is also used to limit the update step size of the new policy. In this sense, the “old policy” acts as a baseline to measure how much the new and old policies differ, i.e., a <strong>proximal policy</strong> (what I call the reference policy here).</p>
      </li>
    </ol>

    <p>The paper points out that these two roles do <em>not</em> have to be played by the same policy, and proposes the Decoupled PPO objective, which explicitly decouples “who generates the data” from “who defines the trust region” at the level of the optimization objective.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2505.24298">AReaL</a> focuses on the mismatch between behavior and reference policies under asynchronous training frameworks: rollouts are often generated by <strong>stale parameter versions</strong> or <strong>different workers</strong>. The paper adopts a Decoupled-PPO-style objective in the asynchronous setting, explicitly separating the behavior distribution from the reference policy, while still maintaining PPO-like optimization properties in this asynchronous regime.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2507.18071">GSPO</a> starts from stability issues of GRPO on long sequences and MoE models. It shows that token-level PPO / GRPO can become highly unstable when MoE expert routing is extremely volatile (especially when routing differs significantly between old and new policies), leading to large variance and training collapse. GSPO proposes a <strong>sequence-level</strong> PPO-style objective and ratio constraint, using the ratio over entire sequences to control updates. This substantially mitigates training collapse in MoE scenarios caused by routing instability and token-level noise.</p>
  </li>
  <li>
    <p><a href="https://fengyao.notion.site/off-policy-rl#28b721e3f6c480c3a756f8fb319e860d">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training</a> observes that in existing LLM RL frameworks (such as VeRL), the inference stack and the training stack often differ across multiple functional modules (e.g., vLLM vs. FSDP / Megatron kernels and operators). This makes the behavior policy $\mu$ differ from the reference policy $\pi_{\theta_{\text{old}}}$, so what is <em>assumed</em> to be on-policy training actually becomes off-policy training with nontrivial bias. The article summarizes two existing ways to handle this: PPO-IS and vanilla-IS, and further proposes <strong>token-level truncated importance sampling (TIS)</strong> to downweight samples with severe training–inference mismatch. The author also wrote two more foundational notes analyzing training–inference mismatch from basic principles: <a href="https://fengyao.notion.site/pg-seq-token-part1-basics">Part I</a> and <a href="https://fengyao.notion.site/pg-seq-token-part2-mismatch">Part II</a>.</p>
  </li>
  <li>
    <p><a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference">Defeating Nondeterminism in LLM Inference</a> points out that the lack of <strong>batch-size invariance</strong> is a core source of randomness in LLM inference: the same input can yield noticeably different probability distributions under different batch compositions and kernel paths. This means that even when you “nominally” have a single set of parameters, the <strong>behavior policy</strong> $\mu$ realized in practice can fluctuate with system load and scheduling, further exacerbating training–inference mismatch.</p>
  </li>
  <li>
    <p><a href="https://ringtech.notion.site/icepop">Small Leak Can Sink a Great Ship—Boost RL Training on MoE with 𝑰𝒄𝒆𝑷𝒐𝒑!</a> observes that the above mismatch issues are further amplified in MoE models: routing itself is highly sensitive to small perturbations, and stacked with inference / training implementation differences and asynchronous sampling, it is easy to magnify bias and instability. The paper proposes IcePop: at the <strong>token level</strong>, it computes importance sampling ratios and applies <strong>two-sided masking</strong> to discard tokens whose ratios are either too large or too small. This removes “very noisy” data from the gradient, stabilizing RL training on MoE models.</p>
  </li>
  <li>
    <p><a href="https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda">When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch</a> gives a systematic analysis of the causes of training–inference mismatch, including large amounts of out-of-distribution and low-probability content introduced by agent workflows, hardware and kernel-level numerical uncertainty, and how <strong>token-level</strong> importance sampling can introduce severe bias on long sequences. It further proposes <strong>sequence-level</strong> masked importance sampling (sequence-level MIS): compute an IS ratio at the sequence level and discard only those sequences whose overall ratio is too large, thereby controlling bias while strongly suppressing training collapse caused by extreme samples. The paper provides reasonably complete theoretical derivations and extensive experimental evidence.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2510.11370">Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</a> focuses on the MoE-specific problem of <strong>routing inconsistency</strong>. The paper finds that even for identical inputs, inference and training can route tokens to different experts due to small differences in operator implementations or parallelism. This “physical-path” mismatch makes the gap between the behavior policy $\mu$ and the reference policy $\pi_{\theta_{\text{old}}}$ much larger than expected and can easily cause training collapse. To address this, the paper proposes <strong>Rollout Routing Replay (R3)</strong>: during rollout it records, for each token, the actual expert indices selected by the inference router, and during training it <strong>replays</strong> these routing decisions instead of recomputing them. In effect, R3 forces the training and inference stacks to share the same routing paths in the MoE topology, aligning the two sides at the level of the computation graph.</p>
  </li>
  <li>
    <p><a href="https://zhuanlan.zhihu.com/p/1959976628290590602">RL 老训崩？训推差异是基石</a> approaches the problem more from a practical perspective, sharing experience on how to engineer for near training–inference consistency: choosing consistent operators and precision settings, monitoring and constraining the log-prob gap between training and inference, etc. The focus is on framework-level engineering practices that can mitigate training–inference difference at the root.</p>
  </li>
  <li>
    <p><a href="https://verl.readthedocs.io/en/latest/algo/rollout_corr.html">verl Rollout Importance Sampling</a> introduces a <strong>Token Veto</strong> mechanism in its rollout correction module: it computes <strong>token-level</strong> importance ratios $\rho_t^{(\text{ref}\leftarrow\text{beh})}$, and if any token in a trajectory satisfies $\min_t \rho_t &lt; \tau_{\text{veto}}$, the entire sequence is discarded from training. This “token-level detection, sequence-level veto” design embodies a conservative “one-vote veto” strategy.</p>
  </li>
  <li><a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf">INTELLECT-3 Technical Report</a> adopts a similar rejection sampling strategy in its asynchronous distributed RL training framework. INTELLECT-3 computes <strong>token-level</strong> importance ratios for each rollout; if any token’s ratio falls below a threshold ($10^{-5}$ in the paper), the entire trajectory is masked.</li>
</ul>

<h2 id="a-minimally-unified-view-from-a-three-policy-trpo-perspective">A Minimally Unified View from a Three-Policy TRPO Perspective</h2>

<p>At first glance, the works listed above seem to tackle different aspects:</p>

<ul>
  <li><strong>Algorithmic level</strong>: how to formulate PPO / GRPO objectives, token-level vs. sequence-level, clip vs. mask, etc.</li>
  <li><strong>Systems level</strong>: how to align inference and training stacks.</li>
  <li><strong>Model level</strong>: how MoE routing amplifies instability, and so on.</li>
</ul>

<p>However, if we align everything along a single axis — <strong>behavior policy vs. reference policy</strong> — a large fraction of these issues can be placed in a relatively simple theoretical framework: a <strong>three-policy TRPO</strong>.</p>

<p>In the next section I’ll unpack this three-policy TRPO in as simple math as I can. You can think of it as “TRPO + triangle inequality” — a very small extension conceptually, but surprisingly handy when analyzing training–inference mismatch in LLM RL:</p>

<ul>
  <li>On the one hand, it helps us understand what exactly “training–inference mismatch” and “asynchronous training frameworks” are harming within the TRPO view.</li>
  <li>On the other hand, it offers a unifying way to interpret TIS, IcePop, sequence-level MIS, etc. In the view of this post, they can all be seen as different incarnations of <strong>Constraint 2</strong> introduced below.</li>
</ul>

<h3 id="three-policies">Three Policies</h3>

<p>We stick to the notation from above and consider a discounted MDP with discount factor $\gamma \in (0,1)$:</p>

<ul>
  <li>States $s \in \mathcal{S}$, actions $a \in \mathcal{A}$.</li>
  <li>Policy $\pi(a \mid s)$.</li>
  <li>Discounted state distribution:
\(d_\pi(s) := (1-\gamma)\sum_{t=0}^\infty \gamma^t \Pr_\pi(s_t = s).\)</li>
  <li>Return (episodic view):
\(\mathcal{J}(\pi) := \mathbb{E}_\pi\Big[\sum_{t=0}^\infty \gamma^t r_t\Big].\)</li>
  <li>Value / Q / advantage functions:
\(V_\pi(s),\quad Q_\pi(s,a),\quad A_\pi(s,a) := Q_\pi(s,a) - V_\pi(s).\)</li>
</ul>

<p>It’s worth spelling out that in the three-policy setup we have:</p>

<ul>
  <li>
    <p><strong>Behavior policy</strong> $\mu$: the policy that actually generates rollouts. Data $(s,a,r,\dots)$ are sampled from it.</p>
  </li>
  <li>
    <p><strong>Reference policy</strong> $\pi_{\theta_{\text{old}}}$: the “old policy” used in the optimization objective for importance sampling ratios, clipping, or KL constraints.</p>
  </li>
  <li>
    <p><strong>Target policy</strong> $\pi_\theta$: the policy we are optimizing in this update.</p>
  </li>
</ul>

<p>In the ideal setup we assume $\mu = \pi_{\theta_{\text{old}}}$; in real systems they are often unequal. This is the mathematical shadow of “training–inference mismatch.”</p>

<h3 id="two-policy-trpo">Two-Policy TRPO</h3>

<blockquote>
  <p>If you’re already familiar with TRPO, feel free to skip ahead to the “Three-Policy TRPO” subsection.</p>
</blockquote>

<p>All the theoretical guarantees in TRPO are stated <strong>with respect to the advantage function of some baseline policy</strong>. Since the only advantage we can estimate reliably in practice is $A_\mu$ (data are sampled under $\mu$), we may as well treat $\mu$ as the baseline policy.</p>

<p>A classical result is the <strong>Performance Difference Lemma</strong>:</p>

<blockquote>
  <p>For any two policies $\mu$ and $\pi_\theta$, we have</p>

\[\mathcal{J}(\pi_\theta) - \mathcal{J}(\mu)
= \frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d_{\pi_\theta},\, a\sim\pi_\theta}[A_\mu(s,a)].\]
</blockquote>

<p>The intuition is simple:</p>

<ul>
  <li>$A_\mu(s,a)$ says: “if I deviate from what $\mu$ would do at state $s$ and instead take action $a$, how much will the long-term return change?”</li>
  <li>Summing that “gain” across all time steps, states, and actions gives the total improvement of the new policy over the behavior policy.</li>
</ul>

<p>The challenge in TRPO is that we cannot compute</p>

\[\mathbb{E}_{s\sim d_{\pi_\theta}, a\sim\pi_\theta}[A_\mu(s,a)]\]

<p>exactly, because $d_{\pi_\theta}$ is the state distribution of the <em>new</em> policy, under which we do not have samples.</p>

<p>So TRPO introduces a surrogate objective by replacing the state distribution with that of the behavior policy:</p>

\[L_\mu(\pi_\theta)
:= \mathcal{J}(\mu) + \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_\mu,\,a\sim \pi_\theta}[A_\mu(s,a)].\]

<p>Intuitively, $L_\mu$ asks the following question: “Under the states visited by the behavior policy, how good is the new policy if we just let it pick the actions?”</p>

<p>Starting from the Performance Difference Lemma, the difference between the true objective and the surrogate is:</p>

\[\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)
= \frac{1}{1-\gamma}\;
  \sum_s \big(d_{\pi_\theta}(s) - d_\mu(s)\big)
  \,\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[A_\mu(s,a)].\]

<p>If we define</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|,\]

<p>we immediately get the following upper bound:</p>

<blockquote>
  <p><strong>Lemma 1</strong></p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\;
    \|d_{\pi_\theta} - d_\mu\|_1.\]
</blockquote>

<p>This reveals the first key quantity:</p>

<blockquote>
  <p><strong>State distribution shift</strong> $|d_{\pi_\theta} - d_\mu|_1$, i.e., “how differently the new policy sees the world, compared to the behavior policy.”</p>
</blockquote>

<p>We usually do <em>not</em> directly impose constraints on $|d_{\pi_\theta} - d_\mu|_1$. Instead, we constrain the per-timestep action distribution difference — via trust regions, KL penalties, clipping, etc.</p>

<p>Define the total variation (TV) distance:</p>

\[D_{\mathrm{TV}}(p,q) := \frac{1}{2}\|p-q\|_1.\]

<p>Assume there is a constant $\beta$ such that</p>

<blockquote>
  <p>For all $s$, the TV distance between the behavior and target policies is bounded:</p>

\[D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big) \le \beta.\]
</blockquote>

<p>Intuitively: in any state, the action distribution of the “new policy” cannot deviate too much from that of the policy that generated the data.</p>

<p>A standard result (provable via coupling) is:</p>

<blockquote>
  <p><strong>Lemma 2</strong>
Under the assumption above,</p>

\[\|d_{\pi_\theta} - d_\mu\|_1
\le \frac{2\gamma}{1-\gamma}\,\beta.\]
</blockquote>

<p>Combining Lemma 1 and Lemma 2, we obtain</p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\; \frac{2\gamma}{1-\gamma}\,\beta
= \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\beta.\]

<p>This gives a compact <strong>two-policy TRPO lower bound (baseline = behavior policy)</strong>:</p>

<blockquote>
  <p><strong>Theorem 1 (Two-Policy TRPO)</strong></p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
\frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\beta.\]
</blockquote>

<p>This suggests:</p>

<ul>
  <li><strong>What really matters for the tightness of $L_\mu(\pi_\theta)$ as a surrogate for $\mathcal{J}(\pi_\theta)$ is how far the behavior policy $\mu$ and the target policy $\pi_\theta$ drift apart:</strong>
\(\beta = \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big).\)</li>
</ul>

<p>If you can directly control this $\beta$, you can essentially port TRPO’s monotonic improvement guarantees to the behavior-policy view.</p>

<h3 id="three-policy-trpo">Three-Policy TRPO</h3>

<p>In practice, especially in large-scale LLM RL, <strong>we often cannot directly control $\beta$ itself.</strong></p>

<p>In most PPO / GRPO / GSPO / RLHF-style frameworks, the actual situation is:</p>

<ul>
  <li>Rollout data are generated by some <strong>behavior policy</strong> $\mu$ (some particular parameter version plus system details inside the inference engine).</li>
  <li>During updates, we would like to leverage a <strong>reference policy</strong> $\pi_{\theta_{\text{old}}}$ to limit the update of the <strong>target policy</strong> $\pi_\theta$.</li>
</ul>

<p>In other words, what we can actually touch and control are two quantities:</p>

<ol>
  <li>
    <p><strong>Reference vs. target</strong>: via KL penalties, clipping, etc., we constrain</p>

\[D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big).\]
  </li>
  <li>
    <p><strong>Behavior vs. reference</strong>: we would <em>like</em> to keep
\(D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)\)
small as well — this is where training–inference mismatch and asynchronous execution come in.</p>
  </li>
</ol>

<p>This motivates defining two “proxy gaps”:</p>

<ul>
  <li>
    <p><strong>Constraint 1: reference vs. target</strong></p>

\[\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big);\]
  </li>
  <li>
    <p><strong>Constraint 2: behavior vs. reference</strong>
\(\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big).\)</p>
  </li>
</ul>

<p>Intuitively:</p>

<ul>
  <li>$\alpha_0$: how far the new policy is from the “old policy” you are using in the loss — this is the trust-region part.</li>
  <li>$\alpha_1$: how far the reference policy used in training is from the <em>actual</em> behavior policy that generated the data — this is the footprint of training–inference mismatch and asynchrony.</li>
</ul>

<p>Now we can plug these two quantities back into the TRPO lower bound.</p>

<p>For any state $s$, by the triangle inequality we have</p>

\[\begin{aligned}
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
&amp;\le
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)
\\
&amp;\quad +
D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big).
\end{aligned}\]

<p>Taking the supremum over $s$ gives</p>

\[\beta
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
\;\le\;
\alpha_1 + \alpha_0.\]

<p>Plugging this inequality into the two-policy TRPO bound (Theorem 1), and denoting</p>

\[C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2},\]

<p>we obtain</p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,\beta
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,(\alpha_0 + \alpha_1).\]

<p>This yields a very direct <strong>three-policy TRPO lower bound</strong>:</p>

<blockquote>
  <p><strong>Theorem 2 (Three-Policy TRPO)</strong>
Let</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|,\quad
C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2},\]

  <p>and</p>

\[\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big),
\quad
\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big).\]

  <p>Then for any target policy $\pi_\theta$,</p>

\[\boxed{
\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\; C\,(\alpha_0 + \alpha_1)
}\]

  <p>where</p>

\[L_\mu(\pi_\theta)
:=
\mathcal{J}(\mu) + \frac{1}{1-\gamma}
  \mathbb{E}_{s\sim d_\mu,a\sim\pi_\theta}[A_\mu(s,a)].\]
</blockquote>

<p>The meaning of this bound is quite straightforward:</p>

<ul>
  <li><strong>The gap between the surrogate objective $L_\mu(\pi_\theta)$ and the true performance $\mathcal{J}(\pi_\theta)$ decomposes into two pieces:</strong>
    <ul>
      <li>The deviation between reference and target policies, $\alpha_0$.</li>
      <li>The deviation between behavior and reference policies, $\alpha_1$.</li>
    </ul>
  </li>
</ul>

<p>As long as both terms are small, <strong>optimizing $L_\mu$ is likely to improve $\mathcal{J}$</strong>.</p>

<h3 id="how-to-control-these-two-deviations-in-practice">How to Control These Two Deviations in Practice?</h3>

<p>We can now revisit various practical methods through the lens of Theorem 2:</p>

<ul>
  <li>Most PPO / GRPO / GSPO-style work focuses on controlling <strong>Constraint 1: $\alpha_0$</strong>.</li>
  <li>Most TIS / IcePop / MIS-style work, in the view of this post, can be understood as primarily targeting <strong>Constraint 2: $\alpha_1$</strong>.</li>
</ul>

<p>In the remainder of this post I will focus on <strong>Constraint 2</strong>.</p>

<p>The goal of Constraint 2 is: <strong>ensure that the data used in training come (effectively) from a behavior policy that is close to the reference policy.</strong></p>

<p>In practice, this usually involves both <strong>system-level mechanisms</strong> and <strong>algorithmic mechanisms (importance sampling)</strong>.</p>

<ol>
  <li><strong>System level: keep the behavior policy from drifting too far</strong>
    <ul>
      <li>
        <p>Asynchronous frameworks:
Tag each sample with a policy version, and only use data generated by parameter versions that are close enough to $\pi_{\theta_{\text{old}}}$.</p>
      </li>
      <li>
        <p>Training–inference alignment:
Use consistent precision, operators, and similar kernel behavior between the training and inference stacks.</p>
      </li>
    </ul>

    <p>These mechanisms act “outside” the algorithm to make $\mu$ closer to $\pi_{\theta_{\text{old}}}$, thereby shrinking $\alpha_1$.</p>
  </li>
  <li>
    <p><strong>Algorithmic level: sample-wise correction</strong></p>

    <p>At the algorithmic level, we no longer attempt to “fix” the entire behavior policy. Instead, we use importance sampling ratios to correct at the <strong>sample level</strong>: we filter or reweight samples so that the behavior policy is close to the reference policy <em>on the subset of data that actually participates in training</em>, or at least reduce the influence of samples with large mismatch.</p>

    <p>Concretely, this gives rise to methods like TIS, IcePop, and MIS, which can be seen as different ways of implementing Constraint 2 at the sample level.</p>
  </li>
</ol>

<h2 id="importance-sampling-and-masking-four-implementations-of-constraint-2">Importance Sampling and Masking: Four Implementations of Constraint 2</h2>

<p>In this section I’ll reuse the notation introduced above to write down the objectives of these three methods, focusing only on the design choices related to “behavior vs. reference policy.” Let the token-level PPO / GRPO-style update term be</p>

\[g_\theta(t)
= \min\big(r_t(\theta) A_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon) A_t\big),\]

<p>where</p>

\[r_t(\theta) = \frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)},
\quad (s_t,a_t)\sim\mu,\quad A_t := A_\mu(s_t,a_t).\]

<p>Here:</p>

<ul>
  <li>$r_t(\theta)$ is the <strong>target vs. reference</strong> ratio (corresponding to Constraint 1).</li>
  <li>$A_t$ is the advantage estimated from data sampled under the behavior policy.</li>
</ul>

<p>To connect token-level $(s_t,a_t)$ with sequence-level $(x,y)$ notation, consider the RLHF setting (reinforcement learning from human feedback) for LLMs:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Prompts are denoted by $x$, and responses by $y = (y_1,\dots,y_{</td>
          <td>y</td>
          <td>})$.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Token-level states and actions are defined as $s_t := (x,y_{&lt;t})$, $a_t := y_t$.</li>
  <li>The behavior and reference policies on sequences can then be written as
\(\mu(y\mid x) = \prod_{t=1}^{|y|}\mu(a_t=y_t\mid s_t),\quad
\pi_{\theta_{\text{old}}}(y\mid x) = \prod_{t=1}^{|y|}\pi_{\theta_{\text{old}}}(a_t=y_t\mid s_t).\)</li>
</ul>

<p>To quantify the deviation between reference and behavior policies, we can define the token-level importance ratio:</p>

\[\rho_t^{(\text{ref}\leftarrow\text{beh})} :=
\frac{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}{\mu(a_t\mid s_t)},\]

<p>and its sequence-level counterpart:</p>

\[\rho(y\mid x) := \frac{\pi_{\theta_{\text{old}}}(y\mid x)}{\mu(y\mid x)}
= \prod_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})}.\]

<p>The difference between TIS, IcePop, and MIS lies in <strong>how they use $\rho$ to implement Constraint 2</strong>.</p>

<h3 id="1-tis-token-level-truncated-importance-sampling">1. TIS: Token-Level Truncated Importance Sampling</h3>

<p>TIS directly truncates the token-level ratio $\rho_t^{(\text{ref}\leftarrow\text{beh})}$; define</p>

\[\color{blue}{w_t = \min\big(\rho_t^{(\text{ref}\leftarrow\text{beh})},\ C_{\text{IS}}\big)}.\]

<p>The update objective becomes</p>

\[L_{\text{TIS}}(\theta)
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{w_t}\; g_\theta(t)\big].\]

<ul>
  <li>The blue $\color{blue}{w_t}$ is the truncated IS weight: extremely large ratios are capped at a constant $C_{\text{IS}}$.</li>
  <li>From the three-policy TRPO perspective, this is a <em>soft</em> way to downweight tokens where behavior and reference policies differ significantly, effectively reducing their contribution to $\alpha_1$ in the gradient.</li>
</ul>

<h3 id="2-icepop-token-level-two-sided-masking-in-moe">2. IcePop: Token-Level Two-Sided Masking in MoE</h3>

<p>IcePop also uses $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ as a discrepancy measure, but opts for <strong>two-sided masking</strong>:</p>

\[\color{blue}{m_t = \mathbf{1}\big[C_{\text{low}} \le \rho_t^{(\text{ref}\leftarrow\text{beh})} \le C_{\text{high}}\big]}.\]

<p>The update objective becomes</p>

\[L_{\text{IcePop}}(\theta)
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{m_t}\; g_\theta(t)\big].\]

<ul>
  <li>The blue $\color{blue}{m_t}$ decides whether a token participates in the update: tokens with ratios that are too large or too small are dropped entirely.</li>
  <li>This is a <em>hard</em> sample selection scheme: only tokens where behavior and reference policies are reasonably aligned (ratios within $[C_{\text{low}}, C_{\text{high}}]$) are kept, implementing a stricter version of Constraint 2 at the token level.</li>
</ul>

<h3 id="3-sequence-level-mis-masked-importance-sampling-over-entire-sequences">3. Sequence-Level MIS: Masked Importance Sampling Over Entire Sequences</h3>

<p>The core operation in sequence-level MIS is to <strong>retain only sequences whose sequence-level IS ratio is below a threshold $C$</strong>, zeroing out the loss for all other sequences:</p>

\[\color{blue}{
\rho(y\mid x)
\leftarrow
\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}
}\]

<p>In a unified loss form, this can be written as</p>

\[L_{\text{MIS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}}
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big].\]

<p>In words:</p>

<ul>
  <li>For <strong>sequences with small IS ratios</strong>, the full weight $\rho(y\mid x)$ is retained for off-policy correction.</li>
  <li>For <strong>sequences whose ratios exceed the threshold $C$</strong>, the entire policy loss is masked out (weight set to $0$).</li>
</ul>

<p>From the three-policy TRPO viewpoint, sequence-level MIS no longer truncates at the token level. Instead, it performs <strong>trajectory-level</strong> filtering: it drops trajectories where behavior and reference policies diverge too much, and only optimizes on the subset with $\rho(y\mid x)\le C$. This implements Constraint 2 at the sequence level.</p>

<h3 id="4-worst-token-reject-sampling-rejecting-entire-sequences-based-on-the-worst-token">4. Worst Token Reject Sampling: Rejecting Entire Sequences Based on the Worst Token</h3>

<p>The verl Token Veto mechanism and INTELLECT-3 both adopt a rejection sampling strategy that can be collectively called <strong>Worst Token Reject Sampling (WTRS)</strong>:</p>

<ul>
  <li>
    <p><strong>verl Token Veto</strong>: In its rollout correction module, if any token in a trajectory has $\min_t \rho_t &lt; \tau_{\text{veto}}$, the entire sequence is discarded via response<em>mask. The threshold $\tau</em>{\text{veto}}$ is user-configurable.</p>
  </li>
  <li>
    <p><strong>INTELLECT-3 Token Masking</strong>: In its asynchronous distributed RL framework, if any token’s ratio is below $10^{-5}$, the entire trajectory is masked.</p>
  </li>
</ul>

<p>The core operation is identical: <strong>if any token in a trajectory has an IS ratio below a threshold $\tau$, the entire sequence is rejected from training.</strong> This can be written as:</p>

\[\color{blue}{
m(y\mid x) = \mathbf{1}\Big\{\min_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})} \ge \tau\Big\}
}\]

<p>In a unified loss form:</p>

\[L_{\text{WTRS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{m(y\mid x)}
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big].\]

<p>In words:</p>

<ul>
  <li>For <strong>sequences where all tokens have IS ratios $\ge \tau$</strong>: participate in training normally.</li>
  <li>For <strong>sequences where any token has an IS ratio $&lt; \tau$</strong>: the entire sequence’s policy loss is masked out.</li>
</ul>

<p>From the three-policy TRPO perspective, WTRS adopts a hybrid “token-level detection, sequence-level veto” strategy: it detects extreme mismatch signals at the <strong>token level</strong>, and once detected, rejects at the <strong>sequence level</strong>. This “one-vote veto” design reflects a conservative philosophy — when a trajectory contains a token that “the behavior policy generated but the reference policy would almost never generate,” <strong>the credibility of the entire trajectory is called into question</strong>, thereby implementing control over Constraint 2 ($\mu$ vs. $\pi_{\theta_{\text{old}}}$ deviation) at the trajectory granularity.</p>

<h2 id="moe-routing-replay-what-does-it-actually-do-in-three-policy-trpo">MoE Routing Replay: What Does It Actually Do in Three-Policy TRPO?</h2>

<p>In MoE (Mixture-of-Experts) models, training–inference mismatch often first appears as <strong>routing inconsistency</strong>: even with identical parameters, the inference and training stacks may route tokens to different experts because of small differences in operators, parallelism, or numerics. A natural engineering response is <strong>routing replay</strong>: during rollout (inference), record the actual expert paths, and during training, force the model to reuse these routing decisions.</p>

<p>These methods are often intuitively described as “implementing Constraint 2 and shrinking $\alpha_1$.” From the three-policy TRPO perspective, a more precise statement is:</p>

<blockquote>
  <p><strong>Routing replay does not tighten the original surrogate objective via a constraint; instead, it rewrites the surrogate objective into one that is conditioned on / replaces the routing.</strong>
It makes routing mismatch invisible in the loss, but it does not actually shrink the true policy distances $\alpha_0$ or $\alpha_1$.</p>
</blockquote>

<p>Below I’ll sketch a <strong>minimal</strong> abstraction that is sufficient to make this concrete.</p>

<h3 id="surrogate-objective-in-moe-separating-routing-and-token-generation">Surrogate Objective in MoE: Separating Routing and Token Generation</h3>

<p>Abstract an MoE model as a two-stage stochastic decision: “first choose an expert $z$, then generate token $a$ conditioned on that expert.” The target policy can be factorized as</p>

\[\pi_\theta(a,z\mid s)=\omega_\theta(z\mid s)\,\pi_\theta(a\mid s,z),\]

<p>where:</p>

<ul>
  <li>$\omega_\theta(z\mid s)$ is the router distribution.</li>
  <li>$\pi_\theta(a\mid s,z)$ is the token distribution conditioned on expert $z$.</li>
</ul>

<p>In the three-policy TRPO setting, the surrogate objective we actually want to optimize can be written as</p>

\[L_\mu(\pi_\theta) = \mathcal{J}(\mu) + \frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_z \omega_\theta(z\mid s)\,F_\theta(s,z)
\bigg],\]

<p>where I use</p>

\[F_\theta(s,z)
:=
\sum_a \pi_\theta(a\mid s,z)\,A_\mu(s,a,z)\]

<p>to denote the expert-level aggregation of advantages.</p>

<p>The key point is that <strong>in the original $L_\mu(\pi_\theta)$, the routing distribution is precisely the current router $\omega_\theta$ that we are updating</strong>. In other words, RL on MoE is updating not only the token-generation distribution but also the router itself.</p>

<h3 id="1-replaying-behavior-policy-routing-behavior-router-replay--r3-style">(1) Replaying Behavior-Policy Routing (Behavior-Router Replay / R3-Style)</h3>

<p>R3-style methods record, during rollout, the set of experts $M_\mu(s)$ actually selected by the behavior policy on the inference side, and during training force the current policy to <strong>route only within this set</strong>. This can be written as a “conditional projection” of the routing distribution:</p>

\[\omega_\theta^{\text{R3}}(z\mid s)
:=
\frac{\omega_\theta(z\mid s)\,\mathbf{1}\{z\in M_\mu(s)\}}
     {\sum_{z'\in M_\mu(s)}\omega_\theta(z'\mid s)} .\]

<p>The surrogate objective that is actually optimized during training becomes</p>

\[L_\mu^{\text{R3}}(\pi_\theta) =
\mathcal{J}(\mu) +
\frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_{z\in M_\mu(s)} \omega_\theta^{\text{R3}}(z\mid s)\,F_\theta(s,z)
\bigg].\]

<p>Compared to the original $L_\mu(\pi_\theta)$, R3 does <em>not</em> push $\omega_\theta$ closer to $\omega_{\text{old}}$ or $\omega_\mu$. Instead, it:</p>

<ul>
  <li><strong>replaces the expectation over $z\sim\omega_\theta$ by a conditional expectation over $z\sim\omega_\theta(\cdot\mid z\in M_\mu(s))$</strong>, and</li>
  <li>equivalently, <strong>shrinks the feasible routing support to $M_\mu(s)$</strong>.</li>
</ul>

<p>So R3 is optimizing a “behavior-routing-conditioned surrogate objective,” rather than the original $L_\mu(\pi_\theta)$. The benefit is substantially reduced variance and improved stability; the cost is that <strong>the router’s exploration and update freedom is constrained at every state</strong>.</p>

<h3 id="2-replaying-reference-policy-routing-reference-router-replay">(2) Replaying Reference-Policy Routing (Reference-Router Replay)</h3>

<p>Another class of routing-replay schemes instead reuses the reference policy’s router $\omega_{\text{old}}$. This is equivalent to training a hybrid policy</p>

\[\hat\pi_\theta(a,z\mid s)
:=
\omega_{\text{old}}(z\mid s)\,\pi_\theta(a\mid s,z),\]

<p>with surrogate objective</p>

\[L_\mu^{\text{ref-replay}}(\pi_\theta) =
\mathcal{J}(\mu) +
\frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_z \omega_{\text{old}}(z\mid s)\,F_\theta(s,z)
\bigg].\]

<p>This has the effect that:</p>

<ul>
  <li>In the surrogate objective, the router is <strong>frozen to the old router</strong> $\omega_{\text{old}}$, so the “reference vs. target” discrepancy in routing is simply removed from the loss.</li>
  <li>Training becomes insensitive to how far the <em>new</em> router $\omega_\theta$ drifts from $\omega_{\text{old}}$, thereby sidestepping the instabilities caused by routing mismatch.</li>
</ul>

<p>Again, this is fundamentally a <strong>change of objective</strong>:</p>

<ul>
  <li>The deviation $\alpha_0$ in the true policy space is not reduced; it is merely rendered invisible by redefining the surrogate in terms of the old router.</li>
  <li>Learning of the router is effectively frozen or heavily suppressed.</li>
</ul>

<h3 id="routing-replay-as-a-change-of-surrogate-objective">Routing Replay as a Change of Surrogate Objective</h3>

<p>Putting these replay variants side by side, they share several properties:</p>

<ol>
  <li><strong>They optimize not the original $L_\mu(\pi_\theta)$, but a surrogate where routing has been conditioned or replaced.</strong></li>
  <li><strong>They do not directly shrink the three-policy TRPO bound’s $\alpha_0$ or $\alpha_1$</strong>. Routing mismatch is removed from the loss, but it still exists in the true policy distances.</li>
  <li><strong>In practice they trade bias for variance</strong>: replay typically lowers variance and improves stability, but may also limit the router’s ability to learn routing patterns that are optimal for the RL objective.</li>
</ol>

<p>So, in the three-policy TRPO view, a more accurate characterization is:</p>

<blockquote>
  <p><strong>Routing replay is best thought of as a rewrite of the surrogate objective, not as a direct implementation of a constraint on $\alpha_0$ or $\alpha_1$.</strong></p>
</blockquote>

<h2 id="conclusion">Conclusion</h2>

<p>If I had to compress this post into a single sentence, it would be:</p>

<blockquote>
  <p><strong>Many issues around “training–inference mismatch” and “asynchronous training” in large-scale LLM RL can be understood, in the TRPO framework, as severely underestimating the deviation between the behavior policy $\mu$ and the reference policy $\pi_{\theta_{\text{old}}}$ — i.e., the term $\alpha_1$.</strong></p>
</blockquote>

<p>From two policies to three, what we did is conceptually very small:</p>

<ul>
  <li>
    <p>We rewrote the TRPO lower bound from an “old vs. new policy” narrative into a “<strong>behavior–reference–target</strong>” three-policy relationship.</p>
  </li>
  <li>We explicitly separated two TV distances:
    <ul>
      <li><strong>Constraint 1: reference vs. target</strong>, $\alpha_0$, corresponding to the KL / clip / trust-region style constraints in PPO / GRPO / GSPO.</li>
      <li><strong>Constraint 2: behavior vs. reference</strong>, $\alpha_1$, capturing real-world factors like asynchronous frameworks, training–inference mismatch, MoE routing volatility, kernel-level nondeterminism, etc.</li>
    </ul>
  </li>
  <li>This leads to a simple conclusion:
The gap between the surrogate $L_\mu(\pi_\theta)$ and the true performance $\mathcal{J}(\pi_\theta)$ scales with $\alpha_0 + \alpha_1$.</li>
</ul>

<p>Under this lens (which is of course only one of many possible perspectives):</p>

<ul>
  <li>
    <p>Decoupled PPO / AReaL can be viewed as <strong>formally acknowledging the existence of three policies</strong> and explicitly decoupling the behavior distribution from the reference policy in the objective.</p>
  </li>
  <li>TIS, IcePop, MIS, and WTRS can be seen as different ways of implementing <strong>Constraint 2</strong> using importance sampling truncation / masking:
    <ul>
      <li>TIS: token-level truncation of IS weights to soften the influence of extreme samples.</li>
      <li>IcePop: token-level two-sided masking in MoE to hard-drop tokens with severe mismatch.</li>
      <li>MIS: sequence-level masking to ignore entire trajectories whose behavior–reference mismatch is too large.</li>
      <li>WTRS: token-level detection of extremely small ratios, rejecting the entire trajectory once such a signal is found.</li>
    </ul>
  </li>
  <li>
    <p><strong>Routing replay</strong> (whether replaying behavior routing in R3-style schemes or replaying reference routing) is better viewed as <strong>changing the surrogate objective</strong> rather than directly implementing a constraint: both variants replace the original $L_\mu(\pi_\theta)$ with a routing-conditioned / routing-frozen surrogate, trading off some objective bias and reduced routing learning freedom for lower variance and greater stability, without actually shrinking $\alpha_0$ or $\alpha_1$—they simply make routing mismatch invisible in the loss.</p>
  </li>
  <li>Engineering advice such as in <em>RL 老训崩？训推差异是基石</em> and system-level work like <em>Defeating Nondeterminism in LLM Inference</em> can be interpreted as efforts to <strong>reduce $\alpha_1$ on the systems and numerical side</strong>, so that the assumptions underlying the algorithms do not break too badly.</li>
</ul>

<p>From this unified perspective, it may also be easier to think about the following practical questions (these are completely open and I don’t have definitive answers):</p>

<ul>
  <li>
    <p>Under what conditions can we still reasonably interpret “LLM RL training” as some approximate form of TRPO / PPO?</p>
  </li>
  <li>For a concrete RL system, where should we invest more effort:
    <ul>
      <li>tightening $\alpha_0$ (stronger KL control, more stable sequence-level objectives), or</li>
      <li>reducing $\alpha_1$ (better training–inference alignment, more aggressive MIS / TIS / IcePop)?</li>
    </ul>
  </li>
  <li>In the presence of MoE, asynchronous sampling, and complex agent workflows, how long can we safely pretend that “$\mu \approx \pi_{\theta_{\text{old}}}$”?</li>
</ul>

<p>This post is just a very <strong>minimal</strong> extension of the classic TRPO framework, making the “three policies” explicit and using them to organize some existing work. There are inevitably misunderstandings and omissions. If you also care about how RL training actually behaves in large LLM systems, I’d be very interested to see how your own setup can be abstracted into a relationship between $\mu$, $\pi_{\theta_{\text{old}}}$, and $\pi_\theta$, and then re-examined through the inequality in Theorem 2. It might give a slightly different intuitive feel for what your system is really optimizing.</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025ThreePolicyTRPO</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{From Two Policies to Three: Extending TRPO under Behavior-Reference Policy Mismatch in LLM RL}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html}</span><span class="p">,</span>
  <span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-11-23}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[Modern LLM RL pipelines often train under an "old policy" that silently drifts away from the behavior policy that actually generates rollouts, breaking the usual on-policy assumptions. This post rewrites the classic TRPO lower bound in a three-policy form — behavior, reference, and target — so that the performance gap cleanly decomposes into two TV distances that we can reason about and control. Seen through this lens, methods like Decoupled PPO, AReaL, TIS, IcePop, sequence-level MIS, Worst Token Reject Sampling (WTRS), MoE routing replay, and common engineering tricks for training–inference alignment all become different ways of shrinking these two deviations.]]></summary></entry><entry xml:lang="zh"><title type="html">从两策略到三策略：LLM RL 中行为策略–参考策略不一致下的 TRPO 扩展</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-zh.html" rel="alternate" type="text/html" title="从两策略到三策略：LLM RL 中行为策略–参考策略不一致下的 TRPO 扩展" /><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-zh</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-zh.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#训推不一致和异步框架" id="markdown-toc-训推不一致和异步框架">训推不一致和异步框架</a></li>
  <li><a href="#相关工作" id="markdown-toc-相关工作">相关工作</a></li>
  <li><a href="#三策略-trpo-视角下的最小统一理解" id="markdown-toc-三策略-trpo-视角下的最小统一理解">三策略 TRPO 视角下的最小统一理解</a>    <ul>
      <li><a href="#三个策略" id="markdown-toc-三个策略">三个策略</a></li>
      <li><a href="#两策略-trpo" id="markdown-toc-两策略-trpo">两策略 TRPO</a></li>
      <li><a href="#三策略-trpo" id="markdown-toc-三策略-trpo">三策略 TRPO</a></li>
      <li><a href="#这两个差异各自怎么约束" id="markdown-toc-这两个差异各自怎么约束">这两个差异各自怎么约束？</a></li>
    </ul>
  </li>
  <li><a href="#重要性采样与掩码四种约束-2-实现" id="markdown-toc-重要性采样与掩码四种约束-2-实现">重要性采样与掩码：四种约束 2 实现</a>    <ul>
      <li><a href="#1-tistoken-level-截断-is" id="markdown-toc-1-tistoken-level-截断-is">1. TIS：token-level 截断 IS</a></li>
      <li><a href="#2-icepopmoe-场景下的-token-level-双侧-mask" id="markdown-toc-2-icepopmoe-场景下的-token-level-双侧-mask">2. IcePop：MoE 场景下的 token-level 双侧 Mask</a></li>
      <li><a href="#3-sequence-level-mis按整条序列-mask-的重要性采样" id="markdown-toc-3-sequence-level-mis按整条序列-mask-的重要性采样">3. sequence-level MIS：按整条序列 Mask 的重要性采样</a></li>
      <li><a href="#4-worst-token-reject-sampling按最差-token-拒绝整条序列" id="markdown-toc-4-worst-token-reject-sampling按最差-token-拒绝整条序列">4. Worst Token Reject Sampling：按最差 token 拒绝整条序列</a></li>
    </ul>
  </li>
  <li><a href="#moe-路由回放它在三策略-trpo-中到底做了什么" id="markdown-toc-moe-路由回放它在三策略-trpo-中到底做了什么">MoE 路由回放：它在三策略 TRPO 中到底做了什么？</a>    <ul>
      <li><a href="#moe-下的-surrogate-objective把路由和token-生成拆开" id="markdown-toc-moe-下的-surrogate-objective把路由和token-生成拆开">MoE 下的 surrogate objective：把“路由”和“token 生成”拆开</a></li>
      <li><a href="#1回放行为策略的路由behavior-router-replay--r3-类" id="markdown-toc-1回放行为策略的路由behavior-router-replay--r3-类">1）回放行为策略的路由（behavior-router replay / R3 类）</a></li>
      <li><a href="#2回放参考策略的路由reference-router-replay" id="markdown-toc-2回放参考策略的路由reference-router-replay">2）回放参考策略的路由（reference-router replay）</a></li>
      <li><a href="#路由回放只是在改写-surrogate-objective" id="markdown-toc-路由回放只是在改写-surrogate-objective">路由回放只是在改写 surrogate objective</a></li>
    </ul>
  </li>
  <li><a href="#小结" id="markdown-toc-小结">小结</a></li>
</ul>

<p><a href="/reinforcement-learning/2025/11/15/three-policy-en.html">English Version</a> | <a href="https://zhuanlan.zhihu.com/p/1973206684907365344">知乎版本 <img src="https://static.zhihu.com/heifetz/favicon.ico" alt="Zhihu" /></a></p>

<p><img src="/assets/img/three-policy/three-policy-mini-class-zh.jpg" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<h2 id="训推不一致和异步框架">训推不一致和异步框架</h2>

<p>最近看到不少关于大模型强化学习中“训推不一致”和“异步训推框架”的讨论，我自己的直觉是：这些看上去复杂多样的问题，很大一部分其实都围绕着一个更基础的矛盾——<strong>行为策略（behavior policy）和参考策略（reference policy）不一致。</strong></p>

<p>本文先简单梳理一下我目前看到的相关工作，然后再尝试从“行为策略 vs 参考策略”的角度，把它们串到同一条线上，为读者提供一个补充视角。</p>

<p>在本文中我会用：</p>

<ul>
  <li><strong>行为策略</strong> $\mu$：实际负责生成 rollout 的策略，也就是“你在什么分布下采样到了这些数据”。在现代 LLM-RL 系统里，它对应的是推理引擎里的那套实现（vLLM / SGLang 等），在异步框架下往往还是<strong>多个 worker 策略的混合分布</strong>。</li>
  <li><strong>参考策略</strong> $\pi_{\theta_{\text{old}}}$：训练目标里拿来做重要性采样、clipping 或 KL 约束的策略，典型地就是 PPO / GRPO 里的“旧策略”（old policy）。</li>
  <li><strong>目标策略</strong> $\pi_\theta$：训练目标里要优化的策略，也就是“你想让模型变成什么样”。典型地就是 PPO / GRPO 里的“新策略”（new policy）。</li>
</ul>

<p>在最经典、理想化的设定里，我们通常<strong>默认</strong> $\mu = \pi_{\theta_{\text{old}}}$。但在现实系统中，受异步更新、不同推理 / 训练后端、MoE 路由波动甚至硬件数值差异等因素影响，二者往往会出现不同程度的偏离。</p>

<h2 id="相关工作">相关工作</h2>

<p>下面按时间线简单列一下我印象比较深的一些工作（只代表我个人看到的片面子集）：</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2110.00641">Decoupled PPO</a> 率先指出，在信赖域策略优化（TRPO 和 PPO）方法中，“旧策略”（old policy）实际承担了两个不同的角色：一是用于重要性采样进行异策略修正，在这个目的下，“旧策略”用于代表训练数据集所服从的行为策略（behavior policy）；二是用于限制新策略的更新幅度，在这个目的下，“旧策略”被用于衡量新旧策略的变化程度，称作近端策略（proximal policy，对应本文中的“参考策略”）。文章指出这两个目的下的“旧策略”可以是不同的策略，从而提出了 Decoupled PPO 更新目标，把“采样用谁”和“对谁做 trust region”在形式上解耦开来。</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2505.24298">AReaL</a> 关注到了异步训练框架下行为策略与参考策略不一致的问题：rollout 往往由滞后的参数版本或不同 worker 产生。文章在异步框架下采用了 Decoupled PPO 风格的目标，将“行为策略分布”和“参考策略”显式区分开来，从而在异步场景下仍然维持类似 PPO 的优化性质。</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2507.18071">GSPO</a> 从 GRPO 在长序列和 MoE 模型上的稳定性问题出发，指出 token-level 的 PPO / GRPO 在专家路由高度波动（尤其是新旧策略之间的路由差异）时，会引入巨大的方差与不稳定。GSPO 提出在 <strong>sequence-level</strong> 定义 PPO-style 目标与比率约束，用整条序列的比率来约束更新，从而在 MoE 场景下显著缓解由路由不一致带来的训练崩溃问题。</p>
  </li>
  <li>
    <p><a href="https://fengyao.notion.site/off-policy-rl#28b721e3f6c480c3a756f8fb319e860d">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training</a> 关注到了现有的一些大模型强化学习训练框架（如 VeRL）中，推理框架和训练框架在不少相同的功能模块上有不同的实现（例如 vLLM 和 FSDP / Megatron 等算子上的差异），导致行为策略 $\mu$ 与参考策略 $\pi_{\theta_{\text{old}}}$ 不一致。这种不一致使得原本假定为同策略（on-policy）的训练，实际上变成了带有明显偏差的异策略（off-policy）训练。文章总结了两种处理这一问题的现有方法：PPO-IS 与 vanilla-IS，并提出在 <strong>token-level</strong> 做截断重要性采样（truncated IS, TIS），以减少训推不一致程度较重的样本在训练中的影响。作者还写了两篇更为基础的分析文章，从原理上分析训推不一致问题：<a href="https://fengyao.notion.site/pg-seq-token-part1-basics">Part I</a> 和 <a href="https://fengyao.notion.site/pg-seq-token-part2-mismatch">Part II</a>。</p>
  </li>
  <li>
    <p><a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference">Defeating Nondeterminism in LLM Inference</a> 指出，批处理大小不变性（batch-size invariance）的缺失是大模型推理框架随机性的核心来源之一：同一个输入在不同的 batch 组合和 kernel 路径下，得到的概率分布会发生可观差异。这意味着，即便“名义上”是同一套参数，真实运行时的行为策略 $\mu$ 也会因为系统负载和调度差异而波动，从而进一步加剧训推不一致。</p>
  </li>
  <li>
    <p><a href="https://ringtech.notion.site/icepop">Small Leak Can Sink a Great Ship—Boost RL Training on MoE with 𝑰𝒄𝒆𝑷𝒐𝒑!</a> 观察到，上述训推不一致问题在 MoE 模型上会进一步加剧：路由本身就对微小扰动高度敏感，再叠加推理 / 训练实现差异和异步采样，很容易放大偏差。文章提出 IcePop 方法：在 <strong>token-level</strong> 通过计算重要性采样比率，对过于大或者过于小的比率进行双侧掩码（masking），将这些“噪声较大”的数据从梯度中丢弃，从而稳定 MoE 上的 RL 训练。</p>
  </li>
  <li>
    <p><a href="https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda">When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch</a> 系统性分析了训推不一致的各种成因，包括智能体工作流中引入的大量分布外和低概率信息、硬件和内核 / kernel 实现带来的计算不确定性，并分析了在 <strong>token-level</strong> 进行重要性采样如何在长序列上引入严重的偏差。文章进一步提出在 <strong>sequence-level</strong> 计算重要性采样掩码（sequence-level masked IS, sequence-level MIS）：只丢弃那些整条序列的重要性采样比率过大的数据，从而在控制偏差的同时，显著抑制由极端样本导致的训练崩溃。文中给出了较为完整的理论推导和丰富的实验支撑。</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2510.11370">Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</a> 聚焦于 MoE 架构下特有的 <strong>路由不一致（Routing Inconsistency）</strong> 问题。文章发现，推理端和训练端即便在输入完全相同的情况下，由于算子实现或并行的微小差异，Router 选中的专家往往不同。这种“物理路径”上的不一致，使得行为策略 $\mu$ 和参考策略 $\pi_{\theta_{\text{old}}}$ 之间的差异远超预期，极易导致训练崩溃。文章提出了 <strong>Rollout Routing Replay (R3)</strong>：在推理阶段记录下每个 token 实际命中的专家索引，并在训练阶段<strong>强制回放</strong>这些路由决策，不再重新进行计算。通过这种方式，R3 在 MoE 拓扑结构上强制对齐了训推两端的计算路径。</p>
  </li>
  <li>
    <p><a href="https://zhuanlan.zhihu.com/p/1959976628290590602">RL 老训崩？训推差异是基石</a> 则更多从实践角度出发，分享了如何在实现上尽可能靠近“训推一致”的经验，包括如何选用一致的算子和精度配置、如何监控与约束训练端和推理端 log-prob 的偏差等，更着力于从训推框架层面入手，在工程上尽量从根本缓解训推差异问题。</p>
  </li>
  <li>
    <p><a href="https://verl.readthedocs.io/en/latest/algo/rollout_corr.html">verl Rollout Importance Sampling</a> 在其 rollout correction 模块中引入了 Token Veto（一票否决）机制：在 <strong>token-level</strong> 计算重要性比率 $\rho_t^{(\text{ref}\leftarrow\text{beh})}$，若轨迹中存在任意 token 使得 $\min_t \rho_t &lt; \tau_{\text{veto}}$，则将整条序列从训练中剔除。这种”token 粒度检测、sequence 粒度否决”的设计体现了一种”一票否决”的保守策略。</p>
  </li>
  <li>
    <p><a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf">INTELLECT-3 Technical Report</a> 在其异步分布式 RL 训练框架中采用了类似的拒绝采样策略。INTELLECT-3 对每条 rollout 计算 <strong>token-level</strong> 重要性比率，若任意 token 的比率低于阈值（文中使用 $10^{-5}$），则对整条轨迹进行 masking。</p>
  </li>
</ul>

<h2 id="三策略-trpo-视角下的最小统一理解">三策略 TRPO 视角下的最小统一理解</h2>

<p>上面列的这些工作，看上去各自解决的是：</p>

<ul>
  <li>算法层：PPO / GRPO 的目标怎么写，token-level 还是 sequence-level，用 clip 还是 mask；</li>
  <li>系统层：推理框架和训练框架怎样对齐；</li>
  <li>模型层：MoE 模型路由问题如何放大训练不稳定，等等。</li>
</ul>

<p>但如果我们把“行为策略 vs 参考策略”这条线拉直，会发现相当一部分问题，其实都可以放到一个相对简单的理论框架里理解：<strong>三策略 TRPO</strong>。</p>

<p>下面这节我会用尽量简单的数学，把这个三策略版 TRPO 摊开——它可以被看作是“TRPO + 三角不等式”的一个小扩展，但在分析大模型 RL 里的训推不一致时非常好用：</p>

<ul>
  <li>一方面让我们重新理解“训推不一致”和“异步训练框架”到底在影响什么；</li>
  <li>另一方面，也帮我们统一理解 TIS、IcePop、sequence-level MIS 等，在本文的视角下，它们其实都是在实施下文的“<strong>约束 2</strong>”。</li>
</ul>

<h3 id="三个策略">三个策略</h3>

<p>沿用前文的记号，我们在一个折扣 MDP 上工作，折扣因子为 $\gamma\in(0,1)$：</p>

<ul>
  <li>状态 $s\in\mathcal{S}$，动作 $a\in\mathcal{A}$；</li>
  <li>策略 $\pi(a\mid s)$；</li>
  <li>折扣状态分布：
\(d_\pi(s) := (1-\gamma)\sum_{t=0}^\infty \gamma^t \Pr_\pi(s_t = s)。\)</li>
  <li>回报（episode 视角）：
\(\mathcal{J}(\pi) := \mathbb{E}_\pi\Big[\sum_{t=0}^\infty \gamma^t r_t\Big]。\)</li>
  <li>值函数 / 优势函数：
\(V_\pi(s),\quad Q_\pi(s,a),\quad A_\pi(s,a) := Q_\pi(s,a) - V_\pi(s)。\)</li>
</ul>

<p>稍微赘述一下，在“三策略”设定里，我们有：</p>

<ul>
  <li><strong>行为策略</strong>（behavior policy）：$\mu$，真正用来 rollout 的策略；数据 $(s,a,r,\dots)$ 都是从它来的。</li>
  <li><strong>参考策略</strong>（reference policy）：$\pi_{\theta_{\text{old}}}$，优化目标里拿来做 ratio、clip 或 KL 约束的那一份“旧策略”。</li>
  <li><strong>目标策略</strong>（target policy）：$\pi_\theta$，我们这一步想要优化的策略。</li>
</ul>

<p>在理想设定里我们默认 $\mu = \pi_{\theta_{\text{old}}}$；现实系统里二者往往不等，这就是“训推不一致”的数学影子。</p>

<h3 id="两策略-trpo">两策略 TRPO</h3>

<blockquote>
  <p>熟悉 TRPO 的读者可以直接跳到后面的“三策略 TRPO”小节。</p>
</blockquote>

<p>TRPO 的所有理论保证，都是建立在<strong>某个“基准策略”的优势函数</strong>之上的。既然实际能算清楚的<strong>只有</strong> $A_\mu$（数据是按 $\mu$ 采的），那我们就直接把 $\mu$ 当成基准。</p>

<p>一个经典的结论是 <strong>性能差分引理（Performance Difference Lemma）</strong>：</p>

<blockquote>
  <p>对任意两策略 $\mu$ 和 $\pi_\theta$，有</p>

\[\mathcal{J}(\pi_\theta) - \mathcal{J}(\mu)
= \frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d_{\pi_\theta},\, a\sim\pi_\theta}[A_\mu(s,a)]。\]
</blockquote>

<p>直觉非常简单：</p>

<ul>
  <li>$A_\mu(s,a)$ 就是在说“如果在 $s$ 里本来按 $\mu$ 行动，现在换成动作 $a$，长期回报会多或少多少”；</li>
  <li>把所有时刻、所有状态、所有动作的“增益”累积起来，就得到新策略比行为策略总共赚了多少。</li>
</ul>

<p>TRPO 的问题在于，我们没法准确算</p>

\[\mathbb{E}_{s\sim d_{\pi_\theta}, a\sim\pi_\theta}[A_\mu(s,a)]，\]

<p>因为 $d_{\pi_\theta}$ 是“新策略”的状态分布，我们没有在它下面采样过。</p>

<p>于是 TRPO 引入了一个替代目标：把状态分布换成行为策略的：</p>

\[L_\mu(\pi_\theta)
:= \mathcal{J}(\mu) + \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_\mu,\,a\sim \pi_\theta}[A_\mu(s,a)]。\]

<p>$L_\mu$ 的直觉解释是：在行为策略的状态分布下，让新策略试着去选动作，看优势有多大。</p>

<p>从性能差分引理出发，两者之差是：</p>

\[\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)
= \frac{1}{1-\gamma}\;
  \sum_s \big(d_{\pi_\theta}(s) - d_\mu(s)\big)
  \,\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[A_\mu(s,a)]。\]

<p>如果我们定义</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|，\]

<p>那么有一个直接的上界：</p>

<blockquote>
  <p><strong>Lemma 1</strong></p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\;
    \|d_{\pi_\theta} - d_\mu\|_1。\]
</blockquote>

<p>这里出现了第一个关键量：</p>

<blockquote>
  <p><strong>状态分布偏移</strong> $|d_{\pi_\theta} - d_\mu|_1$，也就是“新策略和行为策略看到的世界，到底差了多少”。</p>
</blockquote>

<p>我们通常不会直接对 $|d_{\pi_\theta} - d_\mu|_1$ 施加约束，反而是对“每一步 action 分布”的差异施加约束，比如 trust region、KL、clip 等。</p>

<p>记总变差距离（total variation）：</p>

\[D_{\mathrm{TV}}(p,q) := \frac{1}{2}\|p-q\|_1。\]

<p>假设存在常数 $\beta$，使得</p>

<blockquote>
  <p>对所有 $s$，行为策略和目标策略之间的 TV 被 $\beta$ 上界：</p>

\[D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big) \le \beta。\]
</blockquote>

<p>直观含义：在任意状态里，“新策略”和“生成数据的策略”选动作的分布都不会离太远。</p>

<p>一个经典结果（可以用 coupling 证明）是：</p>

<blockquote>
  <p><strong>Lemma 2</strong>
在上述条件下有</p>

\[\|d_{\pi_\theta} - d_\mu\|_1
\le \frac{2\gamma}{1-\gamma}\,\beta。\]
</blockquote>

<p>把它和 Lemma 1 结合：</p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\; \frac{2\gamma}{1-\gamma}\,\beta
= \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\beta。\]

<p>于是我们得到一个形式上相当简洁的<strong>两策略 TRPO 下界（基准为行为策略）</strong>：</p>

<blockquote>
  <p><strong>Theorem 1（两策略 TRPO）</strong></p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
\frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\beta。\]
</blockquote>

<p>这说明：</p>

<ul>
  <li><strong>真正决定“替代目标 $L_\mu$ 靠不靠谱”的，是行为策略 $\mu$ 和目标策略 $\pi_\theta$ 的差异：</strong>
\(\beta = \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big)。\)</li>
</ul>

<p>如果你能直接约束住这个 $\beta$，就能直接把 TRPO 的单调性保证搬到行为策略视角下。</p>

<h3 id="三策略-trpo">三策略 TRPO</h3>

<p>现实问题在于：<strong>大模型强化学习训练里我们可能无法直接控制 $\beta$ 本身。</strong></p>

<p>在大部分 PPO / GRPO / GSPO / 现有 RLHF 框架里，实际发生的是：</p>

<ul>
  <li>rollout 数据是由某个<strong>行为策略</strong> $\mu$ 产生的（推理引擎里的“那一版参数” + 若干系统细节）；</li>
  <li>更新时，我们希望利用<strong>参考策略</strong> $\pi_{\theta_{\text{old}}}$ 来限制<strong>目标策略</strong> $\pi_\theta$ 的更新幅度。</li>
</ul>

<p>也就是说，实际可以“动手”的是两个量：</p>

<ol>
  <li><strong>参考 vs 目标</strong>：我们可以通过 KL / clip 等手段控制
\(D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)。\)</li>
  <li><strong>行为 vs 参考</strong>：我们希望<strong>间接</strong>控制
\(D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)。\)</li>
</ol>

<p>于是自然就定义两个“proxy 差异”：</p>

<ul>
  <li><strong>约束 1：参考 vs 目标</strong>
\(\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big)；\)</li>
  <li><strong>约束 2：行为 vs 参考</strong>
\(\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big)。\)</li>
</ul>

<p>直觉上：</p>

<ul>
  <li>$\alpha_0$：新策略到底离“你宣称的那份旧策略”有多远——这就是 trust region 控制的那部分；</li>
  <li>$\alpha_1$：你用来训练的参考策略，到底跟真实采样时的行为策略差了多少——这就是训推不一致或异步的影子。</li>
</ul>

<p>现在，可以把这两个量塞回 TRPO 的下界里。</p>

<p>对任意状态 $s$，有</p>

\[\begin{aligned}
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
&amp;\le
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)
\\
&amp;\quad +
D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)。
\end{aligned}\]

<p>对 $s$ 取上确界：</p>

\[\beta
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
\;\le\;
\alpha_1 + \alpha_0。\]

<p>把这个不等式塞回两策略 TRPO 的结论（Theorem 1）里，记</p>

\[C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}，\]

<p>即得到：</p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,\beta
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,(\alpha_0 + \alpha_1)。\]

<p>于是，我们得到一个非常直接的<strong>三策略 TRPO 下界</strong>：</p>

<blockquote>
  <p><strong>Theorem 2（三策略 TRPO）</strong>
记</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|,\quad
C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}，\]

  <p>以及</p>

\[\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big)，
\quad
\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big)。\]

  <p>则对任意目标策略 $\pi_\theta$ 有</p>

\[\boxed{
\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\; C\,(\alpha_0 + \alpha_1)
}\]

  <p>其中</p>

\[L_\mu(\pi_\theta)
:=
\mathcal{J}(\mu) + \frac{1}{1-\gamma}
  \mathbb{E}_{s\sim d_\mu,a\sim\pi_\theta}[A_\mu(s,a)]。\]
</blockquote>

<p>这个结论的含义其实很直接：</p>

<ul>
  <li><strong>替代目标 $L_\mu(\pi_\theta)$ 与真实性能 $\mathcal{J}(\pi_\theta)$ 之间的 gap，可以拆成两部分：</strong>
    <ul>
      <li>参考 vs 目标的偏移 $\alpha_0$；</li>
      <li>行为 vs 参考的偏移 $\alpha_1$。</li>
    </ul>
  </li>
</ul>

<p>只要这两个量都小，<strong>优化 $L_\mu$ 就有希望有效提升 $\mathcal{J}$</strong>。</p>

<h3 id="这两个差异各自怎么约束">这两个差异各自怎么约束？</h3>

<p>现在，我们可以从 Theorem 2 回头看各种实际方法：</p>

<ul>
  <li>绝大多数 “PPO / GRPO / GSPO” 类工作，其实是在控制 <strong>约束 1：$\alpha_0$</strong>；</li>
  <li>绝大多数 “TIS / IcePop / MIS” 类工作，在本文的统一视角下，可以理解为主要是在控制 <strong>约束 2：$\alpha_1$</strong>。</li>
</ul>

<p>本文下面只讨论 <strong>约束 2</strong>。</p>

<p>约束 2 的目标是：<strong>保证用来训练的数据，尽可能来自“接近参考策略”的行为策略。</strong></p>

<p>这里通常既有<strong>系统层</strong>的机制，也有<strong>算法层（importance sampling）</strong>的机制。</p>

<ol>
  <li><strong>系统层：让行为策略别飘太远</strong>
    <ul>
      <li>异步框架：给每个样本打上策略版本号，只能用与 $\pi_{\theta_{\text{old}}}$ 相差不大的参数版本采样的数据；</li>
      <li>训推对齐：强调训练框架和推理框架用相同精度、相同算子、相近的内核 / kernel 行为。</li>
    </ul>

    <p>这些机制的目标是：从“算法外部”让 $\mu$ 和 $\pi_{\theta_{\text{old}}}$ 靠近，从而压缩 $\alpha_1$。</p>
  </li>
  <li>
    <p><strong>算法层：样本修正</strong></p>

    <p>在算法层，我们不再试图“纠正整个行为策略”，而是用重要性采样比率在<strong>样本层面</strong>做筛选和重加权，让“真正参与训练的样本子集”上的行为策略尽量接近参考策略，或者减小差异较大的样本在训练上的权重。</p>

    <p>具体来说，就是下面这些方法，它们本质上都可以看作是“实现约束 2 的不同方式”。</p>
  </li>
</ol>

<h2 id="重要性采样与掩码四种约束-2-实现">重要性采样与掩码：四种约束 2 实现</h2>

<p>下面延续前文的记号体系来写这三种方法的目标函数，只聚焦在“行为策略 vs 参考策略”这一维的设计。记 token 级的 PPO / GRPO 风格更新项为</p>

\[g_\theta(t)
= \min\big(r_t(\theta) A_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon) A_t\big),\]

<p>其中</p>

\[r_t(\theta) = \frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)},
\quad (s_t,a_t)\sim\mu,\quad A_t := A_\mu(s_t,a_t)。\]

<p>也就是说：</p>

<ul>
  <li>$r_t(\theta)$ 是 <strong>目标 vs 参考</strong> 的比率（对应约束 1）；</li>
  <li>$A_t$ 基于行为策略采样的数据，是我们能估到的优势函数。</li>
</ul>

<p>为了把 token 级的 $(s_t,a_t)$ 与序列级的 $(x,y)$ 记号打通，在以 RLHF（reinforcement learning from human feedback，人类反馈强化学习）为代表的 LLM-RL 设定中，我们约定：</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>prompt 记为 $x$；回复记为 $y = (y_1,\dots,y_{</td>
          <td>y</td>
          <td>})$；</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>token 级状态 $s_t := (x, y_{\lt t})$，动作 $a_t := y_t$；</li>
  <li>因此行为策略和参考策略在序列上的分布可写成
\(\mu(y\mid x) = \prod_{t=1}^{|y|}\mu(a_t=y_t\mid s_t),\quad
\pi_{\theta_{\text{old}}}(y\mid x) = \prod_{t=1}^{|y|}\pi_{\theta_{\text{old}}}(a_t=y_t\mid s_t)。\)</li>
</ul>

<p>此外，为了描述“参考 vs 行为”的偏移，统一定义 token 级重要性比率</p>

\[\rho_t^{(\text{ref}\leftarrow\text{beh})} :=
\frac{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}{\mu(a_t\mid s_t)}，\]

<p>以及其对应的序列级版本</p>

\[\rho(y\mid x) := \frac{\pi_{\theta_{\text{old}}}(y\mid x)}{\mu(y\mid x)}
= \prod_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})}。\]

<p>接下来，TIS / IcePop / MIS 的区别，就体现在“如何利用这些 $\rho$ 来实现约束 2”。</p>

<h3 id="1-tistoken-level-截断-is">1. TIS：token-level 截断 IS</h3>

<p>TIS 直接对上述 $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ 做截断，记</p>

\[\color{blue}{w_t = \min\big(\rho_t^{(\text{ref}\leftarrow\text{beh})},\ C_{\text{IS}}\big)}。\]

<p>更新目标写成</p>

\[L_{\text{TIS}}(\theta)
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{w_t}\; g_\theta(t)\big]。\]

<ul>
  <li>蓝色的 $\color{blue}{w_t}$ 是被截断的 IS 权重：极端大的比率被压到常数 $C_{\text{IS}}$。</li>
  <li>从三策略 TRPO 的角度看，这相当于在 <strong>token 分布</strong> 上“软削弱”行为策略和参考策略严重不一致的样本，从而在梯度中有效减小那部分样本对 $\alpha_1$ 的贡献。</li>
</ul>

<h3 id="2-icepopmoe-场景下的-token-level-双侧-mask">2. IcePop：MoE 场景下的 token-level 双侧 Mask</h3>

<p>IcePop 同样以 $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ 为度量，但采用 <strong>双侧掩码</strong>：</p>

\[\color{blue}{m_t = \mathbf{1}\big[C_{\text{low}} \le \rho_t^{(\text{ref}\leftarrow\text{beh})} \le C_{\text{high}}\big]}。\]

<p>更新目标写成</p>

\[L_{\text{IcePop}}(\theta)
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{m_t}\; g_\theta(t)\big]。\]

<ul>
  <li>蓝色的 $\color{blue}{m_t}$ 决定某个 token 是否参与更新：比率太大或太小的 token 直接被丢弃。</li>
  <li>这相当于硬性裁掉“行为策略和参考策略极度不一致”的 token，只在 $\rho_t$ 适中的区域上优化，从样本集合层面实施更强的“约束 2”。</li>
</ul>

<h3 id="3-sequence-level-mis按整条序列-mask-的重要性采样">3. sequence-level MIS：按整条序列 Mask 的重要性采样</h3>

<p>MIS 的核心操作是：<strong>只保留 IS 比率不超过阈值 $C$ 的序列，其余序列的损失直接置零</strong>。写成</p>

\[\color{blue}{
\rho(y\mid x)
\leftarrow
\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}
}\]

<p>在统一的损失形式下，可以写成</p>

\[L_{\text{MIS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}}
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big],\]

<p>简而言之：</p>

<ul>
  <li>对于 <strong>IS 比率较小的序列</strong>：保留完整的 $\rho(y\mid x)$ 权重，正常做 off-policy 修正；</li>
  <li>对于 <strong>IS 比率超过阈值 $C$ 的序列</strong>：整个序列的 policy loss 被 mask 掉（权重变成 $0$）。</li>
</ul>

<p>从三策略 TRPO 的角度看，MIS 不再在 token 上做截断，而是直接在<strong>序列级</strong>筛掉“行为策略和参考策略严重不一致”的轨迹，只在 $\rho(y\mid x)\le C$ 的子分布上优化，从而在 trajectory 粒度上实现对“约束 2”（$\mu$ vs $\pi_{\theta_{\text{old}}}$ 偏移）的控制。</p>

<h3 id="4-worst-token-reject-sampling按最差-token-拒绝整条序列">4. Worst Token Reject Sampling：按最差 token 拒绝整条序列</h3>

<p>verl 中的 veto 机制 与 INTELLECT-3 分别在各自的训练框架中采用了一种可统称为 <strong>Worst Token Reject Sampling（WTRS）</strong> 的拒绝采样策略：</p>

<ul>
  <li>
    <p><strong>verl Token Veto</strong>：在其 rollout correction 模块中，若轨迹中存在任意 token 使得 $\min_t \rho_t &lt; \tau_{\text{veto}}$，则通过 response<em>mask 将整条序列剔除。阈值 $\tau</em>{\text{veto}}$ 可由用户配置。</p>
  </li>
  <li>
    <p><strong>INTELLECT-3 Token Masking</strong>：在其异步分布式 RL 框架中，若任意 token 的比率低于 $10^{-5}$，则对整条轨迹进行 masking。</p>
  </li>
</ul>

<p>二者的核心操作一致：<strong>若轨迹中存在任意 token 的 IS 比率低于阈值 $\tau$，则将整条序列从训练中剔除</strong>。写成</p>

\[\color{blue}{
m(y\mid x) = \mathbf{1}\Big\{\min_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})} \ge \tau\Big\}
}\]

<p>在统一的损失形式下，可以写成</p>

\[L_{\text{WTRS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{m(y\mid x)}
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big],\]

<p>简而言之：</p>

<ul>
  <li>对于 <strong>所有 token 的 IS 比率均不低于 $\tau$ 的序列</strong>：正常参与训练；</li>
  <li>对于 <strong>存在任意 token 的 IS 比率低于 $\tau$ 的序列</strong>：整条序列的 policy loss 被 mask 掉。</li>
</ul>

<p>从三策略 TRPO 的角度看，WTRS 采用了”token 粒度检测、sequence 粒度否决”的混合策略：在 <strong>token-level</strong> 检测极端不一致的信号，一旦发现则在 <strong>sequence-level</strong> 执行拒绝。这种”一票否决”的设计体现了一种保守思路——当轨迹中存在”行为策略生成但参考策略几乎不可能生成”的 token 时，<strong>整条轨迹的可信度都将受到质疑</strong>，从而在 trajectory 粒度上实现对”约束 2”（$\mu$ vs $\pi_{\theta_{\text{old}}}$ 偏移）的控制。</p>

<h2 id="moe-路由回放它在三策略-trpo-中到底做了什么">MoE 路由回放：它在三策略 TRPO 中到底做了什么？</h2>

<p>在 MoE（Mixture-of-Experts）模型上，训推不一致往往首先表现为<strong>路由不一致（routing inconsistency）</strong>：即便参数相同，推理端与训练端也可能因为算子、并行或数值细节的微小差异而路由到不同专家。一个很自然的工程应对是<strong>路由回放（routing replay）</strong>：在 rollout（推理）时记录实际命中的专家路径，训练时强制复用这些路由决策。</p>

<p>这类方法经常被直觉性地理解为“在实现约束 2、压小 $\alpha_1$”。但从三策略 TRPO 的视角看，更准确的说法是：</p>

<blockquote>
  <p><strong>路由回放并不是在原 surrogate objective 上收紧约束，而是在把 surrogate objective 改写成另一个“带路由条件/替换”的目标。</strong>
它让路由不一致在 loss 里“不可见”，但并没有让真实策略距离里的 $\alpha_0$ 或 $\alpha_1$ 变小。</p>
</blockquote>

<p>下面用一个<strong>尽量简单</strong>但足够说明问题的建模来把这件事写清楚。</p>

<h3 id="moe-下的-surrogate-objective把路由和token-生成拆开">MoE 下的 surrogate objective：把“路由”和“token 生成”拆开</h3>

<p>把 MoE 抽象成两阶段随机决策：“先选专家 $z$，再在该专家条件下生成 token $a$”。
因此目标策略可以分解为</p>

\[\pi_\theta(a,z\mid s)=\omega_\theta(z\mid s)\,\pi_\theta(a\mid s,z),\]

<p>其中：</p>

<ul>
  <li>$\omega_\theta(z\mid s)$ 是路由器（router）的分布；</li>
  <li>$\pi_\theta(a\mid s,z)$ 是在专家 $z$ 条件下的 token 分布。</li>
</ul>

<p>在三策略 TRPO 中，我们真正想优化的 surrogate objective 为</p>

\[L_\mu(\pi_\theta) = \mathcal{J}(\mu) + \frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_z \omega_\theta(z\mid s)\,F_\theta(s,z)
\bigg],\]

<p>其中我把专家层的优势聚合写成</p>

\[F_\theta(s,z)
:=
\sum_a \pi_\theta(a\mid s,z)\,A_\mu(s,a,z).\]

<p>关键点：<strong>在原始的 $L_\mu(\pi_\theta)$ 里，路由分布是当前要更新的 $\omega_\theta$</strong>。也就是说，MoE 的 RL 训练不仅在更新 token 生成分布，也在更新路由器本身。</p>

<h3 id="1回放行为策略的路由behavior-router-replay--r3-类">1）回放行为策略的路由（behavior-router replay / R3 类）</h3>

<p>R3 的做法是：rollout 时记录推理端实际命中的专家集合 $M_\mu(s)$，训练时强制当前策略<strong>只在该集合内路由</strong>。可以把它写成对路由分布的“条件化投影”：</p>

\[\omega_\theta^{\text{R3}}(z\mid s)
:=
\frac{\omega_\theta(z\mid s)\,\mathbf{1}\{z\in M_\mu(s)\}}
     {\sum_{z'\in M_\mu(s)}\omega_\theta(z'\mid s)} .\]

<p>从而训练时实际优化的 surrogate objective 变为</p>

\[L_\mu^{\text{R3}}(\pi_\theta) =
\mathcal{J}(\mu) +
\frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_{z\in M_\mu(s)} \omega_\theta^{\text{R3}}(z\mid s)\,F_\theta(s,z)
\bigg].\]

<p>和原始 $L_\mu(\pi_\theta)$ 对比可以看到，R3 并没有让 $\omega_\theta$ 逼近 $\omega_{\text{old}}$ 或 $\omega_\mu$；它做的是：</p>

<ul>
  <li><strong>把对 $z\sim\omega_\theta$ 的期望，改成了对 $z\sim\omega_\theta(\cdot\mid z\in M_\mu(s))$ 的条件期望</strong>；</li>
  <li>等价地说，把路由的可行 support 缩到了 $M_\mu(s)$。</li>
</ul>

<p>因此 R3 训练的是一个“被行为路由集合条件化后的 surrogate objective”，而不是原来的 $L_\mu(\pi_\theta)$。
好处是显著降方差、提升稳定性；代价是<strong>在每个状态上都收缩了路由器探索 / 更新的自由度</strong>。</p>

<h3 id="2回放参考策略的路由reference-router-replay">2）回放参考策略的路由（reference-router replay）</h3>

<p>另一类 routing replay 复用的是参考策略（old policy）的路由器 $\omega_{\text{old}}$。这等价于训练一个混合策略</p>

\[\hat\pi_\theta(a,z\mid s)
:=
\omega_{\text{old}}(z\mid s)\,\pi_\theta(a\mid s,z),\]

<p>对应 surrogate objective 为</p>

\[L_\mu^{\text{ref-replay}}(\pi_\theta) =
\mathcal{J}(\mu) +
\frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_z \omega_{\text{old}}(z\mid s)\,F_\theta(s,z)
\bigg].\]

<p>这意味着：</p>

<ul>
  <li>在 surrogate objective 中，路由器被<strong>固定为旧路由器</strong>，路由相关的“参考 vs 目标”差异在 loss 里被直接抹掉；</li>
  <li>训练对“新路由器 $\omega_\theta$ 是否偏离 $\omega_{\text{old}}$”不再敏感，于是路由不一致导致的不稳定被绕开。</li>
</ul>

<p>但注意这同样是<strong>换目标</strong>：</p>

<ul>
  <li>真实策略空间里的 $\alpha_0$ 并没有因此变小，只是被“用旧路由器重定义目标”而在 loss 中不可见；</li>
  <li>路由器的学习被强行冻结或极度削弱。</li>
</ul>

<h3 id="路由回放只是在改写-surrogate-objective">路由回放只是在改写 surrogate objective</h3>

<p>把两类 replay 放在一起看，它们的共同点是：</p>

<ol>
  <li><strong>优化的都不是原始的 $L_\mu(\pi_\theta)$</strong>，而是某个“路由被条件化 / 替换后的 surrogate objective”。</li>
  <li><strong>它们没有直接收缩三策略 TRPO 下界里的 $\alpha_0,\alpha_1$</strong>。replay 让路由不匹配不再显式出现在 loss 中，但不匹配在真实策略距离里仍然存在。</li>
  <li><strong>实践上是在“用偏差换方差”</strong>：回放往往显著降低方差、提升稳定性，但也可能限制了 MoE 在 RL 目标下学到更优的路由模式。</li>
</ol>

<p>所以，从三策略 TRPO 的视角，更准确的理解是：</p>

<blockquote>
  <p><strong>routing replay 是一种 surrogate objective 的改写，而不是对 $\alpha_0$ 或 $\alpha_1$ 的直接实现。</strong></p>
</blockquote>

<h2 id="小结">小结</h2>

<p>如果把这篇文章压缩成一句话，就是：</p>

<blockquote>
  <p><strong>许多“大模型 RL 训推不一致”和“异步训练”问题，在本文的视角下，其实都可以理解为：在 TRPO 框架下，当行为策略 $\mu$ 和参考策略 $\pi_{\theta_{\text{old}}}$ 不一致时，二者之间的偏移（$\alpha_1$）被严重低估了。</strong></p>
</blockquote>

<p>从两策略到三策略，我们做的事情其实很简单：</p>

<ul>
  <li>把 TRPO 的下界从“旧策略 vs 新策略”的叙述，改写成“<strong>行为策略 – 参考策略 – 目标策略</strong>”三者的关系；</li>
  <li>显式地拆出了两个 TV 距离：
    <ul>
      <li><strong>约束 1：参考 vs 目标</strong> $\alpha_0$，对应 PPO / GRPO / GSPO 等工作里最常见的 KL / clip / trust region；</li>
      <li><strong>约束 2：行为 vs 参考</strong> $\alpha_1$，对应异步框架、训推差异、MoE 路由、kernel 非确定性等现实因素；</li>
    </ul>
  </li>
  <li>得到了一个非常直接的结论：
替代目标 $L_\mu(\pi_\theta)$ 和真实性能 $\mathcal{J}(\pi_\theta)$ 的 gap 正比于 $\alpha_0 + \alpha_1$。</li>
</ul>

<p>在这个视角下（当然这只是众多可能视角之一）：</p>

<ul>
  <li>Decoupled PPO / AReaL 可以被看作是在<strong>形式上承认“三策略存在”</strong>，并尝试在目标函数上将“行为分布”和“参考策略”解耦；</li>
  <li>TIS、IcePop、MIS、WTRS 则是通过 IS 或者掩码机制在样本层面实施”约束 2”：
    <ul>
      <li>TIS：用 token-level 截断权重削弱比率过大样本的影响；</li>
      <li>IcePop：在 MoE 场景下用 token-level 双侧掩码硬性丢弃”极端不一致”的 token；</li>
      <li>MIS：在 sequence-level 直接屏蔽整条”比率过大”的轨迹；</li>
      <li>WTRS：在 token-level 检测比率过小的信号，一旦发现则在 sequence-level 拒绝整条轨迹；</li>
    </ul>
  </li>
  <li><strong>routing replay（路由回放）在三策略 TRPO 的视角下更像是“改写 surrogate objective”而非“直接实现约束”</strong>：无论回放行为路由（R3 类）还是回放参考路由，它们都把原本的 $L_{\mu}(\pi_{\theta})$ 改成了一个路由被条件化/替换后的 surrogate objective，用<strong>一定的目标偏差与路由学习自由度的收缩</strong>换取<strong>降低方差与提升稳定性</strong>。因此它并不会真正收缩 $\alpha_0$ 或 $\alpha_1$，而是让路由不一致在 loss 中“不可见”；</li>
  <li>《RL 老训崩？训推差异是基石》、以及前文提到的 <em>Defeating Nondeterminism in LLM Inference</em> 等工程经验，则可以理解为在<strong>系统侧和数值实现侧</strong>，尽可能把 $\alpha_1$ 压低，让算法层的假设不至于完全失效。</li>
</ul>

<p>从这个统一视角出发，也许有助于回答几个实际问题（这里只是抛几个开放性问题）：</p>

<ul>
  <li>在什么条件下，我们还能把“大模型 RL 训练”理解成某种意义上的“近似 TRPO / PPO”？</li>
  <li>对一个具体的 RL 系统，我们究竟应该把主要精力花在：
    <ul>
      <li>收紧 $\alpha_0$（更强的 KL / 更稳的 sequence-level 目标），还是</li>
      <li>压低 $\alpha_1$（更一致的训推框架、更激进的 MIS / TIS / IcePop）？</li>
    </ul>
  </li>
  <li>在 MoE、异步采样、复杂 agent workflow 这些现实设定下，我们还能安全地假装“$\mu \approx \pi_{\theta_{\text{old}}}$”多久？</li>
</ul>

<p>本文只是在 TRPO 这个老框架上做了一个非常“<strong>最小化</strong>”的延展，把“三策略”显式写出来，并用它来整理现有的一些工作。难免有理解偏差或遗漏之处，如果你也关注实际大模型 RL 训练的情况，欢迎把你自己的设定抽象成“$\mu,\pi_{\theta_{\text{old}}},\pi_\theta$ 三者的关系”，再回头看看 Theorem 2 里的那条不等式，或许会有不一样的直观感受。</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025ThreePolicyTRPO</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{From Two Policies to Three: Extending TRPO under Behavior-Reference Policy Mismatch in LLM RL}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html}</span><span class="p">,</span>
  <span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-11-23}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[现代 LLM RL 流程常常在"旧策略"悄然偏离实际生成 rollout 的行为策略时进行训练，破坏了通常的同策略假设。本文将经典的 TRPO 下界改写为三策略形式——行为策略、参考策略和目标策略——使得性能差距可以分解为两个可以推理和控制的 TV 距离。在这一视角下，Decoupled PPO、AReaL、TIS、IcePop、sequence-level MIS、最坏 Token 拒绝采样 (WTRS)、MoE 路由回放等方法，以及常见的训推对齐工程技巧，都可以看作是缩小这两个偏差的不同方式。]]></summary></entry></feed>