<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://xihuai18.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xihuai18.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-11-29T19:19:30+00:00</updated><id>https://xihuai18.github.io/feed.xml</id><title type="html">Xihuai Wangâ€™s Page</title><subtitle>Xihuai&apos;s personal page.
</subtitle><entry xml:lang="zh"><title type="html">ä»ä¸¤ç­–ç•¥åˆ°ä¸‰ç­–ç•¥ï¼šLLM RL ä¸­è¡Œä¸ºç­–ç•¥â€“å‚è€ƒç­–ç•¥ä¸ä¸€è‡´ä¸‹çš„ TRPO æ‰©å±•</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-cn.html" rel="alternate" type="text/html" title="ä»ä¸¤ç­–ç•¥åˆ°ä¸‰ç­–ç•¥ï¼šLLM RL ä¸­è¡Œä¸ºç­–ç•¥â€“å‚è€ƒç­–ç•¥ä¸ä¸€è‡´ä¸‹çš„ TRPO æ‰©å±•" /><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-cn</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-cn.html"><![CDATA[<ul>
  <li>TOC</li>
</ul>

<p><a href="/reinforcement-learning/2025/11/15/three-policy-en.html">English Version</a> | <a href="https://zhuanlan.zhihu.com/p/1973206684907365344">çŸ¥ä¹ç‰ˆæœ¬ <img src="https://static.zhihu.com/heifetz/favicon.ico" alt="Zhihu" /></a></p>

<p><img src="/assets/img/three-policy-mini-class-cn.jpg" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<h2 id="è®­æ¨ä¸ä¸€è‡´å’Œå¼‚æ­¥æ¡†æ¶">è®­æ¨ä¸ä¸€è‡´å’Œå¼‚æ­¥æ¡†æ¶</h2>

<p>æœ€è¿‘çœ‹åˆ°ä¸å°‘å…³äºå¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­â€œè®­æ¨ä¸ä¸€è‡´â€å’Œâ€œå¼‚æ­¥è®­æ¨æ¡†æ¶â€çš„è®¨è®ºï¼Œæˆ‘è‡ªå·±çš„ç›´è§‰æ˜¯ï¼šè¿™äº›çœ‹ä¸Šå»å¤æ‚å¤šæ ·çš„é—®é¢˜ï¼Œå¾ˆå¤§ä¸€éƒ¨åˆ†å…¶å®éƒ½å›´ç»•ç€ä¸€ä¸ªæ›´åŸºç¡€çš„çŸ›ç›¾â€”â€”<strong>è¡Œä¸ºç­–ç•¥ï¼ˆbehavior policyï¼‰å’Œå‚è€ƒç­–ç•¥ï¼ˆreference policyï¼‰ä¸ä¸€è‡´ã€‚</strong></p>

<p>æœ¬æ–‡å…ˆç®€å•æ¢³ç†ä¸€ä¸‹æˆ‘ç›®å‰çœ‹åˆ°çš„ç›¸å…³å·¥ä½œï¼Œç„¶åå†å°è¯•ä»â€œè¡Œä¸ºç­–ç•¥ vs å‚è€ƒç­–ç•¥â€çš„è§’åº¦ï¼ŒæŠŠå®ƒä»¬ä¸²åˆ°åŒä¸€æ¡çº¿ä¸Šï¼Œä¸ºè¯»è€…æä¾›ä¸€ä¸ªè¡¥å……è§†è§’ã€‚</p>

<p>åœ¨æœ¬æ–‡ä¸­æˆ‘ä¼šç”¨ï¼š</p>

<ul>
  <li><strong>è¡Œä¸ºç­–ç•¥</strong> $\mu$ï¼šå®é™…è´Ÿè´£ç”Ÿæˆ rollout çš„ç­–ç•¥ï¼Œä¹Ÿå°±æ˜¯â€œä½ åœ¨ä»€ä¹ˆåˆ†å¸ƒä¸‹é‡‡æ ·åˆ°äº†è¿™äº›æ•°æ®â€ã€‚åœ¨ç°ä»£ LLM-RL ç³»ç»Ÿé‡Œï¼Œå®ƒå¯¹åº”çš„æ˜¯æ¨ç†å¼•æ“é‡Œçš„é‚£å¥—å®ç°ï¼ˆvLLM / SGLang ç­‰ï¼‰ï¼Œåœ¨å¼‚æ­¥æ¡†æ¶ä¸‹å¾€å¾€è¿˜æ˜¯<strong>å¤šä¸ª worker ç­–ç•¥çš„æ··åˆåˆ†å¸ƒ</strong>ã€‚</li>
  <li><strong>å‚è€ƒç­–ç•¥</strong> $\pi_{\theta_{\text{old}}}$ï¼šè®­ç»ƒç›®æ ‡é‡Œæ‹¿æ¥åšé‡è¦æ€§é‡‡æ ·ã€clipping æˆ– KL çº¦æŸçš„ç­–ç•¥ï¼Œå…¸å‹åœ°å°±æ˜¯ PPO / GRPO é‡Œçš„â€œæ—§ç­–ç•¥â€ï¼ˆold policyï¼‰ã€‚</li>
  <li><strong>ç›®æ ‡ç­–ç•¥</strong> $\pi_\theta$ï¼šè®­ç»ƒç›®æ ‡é‡Œè¦ä¼˜åŒ–çš„ç­–ç•¥ï¼Œä¹Ÿå°±æ˜¯â€œä½ æƒ³è®©æ¨¡å‹å˜æˆä»€ä¹ˆæ ·â€ã€‚å…¸å‹åœ°å°±æ˜¯ PPO / GRPO é‡Œçš„â€œæ–°ç­–ç•¥â€ï¼ˆnew policyï¼‰ã€‚</li>
</ul>

<p>åœ¨æœ€ç»å…¸ã€ç†æƒ³åŒ–çš„è®¾å®šé‡Œï¼Œæˆ‘ä»¬é€šå¸¸<strong>é»˜è®¤</strong> $\mu = \pi_{\theta_{\text{old}}}$ã€‚ä½†åœ¨ç°å®ç³»ç»Ÿä¸­ï¼Œå—å¼‚æ­¥æ›´æ–°ã€ä¸åŒæ¨ç† / è®­ç»ƒåç«¯ã€MoE è·¯ç”±æ³¢åŠ¨ç”šè‡³ç¡¬ä»¶æ•°å€¼å·®å¼‚ç­‰å› ç´ å½±å“ï¼ŒäºŒè€…å¾€å¾€ä¼šå‡ºç°ä¸åŒç¨‹åº¦çš„åç¦»ã€‚</p>

<h2 id="ç›¸å…³å·¥ä½œ">ç›¸å…³å·¥ä½œ</h2>

<p>ä¸‹é¢æŒ‰æ—¶é—´çº¿ç®€å•åˆ—ä¸€ä¸‹æˆ‘å°è±¡æ¯”è¾ƒæ·±çš„ä¸€äº›å·¥ä½œï¼ˆåªä»£è¡¨æˆ‘ä¸ªäººçœ‹åˆ°çš„ç‰‡é¢å­é›†ï¼‰ï¼š</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2110.00641">Decoupled PPO</a> ç‡å…ˆæŒ‡å‡ºï¼Œåœ¨ä¿¡èµ–åŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆTRPO å’Œ PPOï¼‰æ–¹æ³•ä¸­ï¼Œâ€œæ—§ç­–ç•¥â€ï¼ˆold policyï¼‰å®é™…æ‰¿æ‹…äº†ä¸¤ä¸ªä¸åŒçš„è§’è‰²ï¼šä¸€æ˜¯ç”¨äºé‡è¦æ€§é‡‡æ ·è¿›è¡Œå¼‚ç­–ç•¥ä¿®æ­£ï¼Œåœ¨è¿™ä¸ªç›®çš„ä¸‹ï¼Œâ€œæ—§ç­–ç•¥â€ç”¨äºä»£è¡¨è®­ç»ƒæ•°æ®é›†æ‰€æœä»çš„è¡Œä¸ºç­–ç•¥ï¼ˆbehavior policyï¼‰ï¼›äºŒæ˜¯ç”¨äºé™åˆ¶æ–°ç­–ç•¥çš„æ›´æ–°å¹…åº¦ï¼Œåœ¨è¿™ä¸ªç›®çš„ä¸‹ï¼Œâ€œæ—§ç­–ç•¥â€è¢«ç”¨äºè¡¡é‡æ–°æ—§ç­–ç•¥çš„å˜åŒ–ç¨‹åº¦ï¼Œç§°ä½œè¿‘ç«¯ç­–ç•¥ï¼ˆproximal policyï¼Œå¯¹åº”æœ¬æ–‡ä¸­çš„â€œå‚è€ƒç­–ç•¥â€ï¼‰ã€‚æ–‡ç« æŒ‡å‡ºè¿™ä¸¤ä¸ªç›®çš„ä¸‹çš„â€œæ—§ç­–ç•¥â€å¯ä»¥æ˜¯ä¸åŒçš„ç­–ç•¥ï¼Œä»è€Œæå‡ºäº† Decoupled PPO æ›´æ–°ç›®æ ‡ï¼ŒæŠŠâ€œé‡‡æ ·ç”¨è°â€å’Œâ€œå¯¹è°åš trust regionâ€åœ¨å½¢å¼ä¸Šè§£è€¦å¼€æ¥ã€‚</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2505.24298">AReaL</a> å…³æ³¨åˆ°äº†å¼‚æ­¥è®­ç»ƒæ¡†æ¶ä¸‹è¡Œä¸ºç­–ç•¥ä¸å‚è€ƒç­–ç•¥ä¸ä¸€è‡´çš„é—®é¢˜ï¼šrollout å¾€å¾€ç”±æ»åçš„å‚æ•°ç‰ˆæœ¬æˆ–ä¸åŒ worker äº§ç”Ÿã€‚æ–‡ç« åœ¨å¼‚æ­¥æ¡†æ¶ä¸‹é‡‡ç”¨äº† Decoupled PPO é£æ ¼çš„ç›®æ ‡ï¼Œå°†â€œè¡Œä¸ºç­–ç•¥åˆ†å¸ƒâ€å’Œâ€œå‚è€ƒç­–ç•¥â€æ˜¾å¼åŒºåˆ†å¼€æ¥ï¼Œä»è€Œåœ¨å¼‚æ­¥åœºæ™¯ä¸‹ä»ç„¶ç»´æŒç±»ä¼¼ PPO çš„ä¼˜åŒ–æ€§è´¨ã€‚</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2507.18071">GSPO</a> ä» GRPO åœ¨é•¿åºåˆ—å’Œ MoE æ¨¡å‹ä¸Šçš„ç¨³å®šæ€§é—®é¢˜å‡ºå‘ï¼ŒæŒ‡å‡º token-level çš„ PPO / GRPO åœ¨ä¸“å®¶è·¯ç”±é«˜åº¦æ³¢åŠ¨ï¼ˆå°¤å…¶æ˜¯æ–°æ—§ç­–ç•¥ä¹‹é—´çš„è·¯ç”±å·®å¼‚ï¼‰æ—¶ï¼Œä¼šå¼•å…¥å·¨å¤§çš„æ–¹å·®ä¸ä¸ç¨³å®šã€‚GSPO æå‡ºåœ¨ <strong>sequence-level</strong> å®šä¹‰ PPO-style ç›®æ ‡ä¸æ¯”ç‡çº¦æŸï¼Œç”¨æ•´æ¡åºåˆ—çš„æ¯”ç‡æ¥çº¦æŸæ›´æ–°ï¼Œä»è€Œåœ¨ MoE åœºæ™¯ä¸‹æ˜¾è‘—ç¼“è§£ç”±è·¯ç”±ä¸ä¸€è‡´å¸¦æ¥çš„è®­ç»ƒå´©æºƒé—®é¢˜ã€‚</p>
  </li>
  <li>
    <p><a href="https://fengyao.notion.site/off-policy-rl#28b721e3f6c480c3a756f8fb319e860d">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training</a> å…³æ³¨åˆ°äº†ç°æœ‰çš„ä¸€äº›å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼ˆå¦‚ VeRLï¼‰ä¸­ï¼Œæ¨ç†æ¡†æ¶å’Œè®­ç»ƒæ¡†æ¶åœ¨ä¸å°‘ç›¸åŒçš„åŠŸèƒ½æ¨¡å—ä¸Šæœ‰ä¸åŒçš„å®ç°ï¼ˆä¾‹å¦‚ vLLM å’Œ FSDP / Megatron ç­‰ç®—å­ä¸Šçš„å·®å¼‚ï¼‰ï¼Œå¯¼è‡´è¡Œä¸ºç­–ç•¥ $\mu$ ä¸å‚è€ƒç­–ç•¥ $\pi_{\theta_{\text{old}}}$ ä¸ä¸€è‡´ã€‚è¿™ç§ä¸ä¸€è‡´ä½¿å¾—åŸæœ¬å‡å®šä¸ºåŒç­–ç•¥ï¼ˆon-policyï¼‰çš„è®­ç»ƒï¼Œå®é™…ä¸Šå˜æˆäº†å¸¦æœ‰æ˜æ˜¾åå·®çš„å¼‚ç­–ç•¥ï¼ˆoff-policyï¼‰è®­ç»ƒã€‚æ–‡ç« æ€»ç»“äº†ä¸¤ç§å¤„ç†è¿™ä¸€é—®é¢˜çš„ç°æœ‰æ–¹æ³•ï¼šPPO-IS ä¸ vanilla-ISï¼Œå¹¶æå‡ºåœ¨ <strong>token-level</strong> åšæˆªæ–­é‡è¦æ€§é‡‡æ ·ï¼ˆtruncated IS, TISï¼‰ï¼Œä»¥å‡å°‘è®­æ¨ä¸ä¸€è‡´ç¨‹åº¦è¾ƒé‡çš„æ ·æœ¬åœ¨è®­ç»ƒä¸­çš„å½±å“ã€‚ä½œè€…è¿˜å†™äº†ä¸¤ç¯‡æ›´ä¸ºåŸºç¡€çš„åˆ†ææ–‡ç« ï¼Œä»åŸç†ä¸Šåˆ†æè®­æ¨ä¸ä¸€è‡´é—®é¢˜ï¼š<a href="https://fengyao.notion.site/pg-seq-token-part1-basics">Part I</a> å’Œ <a href="https://fengyao.notion.site/pg-seq-token-part2-mismatch">Part II</a>ã€‚</p>
  </li>
  <li>
    <p><a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference">Defeating Nondeterminism in LLM Inference</a> æŒ‡å‡ºï¼Œæ‰¹å¤„ç†å¤§å°ä¸å˜æ€§ï¼ˆbatch-size invarianceï¼‰çš„ç¼ºå¤±æ˜¯å¤§æ¨¡å‹æ¨ç†æ¡†æ¶éšæœºæ€§çš„æ ¸å¿ƒæ¥æºä¹‹ä¸€ï¼šåŒä¸€ä¸ªè¾“å…¥åœ¨ä¸åŒçš„ batch ç»„åˆå’Œ kernel è·¯å¾„ä¸‹ï¼Œå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒä¼šå‘ç”Ÿå¯è§‚å·®å¼‚ã€‚è¿™æ„å‘³ç€ï¼Œå³ä¾¿â€œåä¹‰ä¸Šâ€æ˜¯åŒä¸€å¥—å‚æ•°ï¼ŒçœŸå®è¿è¡Œæ—¶çš„è¡Œä¸ºç­–ç•¥ $\mu$ ä¹Ÿä¼šå› ä¸ºç³»ç»Ÿè´Ÿè½½å’Œè°ƒåº¦å·®å¼‚è€Œæ³¢åŠ¨ï¼Œä»è€Œè¿›ä¸€æ­¥åŠ å‰§è®­æ¨ä¸ä¸€è‡´ã€‚</p>
  </li>
  <li>
    <p><a href="https://ringtech.notion.site/icepop">Small Leak Can Sink a Great Shipâ€”Boost RL Training on MoE with ğ‘°ğ’„ğ’†ğ‘·ğ’ğ’‘!</a> è§‚å¯Ÿåˆ°ï¼Œä¸Šè¿°è®­æ¨ä¸ä¸€è‡´é—®é¢˜åœ¨ MoE æ¨¡å‹ä¸Šä¼šè¿›ä¸€æ­¥åŠ å‰§ï¼šè·¯ç”±æœ¬èº«å°±å¯¹å¾®å°æ‰°åŠ¨é«˜åº¦æ•æ„Ÿï¼Œå†å åŠ æ¨ç† / è®­ç»ƒå®ç°å·®å¼‚å’Œå¼‚æ­¥é‡‡æ ·ï¼Œå¾ˆå®¹æ˜“æ”¾å¤§åå·®ã€‚æ–‡ç« æå‡º IcePop æ–¹æ³•ï¼šåœ¨ <strong>token-level</strong> é€šè¿‡è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼Œå¯¹è¿‡äºå¤§æˆ–è€…è¿‡äºå°çš„æ¯”ç‡è¿›è¡ŒåŒä¾§æ©ç ï¼ˆmaskingï¼‰ï¼Œå°†è¿™äº›â€œå™ªå£°è¾ƒå¤§â€çš„æ•°æ®ä»æ¢¯åº¦ä¸­ä¸¢å¼ƒï¼Œä»è€Œç¨³å®š MoE ä¸Šçš„ RL è®­ç»ƒã€‚</p>
  </li>
  <li>
    <p><a href="https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda">When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch</a> ç³»ç»Ÿæ€§åˆ†æäº†è®­æ¨ä¸ä¸€è‡´çš„å„ç§æˆå› ï¼ŒåŒ…æ‹¬æ™ºèƒ½ä½“å·¥ä½œæµä¸­å¼•å…¥çš„å¤§é‡åˆ†å¸ƒå¤–å’Œä½æ¦‚ç‡ä¿¡æ¯ã€ç¡¬ä»¶å’Œå†…æ ¸ / kernel å®ç°å¸¦æ¥çš„è®¡ç®—ä¸ç¡®å®šæ€§ï¼Œå¹¶åˆ†æäº†åœ¨ <strong>token-level</strong> è¿›è¡Œé‡è¦æ€§é‡‡æ ·å¦‚ä½•åœ¨é•¿åºåˆ—ä¸Šå¼•å…¥ä¸¥é‡çš„åå·®ã€‚æ–‡ç« è¿›ä¸€æ­¥æå‡ºåœ¨ <strong>sequence-level</strong> è®¡ç®—é‡è¦æ€§é‡‡æ ·æ©ç ï¼ˆsequence-level masked IS, sequence-level MISï¼‰ï¼šåªä¸¢å¼ƒé‚£äº›æ•´æ¡åºåˆ—çš„é‡è¦æ€§é‡‡æ ·æ¯”ç‡è¿‡å¤§çš„æ•°æ®ï¼Œä»è€Œåœ¨æ§åˆ¶åå·®çš„åŒæ—¶ï¼Œæ˜¾è‘—æŠ‘åˆ¶ç”±æç«¯æ ·æœ¬å¯¼è‡´çš„è®­ç»ƒå´©æºƒã€‚æ–‡ä¸­ç»™å‡ºäº†è¾ƒä¸ºå®Œæ•´çš„ç†è®ºæ¨å¯¼å’Œä¸°å¯Œçš„å®éªŒæ”¯æ’‘ã€‚</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2510.11370">Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</a> èšç„¦äº MoE æ¶æ„ä¸‹ç‰¹æœ‰çš„ <strong>è·¯ç”±ä¸ä¸€è‡´ï¼ˆRouting Inconsistencyï¼‰</strong> é—®é¢˜ã€‚æ–‡ç« å‘ç°ï¼Œæ¨ç†ç«¯å’Œè®­ç»ƒç«¯å³ä¾¿åœ¨è¾“å…¥å®Œå…¨ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œç”±äºç®—å­å®ç°æˆ–å¹¶è¡Œçš„å¾®å°å·®å¼‚ï¼ŒRouter é€‰ä¸­çš„ä¸“å®¶å¾€å¾€ä¸åŒã€‚è¿™ç§â€œç‰©ç†è·¯å¾„â€ä¸Šçš„ä¸ä¸€è‡´ï¼Œä½¿å¾—è¡Œä¸ºç­–ç•¥ $\mu$ å’Œå‚è€ƒç­–ç•¥ $\pi_{\theta_{\text{old}}}$ ä¹‹é—´çš„å·®å¼‚è¿œè¶…é¢„æœŸï¼Œææ˜“å¯¼è‡´è®­ç»ƒå´©æºƒã€‚æ–‡ç« æå‡ºäº† <strong>Rollout Routing Replay (R3)</strong>ï¼šåœ¨æ¨ç†é˜¶æ®µè®°å½•ä¸‹æ¯ä¸ª token å®é™…å‘½ä¸­çš„ä¸“å®¶ç´¢å¼•ï¼Œå¹¶åœ¨è®­ç»ƒé˜¶æ®µ<strong>å¼ºåˆ¶å›æ”¾</strong>è¿™äº›è·¯ç”±å†³ç­–ï¼Œä¸å†é‡æ–°è¿›è¡Œè®¡ç®—ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒR3 åœ¨ MoE æ‹“æ‰‘ç»“æ„ä¸Šå¼ºåˆ¶å¯¹é½äº†è®­æ¨ä¸¤ç«¯çš„è®¡ç®—è·¯å¾„ã€‚</p>
  </li>
  <li>
    <p><a href="https://zhuanlan.zhihu.com/p/1959976628290590602">RL è€è®­å´©ï¼Ÿè®­æ¨å·®å¼‚æ˜¯åŸºçŸ³</a> åˆ™æ›´å¤šä»å®è·µè§’åº¦å‡ºå‘ï¼Œåˆ†äº«äº†å¦‚ä½•åœ¨å®ç°ä¸Šå°½å¯èƒ½é è¿‘â€œè®­æ¨ä¸€è‡´â€çš„ç»éªŒï¼ŒåŒ…æ‹¬å¦‚ä½•é€‰ç”¨ä¸€è‡´çš„ç®—å­å’Œç²¾åº¦é…ç½®ã€å¦‚ä½•ç›‘æ§ä¸çº¦æŸè®­ç»ƒç«¯å’Œæ¨ç†ç«¯ log-prob çš„åå·®ç­‰ï¼Œæ›´ç€åŠ›äºä»è®­æ¨æ¡†æ¶å±‚é¢å…¥æ‰‹ï¼Œåœ¨å·¥ç¨‹ä¸Šå°½é‡ä»æ ¹æœ¬ç¼“è§£è®­æ¨å·®å¼‚é—®é¢˜ã€‚</p>
  </li>
  <li>
    <p><a href="https://verl.readthedocs.io/en/latest/algo/rollout_corr.html">verl Rollout Importance Sampling</a> åœ¨å…¶ rollout correction æ¨¡å—ä¸­å¼•å…¥äº† Token Vetoï¼ˆä¸€ç¥¨å¦å†³ï¼‰æœºåˆ¶ï¼šåœ¨ <strong>token-level</strong> è®¡ç®—é‡è¦æ€§æ¯”ç‡ $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ï¼Œè‹¥è½¨è¿¹ä¸­å­˜åœ¨ä»»æ„ token ä½¿å¾— $\min_t \rho_t &lt; \tau_{\text{veto}}$ï¼Œåˆ™å°†æ•´æ¡åºåˆ—ä»è®­ç»ƒä¸­å‰”é™¤ã€‚è¿™ç§â€token ç²’åº¦æ£€æµ‹ã€sequence ç²’åº¦å¦å†³â€çš„è®¾è®¡ä½“ç°äº†ä¸€ç§â€ä¸€ç¥¨å¦å†³â€çš„ä¿å®ˆç­–ç•¥ã€‚</p>
  </li>
  <li>
    <p><a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf">INTELLECT-3 Technical Report</a> åœ¨å…¶å¼‚æ­¥åˆ†å¸ƒå¼ RL è®­ç»ƒæ¡†æ¶ä¸­é‡‡ç”¨äº†ç±»ä¼¼çš„æ‹’ç»é‡‡æ ·ç­–ç•¥ã€‚INTELLECT-3 å¯¹æ¯æ¡ rollout è®¡ç®— <strong>token-level</strong> é‡è¦æ€§æ¯”ç‡ï¼Œè‹¥ä»»æ„ token çš„æ¯”ç‡ä½äºé˜ˆå€¼ï¼ˆæ–‡ä¸­ä½¿ç”¨ $10^{-5}$ï¼‰ï¼Œåˆ™å¯¹æ•´æ¡è½¨è¿¹è¿›è¡Œ maskingã€‚</p>
  </li>
</ul>

<h2 id="ä¸‰ç­–ç•¥-trpo-è§†è§’ä¸‹çš„æœ€å°ç»Ÿä¸€ç†è§£">ä¸‰ç­–ç•¥ TRPO è§†è§’ä¸‹çš„æœ€å°ç»Ÿä¸€ç†è§£</h2>

<p>ä¸Šé¢åˆ—çš„è¿™äº›å·¥ä½œï¼Œçœ‹ä¸Šå»å„è‡ªè§£å†³çš„æ˜¯ï¼š</p>

<ul>
  <li>ç®—æ³•å±‚ï¼šPPO / GRPO çš„ç›®æ ‡æ€ä¹ˆå†™ï¼Œtoken-level è¿˜æ˜¯ sequence-levelï¼Œç”¨ clip è¿˜æ˜¯ maskï¼›</li>
  <li>ç³»ç»Ÿå±‚ï¼šæ¨ç†æ¡†æ¶å’Œè®­ç»ƒæ¡†æ¶æ€æ ·å¯¹é½ï¼›</li>
  <li>æ¨¡å‹å±‚ï¼šMoE æ¨¡å‹è·¯ç”±é—®é¢˜å¦‚ä½•æ”¾å¤§è®­ç»ƒä¸ç¨³å®šï¼Œç­‰ç­‰ã€‚</li>
</ul>

<p>ä½†å¦‚æœæˆ‘ä»¬æŠŠâ€œè¡Œä¸ºç­–ç•¥ vs å‚è€ƒç­–ç•¥â€è¿™æ¡çº¿æ‹‰ç›´ï¼Œä¼šå‘ç°ç›¸å½“ä¸€éƒ¨åˆ†é—®é¢˜ï¼Œå…¶å®éƒ½å¯ä»¥æ”¾åˆ°ä¸€ä¸ªç›¸å¯¹ç®€å•çš„ç†è®ºæ¡†æ¶é‡Œç†è§£ï¼š<strong>ä¸‰ç­–ç•¥ TRPO</strong>ã€‚</p>

<p>ä¸‹é¢è¿™èŠ‚æˆ‘ä¼šç”¨å°½é‡ç®€å•çš„æ•°å­¦ï¼ŒæŠŠè¿™ä¸ªä¸‰ç­–ç•¥ç‰ˆ TRPO æ‘Šå¼€â€”â€”å®ƒå¯ä»¥è¢«çœ‹ä½œæ˜¯â€œTRPO + ä¸‰è§’ä¸ç­‰å¼â€çš„ä¸€ä¸ªå°æ‰©å±•ï¼Œä½†åœ¨åˆ†æå¤§æ¨¡å‹ RL é‡Œçš„è®­æ¨ä¸ä¸€è‡´æ—¶éå¸¸å¥½ç”¨ï¼š</p>

<ul>
  <li>ä¸€æ–¹é¢è®©æˆ‘ä»¬é‡æ–°ç†è§£â€œè®­æ¨ä¸ä¸€è‡´â€å’Œâ€œå¼‚æ­¥è®­ç»ƒæ¡†æ¶â€åˆ°åº•åœ¨å½±å“ä»€ä¹ˆï¼›</li>
  <li>å¦ä¸€æ–¹é¢ï¼Œä¹Ÿå¸®æˆ‘ä»¬ç»Ÿä¸€ç†è§£ TISã€IcePopã€sequence-level MIS ç­‰ï¼Œåœ¨æœ¬æ–‡çš„è§†è§’ä¸‹ï¼Œå®ƒä»¬å…¶å®éƒ½æ˜¯åœ¨å®æ–½ä¸‹æ–‡çš„â€œ<strong>çº¦æŸ 2</strong>â€ã€‚</li>
</ul>

<h3 id="ä¸‰ä¸ªç­–ç•¥">ä¸‰ä¸ªç­–ç•¥</h3>

<p>æ²¿ç”¨å‰æ–‡çš„è®°å·ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªæŠ˜æ‰£ MDP ä¸Šå·¥ä½œï¼ŒæŠ˜æ‰£å› å­ä¸º $\gamma\in(0,1)$ï¼š</p>

<ul>
  <li>çŠ¶æ€ $s\in\mathcal{S}$ï¼ŒåŠ¨ä½œ $a\in\mathcal{A}$ï¼›</li>
  <li>ç­–ç•¥ $\pi(a\mid s)$ï¼›</li>
  <li>æŠ˜æ‰£çŠ¶æ€åˆ†å¸ƒï¼š
\(d_\pi(s) := (1-\gamma)\sum_{t=0}^\infty \gamma^t \Pr_\pi(s_t = s)ã€‚\)</li>
  <li>å›æŠ¥ï¼ˆepisode è§†è§’ï¼‰ï¼š
\(\mathcal{J}(\pi) := \mathbb{E}_\pi\Big[\sum_{t=0}^\infty \gamma^t r_t\Big]ã€‚\)</li>
  <li>å€¼å‡½æ•° / ä¼˜åŠ¿å‡½æ•°ï¼š
\(V_\pi(s),\quad Q_\pi(s,a),\quad A_\pi(s,a) := Q_\pi(s,a) - V_\pi(s)ã€‚\)</li>
</ul>

<p>ç¨å¾®èµ˜è¿°ä¸€ä¸‹ï¼Œåœ¨â€œä¸‰ç­–ç•¥â€è®¾å®šé‡Œï¼Œæˆ‘ä»¬æœ‰ï¼š</p>

<ul>
  <li><strong>è¡Œä¸ºç­–ç•¥</strong>ï¼ˆbehavior policyï¼‰ï¼š$\mu$ï¼ŒçœŸæ­£ç”¨æ¥ rollout çš„ç­–ç•¥ï¼›æ•°æ® $(s,a,r,\dots)$ éƒ½æ˜¯ä»å®ƒæ¥çš„ã€‚</li>
  <li><strong>å‚è€ƒç­–ç•¥</strong>ï¼ˆreference policyï¼‰ï¼š$\pi_{\theta_{\text{old}}}$ï¼Œä¼˜åŒ–ç›®æ ‡é‡Œæ‹¿æ¥åš ratioã€clip æˆ– KL çº¦æŸçš„é‚£ä¸€ä»½â€œæ—§ç­–ç•¥â€ã€‚</li>
  <li><strong>ç›®æ ‡ç­–ç•¥</strong>ï¼ˆtarget policyï¼‰ï¼š$\pi_\theta$ï¼Œæˆ‘ä»¬è¿™ä¸€æ­¥æƒ³è¦ä¼˜åŒ–çš„ç­–ç•¥ã€‚</li>
</ul>

<p>åœ¨ç†æƒ³è®¾å®šé‡Œæˆ‘ä»¬é»˜è®¤ $\mu = \pi_{\theta_{\text{old}}}$ï¼›ç°å®ç³»ç»Ÿé‡ŒäºŒè€…å¾€å¾€ä¸ç­‰ï¼Œè¿™å°±æ˜¯â€œè®­æ¨ä¸ä¸€è‡´â€çš„æ•°å­¦å½±å­ã€‚</p>

<h3 id="ä¸¤ç­–ç•¥-trpo">ä¸¤ç­–ç•¥ TRPO</h3>

<blockquote>
  <p>ç†Ÿæ‚‰ TRPO çš„è¯»è€…å¯ä»¥ç›´æ¥è·³åˆ°åé¢çš„â€œä¸‰ç­–ç•¥ TRPOâ€å°èŠ‚ã€‚</p>
</blockquote>

<p>TRPO çš„æ‰€æœ‰ç†è®ºä¿è¯ï¼Œéƒ½æ˜¯å»ºç«‹åœ¨<strong>æŸä¸ªâ€œåŸºå‡†ç­–ç•¥â€çš„ä¼˜åŠ¿å‡½æ•°</strong>ä¹‹ä¸Šçš„ã€‚æ—¢ç„¶å®é™…èƒ½ç®—æ¸…æ¥šçš„<strong>åªæœ‰</strong> $A_\mu$ï¼ˆæ•°æ®æ˜¯æŒ‰ $\mu$ é‡‡çš„ï¼‰ï¼Œé‚£æˆ‘ä»¬å°±ç›´æ¥æŠŠ $\mu$ å½“æˆåŸºå‡†ã€‚</p>

<p>ä¸€ä¸ªç»å…¸çš„ç»“è®ºæ˜¯ <strong>æ€§èƒ½å·®åˆ†å¼•ç†ï¼ˆPerformance Difference Lemmaï¼‰</strong>ï¼š</p>

<blockquote>
  <p>å¯¹ä»»æ„ä¸¤ç­–ç•¥ $\mu$ å’Œ $\pi_\theta$ï¼Œæœ‰</p>

\[\mathcal{J}(\pi_\theta) - \mathcal{J}(\mu)
= \frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d_{\pi_\theta},\, a\sim\pi_\theta}[A_\mu(s,a)]ã€‚\]
</blockquote>

<p>ç›´è§‰éå¸¸ç®€å•ï¼š</p>

<ul>
  <li>$A_\mu(s,a)$ å°±æ˜¯åœ¨è¯´â€œå¦‚æœåœ¨ $s$ é‡Œæœ¬æ¥æŒ‰ $\mu$ è¡ŒåŠ¨ï¼Œç°åœ¨æ¢æˆåŠ¨ä½œ $a$ï¼Œé•¿æœŸå›æŠ¥ä¼šå¤šæˆ–å°‘å¤šå°‘â€ï¼›</li>
  <li>æŠŠæ‰€æœ‰æ—¶åˆ»ã€æ‰€æœ‰çŠ¶æ€ã€æ‰€æœ‰åŠ¨ä½œçš„â€œå¢ç›Šâ€ç´¯ç§¯èµ·æ¥ï¼Œå°±å¾—åˆ°æ–°ç­–ç•¥æ¯”è¡Œä¸ºç­–ç•¥æ€»å…±èµšäº†å¤šå°‘ã€‚</li>
</ul>

<p>TRPO çš„é—®é¢˜åœ¨äºï¼Œæˆ‘ä»¬æ²¡æ³•å‡†ç¡®ç®—</p>

\[\mathbb{E}_{s\sim d_{\pi_\theta}, a\sim\pi_\theta}[A_\mu(s,a)]ï¼Œ\]

<p>å› ä¸º $d_{\pi_\theta}$ æ˜¯â€œæ–°ç­–ç•¥â€çš„çŠ¶æ€åˆ†å¸ƒï¼Œæˆ‘ä»¬æ²¡æœ‰åœ¨å®ƒä¸‹é¢é‡‡æ ·è¿‡ã€‚</p>

<p>äºæ˜¯ TRPO å¼•å…¥äº†ä¸€ä¸ªæ›¿ä»£ç›®æ ‡ï¼šæŠŠçŠ¶æ€åˆ†å¸ƒæ¢æˆè¡Œä¸ºç­–ç•¥çš„ï¼š</p>

\[L_\mu(\pi_\theta)
:= \mathcal{J}(\mu) + \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_\mu,\,a\sim \pi_\theta}[A_\mu(s,a)]ã€‚\]

<p>$L_\mu$ çš„ç›´è§‰è§£é‡Šæ˜¯ï¼šåœ¨è¡Œä¸ºç­–ç•¥çš„çŠ¶æ€åˆ†å¸ƒä¸‹ï¼Œè®©æ–°ç­–ç•¥è¯•ç€å»é€‰åŠ¨ä½œï¼Œçœ‹ä¼˜åŠ¿æœ‰å¤šå¤§ã€‚</p>

<p>ä»æ€§èƒ½å·®åˆ†å¼•ç†å‡ºå‘ï¼Œä¸¤è€…ä¹‹å·®æ˜¯ï¼š</p>

\[\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)
= \frac{1}{1-\gamma}\;
  \sum_s \big(d_{\pi_\theta}(s) - d_\mu(s)\big)
  \,\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[A_\mu(s,a)]ã€‚\]

<p>å¦‚æœæˆ‘ä»¬å®šä¹‰</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|ï¼Œ\]

<p>é‚£ä¹ˆæœ‰ä¸€ä¸ªç›´æ¥çš„ä¸Šç•Œï¼š</p>

<blockquote>
  <p><strong>Lemma 1</strong></p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\;
    \|d_{\pi_\theta} - d_\mu\|_1ã€‚\]
</blockquote>

<p>è¿™é‡Œå‡ºç°äº†ç¬¬ä¸€ä¸ªå…³é”®é‡ï¼š</p>

<blockquote>
  <p><strong>çŠ¶æ€åˆ†å¸ƒåç§»</strong> $|d_{\pi_\theta} - d_\mu|_1$ï¼Œä¹Ÿå°±æ˜¯â€œæ–°ç­–ç•¥å’Œè¡Œä¸ºç­–ç•¥çœ‹åˆ°çš„ä¸–ç•Œï¼Œåˆ°åº•å·®äº†å¤šå°‘â€ã€‚</p>
</blockquote>

<p>æˆ‘ä»¬é€šå¸¸ä¸ä¼šç›´æ¥å¯¹ $|d_{\pi_\theta} - d_\mu|_1$ æ–½åŠ çº¦æŸï¼Œåè€Œæ˜¯å¯¹â€œæ¯ä¸€æ­¥ action åˆ†å¸ƒâ€çš„å·®å¼‚æ–½åŠ çº¦æŸï¼Œæ¯”å¦‚ trust regionã€KLã€clip ç­‰ã€‚</p>

<p>è®°æ€»å˜å·®è·ç¦»ï¼ˆtotal variationï¼‰ï¼š</p>

\[D_{\mathrm{TV}}(p,q) := \frac{1}{2}\|p-q\|_1ã€‚\]

<p>å‡è®¾å­˜åœ¨å¸¸æ•° $\beta$ï¼Œä½¿å¾—</p>

<blockquote>
  <p>å¯¹æ‰€æœ‰ $s$ï¼Œè¡Œä¸ºç­–ç•¥å’Œç›®æ ‡ç­–ç•¥ä¹‹é—´çš„ TV è¢« $\beta$ ä¸Šç•Œï¼š</p>

\[D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big) \le \betaã€‚\]
</blockquote>

<p>ç›´è§‚å«ä¹‰ï¼šåœ¨ä»»æ„çŠ¶æ€é‡Œï¼Œâ€œæ–°ç­–ç•¥â€å’Œâ€œç”Ÿæˆæ•°æ®çš„ç­–ç•¥â€é€‰åŠ¨ä½œçš„åˆ†å¸ƒéƒ½ä¸ä¼šç¦»å¤ªè¿œã€‚</p>

<p>ä¸€ä¸ªç»å…¸ç»“æœï¼ˆå¯ä»¥ç”¨ coupling è¯æ˜ï¼‰æ˜¯ï¼š</p>

<blockquote>
  <p><strong>Lemma 2</strong>
åœ¨ä¸Šè¿°æ¡ä»¶ä¸‹æœ‰</p>

\[\|d_{\pi_\theta} - d_\mu\|_1
\le \frac{2\gamma}{1-\gamma}\,\betaã€‚\]
</blockquote>

<p>æŠŠå®ƒå’Œ Lemma 1 ç»“åˆï¼š</p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\; \frac{2\gamma}{1-\gamma}\,\beta
= \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\betaã€‚\]

<p>äºæ˜¯æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå½¢å¼ä¸Šç›¸å½“ç®€æ´çš„<strong>ä¸¤ç­–ç•¥ TRPO ä¸‹ç•Œï¼ˆåŸºå‡†ä¸ºè¡Œä¸ºç­–ç•¥ï¼‰</strong>ï¼š</p>

<blockquote>
  <p><strong>Theorem 1ï¼ˆä¸¤ç­–ç•¥ TRPOï¼‰</strong></p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
\frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\betaã€‚\]
</blockquote>

<p>è¿™è¯´æ˜ï¼š</p>

<ul>
  <li><strong>çœŸæ­£å†³å®šâ€œæ›¿ä»£ç›®æ ‡ $L_\mu$ é ä¸é è°±â€çš„ï¼Œæ˜¯è¡Œä¸ºç­–ç•¥ $\mu$ å’Œç›®æ ‡ç­–ç•¥ $\pi_\theta$ çš„å·®å¼‚ï¼š</strong>
\(\beta = \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big)ã€‚\)</li>
</ul>

<p>å¦‚æœä½ èƒ½ç›´æ¥çº¦æŸä½è¿™ä¸ª $\beta$ï¼Œå°±èƒ½ç›´æ¥æŠŠ TRPO çš„å•è°ƒæ€§ä¿è¯æ¬åˆ°è¡Œä¸ºç­–ç•¥è§†è§’ä¸‹ã€‚</p>

<h3 id="ä¸‰ç­–ç•¥-trpo">ä¸‰ç­–ç•¥ TRPO</h3>

<p>ç°å®é—®é¢˜åœ¨äºï¼š<strong>å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒé‡Œæˆ‘ä»¬å¯èƒ½æ— æ³•ç›´æ¥æ§åˆ¶ $\beta$ æœ¬èº«ã€‚</strong></p>

<p>åœ¨å¤§éƒ¨åˆ† PPO / GRPO / GSPO / ç°æœ‰ RLHF æ¡†æ¶é‡Œï¼Œå®é™…å‘ç”Ÿçš„æ˜¯ï¼š</p>

<ul>
  <li>rollout æ•°æ®æ˜¯ç”±æŸä¸ª<strong>è¡Œä¸ºç­–ç•¥</strong> $\mu$ äº§ç”Ÿçš„ï¼ˆæ¨ç†å¼•æ“é‡Œçš„â€œé‚£ä¸€ç‰ˆå‚æ•°â€ + è‹¥å¹²ç³»ç»Ÿç»†èŠ‚ï¼‰ï¼›</li>
  <li>æ›´æ–°æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ©ç”¨<strong>å‚è€ƒç­–ç•¥</strong> $\pi_{\theta_{\text{old}}}$ æ¥é™åˆ¶<strong>ç›®æ ‡ç­–ç•¥</strong> $\pi_\theta$ çš„æ›´æ–°å¹…åº¦ã€‚</li>
</ul>

<p>ä¹Ÿå°±æ˜¯è¯´ï¼Œå®é™…å¯ä»¥â€œåŠ¨æ‰‹â€çš„æ˜¯ä¸¤ä¸ªé‡ï¼š</p>

<ol>
  <li><strong>å‚è€ƒ vs ç›®æ ‡</strong>ï¼šæˆ‘ä»¬å¯ä»¥é€šè¿‡ KL / clip ç­‰æ‰‹æ®µæ§åˆ¶
\(D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)ã€‚\)</li>
  <li><strong>è¡Œä¸º vs å‚è€ƒ</strong>ï¼šæˆ‘ä»¬å¸Œæœ›<strong>é—´æ¥</strong>æ§åˆ¶
\(D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)ã€‚\)</li>
</ol>

<p>äºæ˜¯è‡ªç„¶å°±å®šä¹‰ä¸¤ä¸ªâ€œproxy å·®å¼‚â€ï¼š</p>

<ul>
  <li><strong>çº¦æŸ 1ï¼šå‚è€ƒ vs ç›®æ ‡</strong>
\(\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big)ï¼›\)</li>
  <li><strong>çº¦æŸ 2ï¼šè¡Œä¸º vs å‚è€ƒ</strong>
\(\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big)ã€‚\)</li>
</ul>

<p>ç›´è§‰ä¸Šï¼š</p>

<ul>
  <li>$\alpha_0$ï¼šæ–°ç­–ç•¥åˆ°åº•ç¦»â€œä½ å®£ç§°çš„é‚£ä»½æ—§ç­–ç•¥â€æœ‰å¤šè¿œâ€”â€”è¿™å°±æ˜¯ trust region æ§åˆ¶çš„é‚£éƒ¨åˆ†ï¼›</li>
  <li>$\alpha_1$ï¼šä½ ç”¨æ¥è®­ç»ƒçš„å‚è€ƒç­–ç•¥ï¼Œåˆ°åº•è·ŸçœŸå®é‡‡æ ·æ—¶çš„è¡Œä¸ºç­–ç•¥å·®äº†å¤šå°‘â€”â€”è¿™å°±æ˜¯è®­æ¨ä¸ä¸€è‡´æˆ–å¼‚æ­¥çš„å½±å­ã€‚</li>
</ul>

<p>ç°åœ¨ï¼Œå¯ä»¥æŠŠè¿™ä¸¤ä¸ªé‡å¡å› TRPO çš„ä¸‹ç•Œé‡Œã€‚</p>

<p>å¯¹ä»»æ„çŠ¶æ€ $s$ï¼Œæœ‰</p>

\[\begin{aligned}
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
&amp;\le
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)
\\
&amp;\quad +
D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)ã€‚
\end{aligned}\]

<p>å¯¹ $s$ å–ä¸Šç¡®ç•Œï¼š</p>

\[\beta
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
\;\le\;
\alpha_1 + \alpha_0ã€‚\]

<p>æŠŠè¿™ä¸ªä¸ç­‰å¼å¡å›ä¸¤ç­–ç•¥ TRPO çš„ç»“è®ºï¼ˆTheorem 1ï¼‰é‡Œï¼Œè®°</p>

\[C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}ï¼Œ\]

<p>å³å¾—åˆ°ï¼š</p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,\beta
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,(\alpha_0 + \alpha_1)ã€‚\]

<p>äºæ˜¯ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªéå¸¸ç›´æ¥çš„<strong>ä¸‰ç­–ç•¥ TRPO ä¸‹ç•Œ</strong>ï¼š</p>

<blockquote>
  <p><strong>Theorem 2ï¼ˆä¸‰ç­–ç•¥ TRPOï¼‰</strong>
è®°</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|,\quad
C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}ï¼Œ\]

  <p>ä»¥åŠ</p>

\[\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big)ï¼Œ
\quad
\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big)ã€‚\]

  <p>åˆ™å¯¹ä»»æ„ç›®æ ‡ç­–ç•¥ $\pi_\theta$ æœ‰</p>

\[\boxed{
\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\; C\,(\alpha_0 + \alpha_1)
}\]

  <p>å…¶ä¸­</p>

\[L_\mu(\pi_\theta)
:=
\mathcal{J}(\mu) + \frac{1}{1-\gamma}
  \mathbb{E}_{s\sim d_\mu,a\sim\pi_\theta}[A_\mu(s,a)]ã€‚\]
</blockquote>

<p>è¿™ä¸ªç»“è®ºçš„å«ä¹‰å…¶å®å¾ˆç›´æ¥ï¼š</p>

<ul>
  <li><strong>æ›¿ä»£ç›®æ ‡ $L_\mu(\pi_\theta)$ ä¸çœŸå®æ€§èƒ½ $\mathcal{J}(\pi_\theta)$ ä¹‹é—´çš„ gapï¼Œå¯ä»¥æ‹†æˆä¸¤éƒ¨åˆ†ï¼š</strong>
    <ul>
      <li>å‚è€ƒ vs ç›®æ ‡çš„åç§» $\alpha_0$ï¼›</li>
      <li>è¡Œä¸º vs å‚è€ƒçš„åç§» $\alpha_1$ã€‚</li>
    </ul>
  </li>
</ul>

<p>åªè¦è¿™ä¸¤ä¸ªé‡éƒ½å°ï¼Œ<strong>ä¼˜åŒ– $L_\mu$ å°±æœ‰å¸Œæœ›æœ‰æ•ˆæå‡ $\mathcal{J}$</strong>ã€‚</p>

<h3 id="è¿™ä¸¤ä¸ªå·®å¼‚å„è‡ªæ€ä¹ˆçº¦æŸ">è¿™ä¸¤ä¸ªå·®å¼‚å„è‡ªæ€ä¹ˆçº¦æŸï¼Ÿ</h3>

<p>ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä» Theorem 2 å›å¤´çœ‹å„ç§å®é™…æ–¹æ³•ï¼š</p>

<ul>
  <li>ç»å¤§å¤šæ•° â€œPPO / GRPO / GSPOâ€ ç±»å·¥ä½œï¼Œå…¶å®æ˜¯åœ¨æ§åˆ¶ <strong>çº¦æŸ 1ï¼š$\alpha_0$</strong>ï¼›</li>
  <li>ç»å¤§å¤šæ•° â€œTIS / IcePop / MISâ€ ç±»å·¥ä½œï¼Œåœ¨æœ¬æ–‡çš„ç»Ÿä¸€è§†è§’ä¸‹ï¼Œå¯ä»¥ç†è§£ä¸ºä¸»è¦æ˜¯åœ¨æ§åˆ¶ <strong>çº¦æŸ 2ï¼š$\alpha_1$</strong>ã€‚</li>
</ul>

<p>æœ¬æ–‡ä¸‹é¢åªè®¨è®º <strong>çº¦æŸ 2</strong>ã€‚</p>

<p>çº¦æŸ 2 çš„ç›®æ ‡æ˜¯ï¼š<strong>ä¿è¯ç”¨æ¥è®­ç»ƒçš„æ•°æ®ï¼Œå°½å¯èƒ½æ¥è‡ªâ€œæ¥è¿‘å‚è€ƒç­–ç•¥â€çš„è¡Œä¸ºç­–ç•¥ã€‚</strong></p>

<p>è¿™é‡Œé€šå¸¸æ—¢æœ‰<strong>ç³»ç»Ÿå±‚</strong>çš„æœºåˆ¶ï¼Œä¹Ÿæœ‰<strong>ç®—æ³•å±‚ï¼ˆimportance samplingï¼‰</strong>çš„æœºåˆ¶ã€‚</p>

<ol>
  <li><strong>ç³»ç»Ÿå±‚ï¼šè®©è¡Œä¸ºç­–ç•¥åˆ«é£˜å¤ªè¿œ</strong>
    <ul>
      <li>å¼‚æ­¥æ¡†æ¶ï¼šç»™æ¯ä¸ªæ ·æœ¬æ‰“ä¸Šç­–ç•¥ç‰ˆæœ¬å·ï¼Œåªèƒ½ç”¨ä¸ $\pi_{\theta_{\text{old}}}$ ç›¸å·®ä¸å¤§çš„å‚æ•°ç‰ˆæœ¬é‡‡æ ·çš„æ•°æ®ï¼›</li>
      <li>è®­æ¨å¯¹é½ï¼šå¼ºè°ƒè®­ç»ƒæ¡†æ¶å’Œæ¨ç†æ¡†æ¶ç”¨ç›¸åŒç²¾åº¦ã€ç›¸åŒç®—å­ã€ç›¸è¿‘çš„å†…æ ¸ / kernel è¡Œä¸ºã€‚</li>
    </ul>

    <p>è¿™äº›æœºåˆ¶çš„ç›®æ ‡æ˜¯ï¼šä»â€œç®—æ³•å¤–éƒ¨â€è®© $\mu$ å’Œ $\pi_{\theta_{\text{old}}}$ é è¿‘ï¼Œä»è€Œå‹ç¼© $\alpha_1$ã€‚</p>
  </li>
  <li>
    <p><strong>ç®—æ³•å±‚ï¼šæ ·æœ¬ä¿®æ­£</strong></p>

    <p>åœ¨ç®—æ³•å±‚ï¼Œæˆ‘ä»¬ä¸å†è¯•å›¾â€œçº æ­£æ•´ä¸ªè¡Œä¸ºç­–ç•¥â€ï¼Œè€Œæ˜¯ç”¨é‡è¦æ€§é‡‡æ ·æ¯”ç‡åœ¨<strong>æ ·æœ¬å±‚é¢</strong>åšç­›é€‰å’Œé‡åŠ æƒï¼Œè®©â€œçœŸæ­£å‚ä¸è®­ç»ƒçš„æ ·æœ¬å­é›†â€ä¸Šçš„è¡Œä¸ºç­–ç•¥å°½é‡æ¥è¿‘å‚è€ƒç­–ç•¥ï¼Œæˆ–è€…å‡å°å·®å¼‚è¾ƒå¤§çš„æ ·æœ¬åœ¨è®­ç»ƒä¸Šçš„æƒé‡ã€‚</p>

    <p>å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ä¸‹é¢è¿™äº›æ–¹æ³•ï¼Œå®ƒä»¬æœ¬è´¨ä¸Šéƒ½å¯ä»¥çœ‹ä½œæ˜¯â€œå®ç°çº¦æŸ 2 çš„ä¸åŒæ–¹å¼â€ã€‚</p>
  </li>
</ol>

<h2 id="é‡è¦æ€§é‡‡æ ·ä¸æ©ç å››ç§çº¦æŸ-2-å®ç°">é‡è¦æ€§é‡‡æ ·ä¸æ©ç ï¼šå››ç§çº¦æŸ 2 å®ç°</h2>

<p>ä¸‹é¢å»¶ç»­å‰æ–‡çš„è®°å·ä½“ç³»æ¥å†™è¿™ä¸‰ç§æ–¹æ³•çš„ç›®æ ‡å‡½æ•°ï¼Œåªèšç„¦åœ¨â€œè¡Œä¸ºç­–ç•¥ vs å‚è€ƒç­–ç•¥â€è¿™ä¸€ç»´çš„è®¾è®¡ã€‚è®° token çº§çš„ PPO / GRPO é£æ ¼æ›´æ–°é¡¹ä¸º</p>

\[g_\theta(t)
= \min\big(r_t(\theta) A_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon) A_t\big),\]

<p>å…¶ä¸­</p>

\[r_t(\theta) = \frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)},
\quad (s_t,a_t)\sim\mu,\quad A_t := A_\mu(s_t,a_t)ã€‚\]

<p>ä¹Ÿå°±æ˜¯è¯´ï¼š</p>

<ul>
  <li>$r_t(\theta)$ æ˜¯ <strong>ç›®æ ‡ vs å‚è€ƒ</strong> çš„æ¯”ç‡ï¼ˆå¯¹åº”çº¦æŸ 1ï¼‰ï¼›</li>
  <li>$A_t$ åŸºäºè¡Œä¸ºç­–ç•¥é‡‡æ ·çš„æ•°æ®ï¼Œæ˜¯æˆ‘ä»¬èƒ½ä¼°åˆ°çš„ä¼˜åŠ¿å‡½æ•°ã€‚</li>
</ul>

<p>ä¸ºäº†æŠŠ token çº§çš„ $(s_t,a_t)$ ä¸åºåˆ—çº§çš„ $(x,y)$ è®°å·æ‰“é€šï¼Œåœ¨ä»¥ RLHFï¼ˆreinforcement learning from human feedbackï¼Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰ä¸ºä»£è¡¨çš„ LLM-RL è®¾å®šä¸­ï¼Œæˆ‘ä»¬çº¦å®šï¼š</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>prompt è®°ä¸º $x$ï¼›å›å¤è®°ä¸º $y = (y_1,\dots,y_{</td>
          <td>y</td>
          <td>})$ï¼›</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>token çº§çŠ¶æ€ $s_t := (x, y_{\lt t})$ï¼ŒåŠ¨ä½œ $a_t := y_t$ï¼›</li>
  <li>å› æ­¤è¡Œä¸ºç­–ç•¥å’Œå‚è€ƒç­–ç•¥åœ¨åºåˆ—ä¸Šçš„åˆ†å¸ƒå¯å†™æˆ
\(\mu(y\mid x) = \prod_{t=1}^{|y|}\mu(a_t=y_t\mid s_t),\quad
\pi_{\theta_{\text{old}}}(y\mid x) = \prod_{t=1}^{|y|}\pi_{\theta_{\text{old}}}(a_t=y_t\mid s_t)ã€‚\)</li>
</ul>

<p>æ­¤å¤–ï¼Œä¸ºäº†æè¿°â€œå‚è€ƒ vs è¡Œä¸ºâ€çš„åç§»ï¼Œç»Ÿä¸€å®šä¹‰ token çº§é‡è¦æ€§æ¯”ç‡</p>

\[\rho_t^{(\text{ref}\leftarrow\text{beh})} :=
\frac{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}{\mu(a_t\mid s_t)}ï¼Œ\]

<p>ä»¥åŠå…¶å¯¹åº”çš„åºåˆ—çº§ç‰ˆæœ¬</p>

\[\rho(y\mid x) := \frac{\pi_{\theta_{\text{old}}}(y\mid x)}{\mu(y\mid x)}
= \prod_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})}ã€‚\]

<p>æ¥ä¸‹æ¥ï¼ŒTIS / IcePop / MIS çš„åŒºåˆ«ï¼Œå°±ä½“ç°åœ¨â€œå¦‚ä½•åˆ©ç”¨è¿™äº› $\rho$ æ¥å®ç°çº¦æŸ 2â€ã€‚</p>

<h3 id="1-tistoken-level-æˆªæ–­-is">1. TISï¼štoken-level æˆªæ–­ IS</h3>

<p>TIS ç›´æ¥å¯¹ä¸Šè¿° $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ åšæˆªæ–­ï¼Œè®°</p>

\[\color{blue}{w_t = \min\big(\rho_t^{(\text{ref}\leftarrow\text{beh})},\ C_{\text{IS}}\big)}ã€‚\]

<p>æ›´æ–°ç›®æ ‡å†™æˆ</p>

\[L_{\text{TIS}}(\theta)
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{w_t}\; g_\theta(t)\big]ã€‚\]

<ul>
  <li>è“è‰²çš„ $\color{blue}{w_t}$ æ˜¯è¢«æˆªæ–­çš„ IS æƒé‡ï¼šæç«¯å¤§çš„æ¯”ç‡è¢«å‹åˆ°å¸¸æ•° $C_{\text{IS}}$ã€‚</li>
  <li>ä»ä¸‰ç­–ç•¥ TRPO çš„è§’åº¦çœ‹ï¼Œè¿™ç›¸å½“äºåœ¨ <strong>token åˆ†å¸ƒ</strong> ä¸Šâ€œè½¯å‰Šå¼±â€è¡Œä¸ºç­–ç•¥å’Œå‚è€ƒç­–ç•¥ä¸¥é‡ä¸ä¸€è‡´çš„æ ·æœ¬ï¼Œä»è€Œåœ¨æ¢¯åº¦ä¸­æœ‰æ•ˆå‡å°é‚£éƒ¨åˆ†æ ·æœ¬å¯¹ $\alpha_1$ çš„è´¡çŒ®ã€‚</li>
</ul>

<h3 id="2-icepopmoe-åœºæ™¯ä¸‹çš„-token-level-åŒä¾§-mask">2. IcePopï¼šMoE åœºæ™¯ä¸‹çš„ token-level åŒä¾§ Mask</h3>

<p>IcePop åŒæ ·ä»¥ $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ ä¸ºåº¦é‡ï¼Œä½†é‡‡ç”¨ <strong>åŒä¾§æ©ç </strong>ï¼š</p>

\[\color{blue}{m_t = \mathbf{1}\big[C_{\text{low}} \le \rho_t^{(\text{ref}\leftarrow\text{beh})} \le C_{\text{high}}\big]}ã€‚\]

<p>æ›´æ–°ç›®æ ‡å†™æˆ</p>

\[L_{\text{IcePop}}(\theta)
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{m_t}\; g_\theta(t)\big]ã€‚\]

<ul>
  <li>è“è‰²çš„ $\color{blue}{m_t}$ å†³å®šæŸä¸ª token æ˜¯å¦å‚ä¸æ›´æ–°ï¼šæ¯”ç‡å¤ªå¤§æˆ–å¤ªå°çš„ token ç›´æ¥è¢«ä¸¢å¼ƒã€‚</li>
  <li>è¿™ç›¸å½“äºç¡¬æ€§è£æ‰â€œè¡Œä¸ºç­–ç•¥å’Œå‚è€ƒç­–ç•¥æåº¦ä¸ä¸€è‡´â€çš„ tokenï¼Œåªåœ¨ $\rho_t$ é€‚ä¸­çš„åŒºåŸŸä¸Šä¼˜åŒ–ï¼Œä»æ ·æœ¬é›†åˆå±‚é¢å®æ–½æ›´å¼ºçš„â€œçº¦æŸ 2â€ã€‚</li>
</ul>

<h3 id="3-sequence-level-misæŒ‰æ•´æ¡åºåˆ—-mask-çš„é‡è¦æ€§é‡‡æ ·">3. sequence-level MISï¼šæŒ‰æ•´æ¡åºåˆ— Mask çš„é‡è¦æ€§é‡‡æ ·</h3>

<p>MIS çš„æ ¸å¿ƒæ“ä½œæ˜¯ï¼š<strong>åªä¿ç•™ IS æ¯”ç‡ä¸è¶…è¿‡é˜ˆå€¼ $C$ çš„åºåˆ—ï¼Œå…¶ä½™åºåˆ—çš„æŸå¤±ç›´æ¥ç½®é›¶</strong>ã€‚å†™æˆ</p>

\[\color{blue}{
\rho(y\mid x)
\leftarrow
\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}
}\]

<p>åœ¨ç»Ÿä¸€çš„æŸå¤±å½¢å¼ä¸‹ï¼Œå¯ä»¥å†™æˆ</p>

\[L_{\text{MIS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}}
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big],\]

<p>ç®€è€Œè¨€ä¹‹ï¼š</p>

<ul>
  <li>å¯¹äº <strong>IS æ¯”ç‡è¾ƒå°çš„åºåˆ—</strong>ï¼šä¿ç•™å®Œæ•´çš„ $\rho(y\mid x)$ æƒé‡ï¼Œæ­£å¸¸åš off-policy ä¿®æ­£ï¼›</li>
  <li>å¯¹äº <strong>IS æ¯”ç‡è¶…è¿‡é˜ˆå€¼ $C$ çš„åºåˆ—</strong>ï¼šæ•´ä¸ªåºåˆ—çš„ policy loss è¢« mask æ‰ï¼ˆæƒé‡å˜æˆ $0$ï¼‰ã€‚</li>
</ul>

<p>ä»ä¸‰ç­–ç•¥ TRPO çš„è§’åº¦çœ‹ï¼ŒMIS ä¸å†åœ¨ token ä¸Šåšæˆªæ–­ï¼Œè€Œæ˜¯ç›´æ¥åœ¨<strong>åºåˆ—çº§</strong>ç­›æ‰â€œè¡Œä¸ºç­–ç•¥å’Œå‚è€ƒç­–ç•¥ä¸¥é‡ä¸ä¸€è‡´â€çš„è½¨è¿¹ï¼Œåªåœ¨ $\rho(y\mid x)\le C$ çš„å­åˆ†å¸ƒä¸Šä¼˜åŒ–ï¼Œä»è€Œåœ¨ trajectory ç²’åº¦ä¸Šå®ç°å¯¹â€œçº¦æŸ 2â€ï¼ˆ$\mu$ vs $\pi_{\theta_{\text{old}}}$ åç§»ï¼‰çš„æ§åˆ¶ã€‚</p>

<h3 id="4-worst-token-reject-samplingæŒ‰æœ€å·®-token-æ‹’ç»æ•´æ¡åºåˆ—">4. Worst Token Reject Samplingï¼šæŒ‰æœ€å·® token æ‹’ç»æ•´æ¡åºåˆ—</h3>

<p>verl ä¸­çš„ veto æœºåˆ¶ ä¸ INTELLECT-3 åˆ†åˆ«åœ¨å„è‡ªçš„è®­ç»ƒæ¡†æ¶ä¸­é‡‡ç”¨äº†ä¸€ç§å¯ç»Ÿç§°ä¸º <strong>Worst Token Reject Samplingï¼ˆWTRSï¼‰</strong> çš„æ‹’ç»é‡‡æ ·ç­–ç•¥ï¼š</p>

<ul>
  <li>
    <p><strong>verl Token Veto</strong>ï¼šåœ¨å…¶ rollout correction æ¨¡å—ä¸­ï¼Œè‹¥è½¨è¿¹ä¸­å­˜åœ¨ä»»æ„ token ä½¿å¾— $\min_t \rho_t &lt; \tau_{\text{veto}}$ï¼Œåˆ™é€šè¿‡ response<em>mask å°†æ•´æ¡åºåˆ—å‰”é™¤ã€‚é˜ˆå€¼ $\tau</em>{\text{veto}}$ å¯ç”±ç”¨æˆ·é…ç½®ã€‚</p>
  </li>
  <li>
    <p><strong>INTELLECT-3 Token Masking</strong>ï¼šåœ¨å…¶å¼‚æ­¥åˆ†å¸ƒå¼ RL æ¡†æ¶ä¸­ï¼Œè‹¥ä»»æ„ token çš„æ¯”ç‡ä½äº $10^{-5}$ï¼Œåˆ™å¯¹æ•´æ¡è½¨è¿¹è¿›è¡Œ maskingã€‚</p>
  </li>
</ul>

<p>äºŒè€…çš„æ ¸å¿ƒæ“ä½œä¸€è‡´ï¼š<strong>è‹¥è½¨è¿¹ä¸­å­˜åœ¨ä»»æ„ token çš„ IS æ¯”ç‡ä½äºé˜ˆå€¼ $\tau$ï¼Œåˆ™å°†æ•´æ¡åºåˆ—ä»è®­ç»ƒä¸­å‰”é™¤</strong>ã€‚å†™æˆ</p>

\[\color{blue}{
m(y\mid x) = \mathbf{1}\Big\{\min_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})} \ge \tau\Big\}
}\]

<p>åœ¨ç»Ÿä¸€çš„æŸå¤±å½¢å¼ä¸‹ï¼Œå¯ä»¥å†™æˆ</p>

\[L_{\text{WTRS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{m(y\mid x)}
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big],\]

<p>ç®€è€Œè¨€ä¹‹ï¼š</p>

<ul>
  <li>å¯¹äº <strong>æ‰€æœ‰ token çš„ IS æ¯”ç‡å‡ä¸ä½äº $\tau$ çš„åºåˆ—</strong>ï¼šæ­£å¸¸å‚ä¸è®­ç»ƒï¼›</li>
  <li>å¯¹äº <strong>å­˜åœ¨ä»»æ„ token çš„ IS æ¯”ç‡ä½äº $\tau$ çš„åºåˆ—</strong>ï¼šæ•´æ¡åºåˆ—çš„ policy loss è¢« mask æ‰ã€‚</li>
</ul>

<p>ä»ä¸‰ç­–ç•¥ TRPO çš„è§’åº¦çœ‹ï¼ŒWTRS é‡‡ç”¨äº†â€token ç²’åº¦æ£€æµ‹ã€sequence ç²’åº¦å¦å†³â€çš„æ··åˆç­–ç•¥ï¼šåœ¨ <strong>token-level</strong> æ£€æµ‹æç«¯ä¸ä¸€è‡´çš„ä¿¡å·ï¼Œä¸€æ—¦å‘ç°åˆ™åœ¨ <strong>sequence-level</strong> æ‰§è¡Œæ‹’ç»ã€‚è¿™ç§â€ä¸€ç¥¨å¦å†³â€çš„è®¾è®¡ä½“ç°äº†ä¸€ç§ä¿å®ˆæ€è·¯â€”â€”å½“è½¨è¿¹ä¸­å­˜åœ¨â€è¡Œä¸ºç­–ç•¥ç”Ÿæˆä½†å‚è€ƒç­–ç•¥å‡ ä¹ä¸å¯èƒ½ç”Ÿæˆâ€çš„ token æ—¶ï¼Œ<strong>æ•´æ¡è½¨è¿¹çš„å¯ä¿¡åº¦éƒ½å°†å—åˆ°è´¨ç–‘</strong>ï¼Œä»è€Œåœ¨ trajectory ç²’åº¦ä¸Šå®ç°å¯¹â€çº¦æŸ 2â€ï¼ˆ$\mu$ vs $\pi_{\theta_{\text{old}}}$ åç§»ï¼‰çš„æ§åˆ¶ã€‚</p>

<h2 id="moe-è·¯ç”±å›æ”¾å®ƒåœ¨ä¸‰ç­–ç•¥-trpo-ä¸­åˆ°åº•åšäº†ä»€ä¹ˆ">MoE è·¯ç”±å›æ”¾ï¼šå®ƒåœ¨ä¸‰ç­–ç•¥ TRPO ä¸­åˆ°åº•åšäº†ä»€ä¹ˆï¼Ÿ</h2>

<p>åœ¨ MoEï¼ˆMixture-of-Expertsï¼‰æ¨¡å‹ä¸Šï¼Œè®­æ¨ä¸ä¸€è‡´å¾€å¾€é¦–å…ˆè¡¨ç°ä¸º<strong>è·¯ç”±ä¸ä¸€è‡´ï¼ˆrouting inconsistencyï¼‰</strong>ï¼šå³ä¾¿å‚æ•°ç›¸åŒï¼Œæ¨ç†ç«¯ä¸è®­ç»ƒç«¯ä¹Ÿå¯èƒ½å› ä¸ºç®—å­ã€å¹¶è¡Œæˆ–æ•°å€¼ç»†èŠ‚çš„å¾®å°å·®å¼‚è€Œè·¯ç”±åˆ°ä¸åŒä¸“å®¶ã€‚ä¸€ä¸ªå¾ˆè‡ªç„¶çš„å·¥ç¨‹åº”å¯¹æ˜¯<strong>è·¯ç”±å›æ”¾ï¼ˆrouting replayï¼‰</strong>ï¼šåœ¨ rolloutï¼ˆæ¨ç†ï¼‰æ—¶è®°å½•å®é™…å‘½ä¸­çš„ä¸“å®¶è·¯å¾„ï¼Œè®­ç»ƒæ—¶å¼ºåˆ¶å¤ç”¨è¿™äº›è·¯ç”±å†³ç­–ã€‚</p>

<p>è¿™ç±»æ–¹æ³•ç»å¸¸è¢«ç›´è§‰æ€§åœ°ç†è§£ä¸ºâ€œåœ¨å®ç°çº¦æŸ 2ã€å‹å° $\alpha_1$â€ã€‚ä½†ä»ä¸‰ç­–ç•¥ TRPO çš„è§†è§’çœ‹ï¼Œæ›´å‡†ç¡®çš„è¯´æ³•æ˜¯ï¼š</p>

<blockquote>
  <p><strong>è·¯ç”±å›æ”¾å¹¶ä¸æ˜¯åœ¨åŸ surrogate objective ä¸Šæ”¶ç´§çº¦æŸï¼Œè€Œæ˜¯åœ¨æŠŠ surrogate objective æ”¹å†™æˆå¦ä¸€ä¸ªâ€œå¸¦è·¯ç”±æ¡ä»¶/æ›¿æ¢â€çš„ç›®æ ‡ã€‚</strong>
å®ƒè®©è·¯ç”±ä¸ä¸€è‡´åœ¨ loss é‡Œâ€œä¸å¯è§â€ï¼Œä½†å¹¶æ²¡æœ‰è®©çœŸå®ç­–ç•¥è·ç¦»é‡Œçš„ $\alpha_0$ æˆ– $\alpha_1$ å˜å°ã€‚</p>
</blockquote>

<p>ä¸‹é¢ç”¨ä¸€ä¸ª<strong>å°½é‡ç®€å•</strong>ä½†è¶³å¤Ÿè¯´æ˜é—®é¢˜çš„å»ºæ¨¡æ¥æŠŠè¿™ä»¶äº‹å†™æ¸…æ¥šã€‚</p>

<h3 id="moe-ä¸‹çš„-surrogate-objectiveæŠŠè·¯ç”±å’Œtoken-ç”Ÿæˆæ‹†å¼€">MoE ä¸‹çš„ surrogate objectiveï¼šæŠŠâ€œè·¯ç”±â€å’Œâ€œtoken ç”Ÿæˆâ€æ‹†å¼€</h3>

<p>æŠŠ MoE æŠ½è±¡æˆä¸¤é˜¶æ®µéšæœºå†³ç­–ï¼šâ€œå…ˆé€‰ä¸“å®¶ $z$ï¼Œå†åœ¨è¯¥ä¸“å®¶æ¡ä»¶ä¸‹ç”Ÿæˆ token $a$â€ã€‚
å› æ­¤ç›®æ ‡ç­–ç•¥å¯ä»¥åˆ†è§£ä¸º</p>

\[\pi_\theta(a,z\mid s)=\omega_\theta(z\mid s)\,\pi_\theta(a\mid s,z),\]

<p>å…¶ä¸­ï¼š</p>

<ul>
  <li>$\omega_\theta(z\mid s)$ æ˜¯è·¯ç”±å™¨ï¼ˆrouterï¼‰çš„åˆ†å¸ƒï¼›</li>
  <li>$\pi_\theta(a\mid s,z)$ æ˜¯åœ¨ä¸“å®¶ $z$ æ¡ä»¶ä¸‹çš„ token åˆ†å¸ƒã€‚</li>
</ul>

<p>åœ¨ä¸‰ç­–ç•¥ TRPO ä¸­ï¼Œæˆ‘ä»¬çœŸæ­£æƒ³ä¼˜åŒ–çš„ surrogate objective ä¸º</p>

\[L_\mu(\pi_\theta) = \mathcal{J}(\mu) + \frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_z \omega_\theta(z\mid s)\,F_\theta(s,z)
\bigg],\]

<p>å…¶ä¸­æˆ‘æŠŠä¸“å®¶å±‚çš„ä¼˜åŠ¿èšåˆå†™æˆ</p>

\[F_\theta(s,z)
:=
\sum_a \pi_\theta(a\mid s,z)\,A_\mu(s,a,z).\]

<p>å…³é”®ç‚¹ï¼š<strong>åœ¨åŸå§‹çš„ $L_\mu(\pi_\theta)$ é‡Œï¼Œè·¯ç”±åˆ†å¸ƒæ˜¯å½“å‰è¦æ›´æ–°çš„ $\omega_\theta$</strong>ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒMoE çš„ RL è®­ç»ƒä¸ä»…åœ¨æ›´æ–° token ç”Ÿæˆåˆ†å¸ƒï¼Œä¹Ÿåœ¨æ›´æ–°è·¯ç”±å™¨æœ¬èº«ã€‚</p>

<h3 id="1å›æ”¾è¡Œä¸ºç­–ç•¥çš„è·¯ç”±behavior-router-replay--r3-ç±»">1ï¼‰å›æ”¾è¡Œä¸ºç­–ç•¥çš„è·¯ç”±ï¼ˆbehavior-router replay / R3 ç±»ï¼‰</h3>

<p>R3 çš„åšæ³•æ˜¯ï¼šrollout æ—¶è®°å½•æ¨ç†ç«¯å®é™…å‘½ä¸­çš„ä¸“å®¶é›†åˆ $M_\mu(s)$ï¼Œè®­ç»ƒæ—¶å¼ºåˆ¶å½“å‰ç­–ç•¥<strong>åªåœ¨è¯¥é›†åˆå†…è·¯ç”±</strong>ã€‚å¯ä»¥æŠŠå®ƒå†™æˆå¯¹è·¯ç”±åˆ†å¸ƒçš„â€œæ¡ä»¶åŒ–æŠ•å½±â€ï¼š</p>

\[\omega_\theta^{\text{R3}}(z\mid s)
:=
\frac{\omega_\theta(z\mid s)\,\mathbf{1}\{z\in M_\mu(s)\}}
     {\sum_{z'\in M_\mu(s)}\omega_\theta(z'\mid s)} .\]

<p>ä»è€Œè®­ç»ƒæ—¶å®é™…ä¼˜åŒ–çš„ surrogate objective å˜ä¸º</p>

\[L_\mu^{\text{R3}}(\pi_\theta) =
\mathcal{J}(\mu) +
\frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_{z\in M_\mu(s)} \omega_\theta^{\text{R3}}(z\mid s)\,F_\theta(s,z)
\bigg].\]

<p>å’ŒåŸå§‹ $L_\mu(\pi_\theta)$ å¯¹æ¯”å¯ä»¥çœ‹åˆ°ï¼ŒR3 å¹¶æ²¡æœ‰è®© $\omega_\theta$ é€¼è¿‘ $\omega_{\text{old}}$ æˆ– $\omega_\mu$ï¼›å®ƒåšçš„æ˜¯ï¼š</p>

<ul>
  <li><strong>æŠŠå¯¹ $z\sim\omega_\theta$ çš„æœŸæœ›ï¼Œæ”¹æˆäº†å¯¹ $z\sim\omega_\theta(\cdot\mid z\in M_\mu(s))$ çš„æ¡ä»¶æœŸæœ›</strong>ï¼›</li>
  <li>ç­‰ä»·åœ°è¯´ï¼ŒæŠŠè·¯ç”±çš„å¯è¡Œ support ç¼©åˆ°äº† $M_\mu(s)$ã€‚</li>
</ul>

<p>å› æ­¤ R3 è®­ç»ƒçš„æ˜¯ä¸€ä¸ªâ€œè¢«è¡Œä¸ºè·¯ç”±é›†åˆæ¡ä»¶åŒ–åçš„ surrogate objectiveâ€ï¼Œè€Œä¸æ˜¯åŸæ¥çš„ $L_\mu(\pi_\theta)$ã€‚
å¥½å¤„æ˜¯æ˜¾è‘—é™æ–¹å·®ã€æå‡ç¨³å®šæ€§ï¼›ä»£ä»·æ˜¯<strong>åœ¨æ¯ä¸ªçŠ¶æ€ä¸Šéƒ½æ”¶ç¼©äº†è·¯ç”±å™¨æ¢ç´¢ / æ›´æ–°çš„è‡ªç”±åº¦</strong>ã€‚</p>

<h3 id="2å›æ”¾å‚è€ƒç­–ç•¥çš„è·¯ç”±reference-router-replay">2ï¼‰å›æ”¾å‚è€ƒç­–ç•¥çš„è·¯ç”±ï¼ˆreference-router replayï¼‰</h3>

<p>å¦ä¸€ç±» routing replay å¤ç”¨çš„æ˜¯å‚è€ƒç­–ç•¥ï¼ˆold policyï¼‰çš„è·¯ç”±å™¨ $\omega_{\text{old}}$ã€‚è¿™ç­‰ä»·äºè®­ç»ƒä¸€ä¸ªæ··åˆç­–ç•¥</p>

\[\hat\pi_\theta(a,z\mid s)
:=
\omega_{\text{old}}(z\mid s)\,\pi_\theta(a\mid s,z),\]

<p>å¯¹åº” surrogate objective ä¸º</p>

\[L_\mu^{\text{ref-replay}}(\pi_\theta) =
\mathcal{J}(\mu) +
\frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_z \omega_{\text{old}}(z\mid s)\,F_\theta(s,z)
\bigg].\]

<p>è¿™æ„å‘³ç€ï¼š</p>

<ul>
  <li>åœ¨ surrogate objective ä¸­ï¼Œè·¯ç”±å™¨è¢«<strong>å›ºå®šä¸ºæ—§è·¯ç”±å™¨</strong>ï¼Œè·¯ç”±ç›¸å…³çš„â€œå‚è€ƒ vs ç›®æ ‡â€å·®å¼‚åœ¨ loss é‡Œè¢«ç›´æ¥æŠ¹æ‰ï¼›</li>
  <li>è®­ç»ƒå¯¹â€œæ–°è·¯ç”±å™¨ $\omega_\theta$ æ˜¯å¦åç¦» $\omega_{\text{old}}$â€ä¸å†æ•æ„Ÿï¼Œäºæ˜¯è·¯ç”±ä¸ä¸€è‡´å¯¼è‡´çš„ä¸ç¨³å®šè¢«ç»•å¼€ã€‚</li>
</ul>

<p>ä½†æ³¨æ„è¿™åŒæ ·æ˜¯<strong>æ¢ç›®æ ‡</strong>ï¼š</p>

<ul>
  <li>çœŸå®ç­–ç•¥ç©ºé—´é‡Œçš„ $\alpha_0$ å¹¶æ²¡æœ‰å› æ­¤å˜å°ï¼Œåªæ˜¯è¢«â€œç”¨æ—§è·¯ç”±å™¨é‡å®šä¹‰ç›®æ ‡â€è€Œåœ¨ loss ä¸­ä¸å¯è§ï¼›</li>
  <li>è·¯ç”±å™¨çš„å­¦ä¹ è¢«å¼ºè¡Œå†»ç»“æˆ–æåº¦å‰Šå¼±ã€‚</li>
</ul>

<h3 id="è·¯ç”±å›æ”¾åªæ˜¯åœ¨æ”¹å†™-surrogate-objective">è·¯ç”±å›æ”¾åªæ˜¯åœ¨æ”¹å†™ surrogate objective</h3>

<p>æŠŠä¸¤ç±» replay æ”¾åœ¨ä¸€èµ·çœ‹ï¼Œå®ƒä»¬çš„å…±åŒç‚¹æ˜¯ï¼š</p>

<ol>
  <li><strong>ä¼˜åŒ–çš„éƒ½ä¸æ˜¯åŸå§‹çš„ $L_\mu(\pi_\theta)$</strong>ï¼Œè€Œæ˜¯æŸä¸ªâ€œè·¯ç”±è¢«æ¡ä»¶åŒ– / æ›¿æ¢åçš„ surrogate objectiveâ€ã€‚</li>
  <li><strong>å®ƒä»¬æ²¡æœ‰ç›´æ¥æ”¶ç¼©ä¸‰ç­–ç•¥ TRPO ä¸‹ç•Œé‡Œçš„ $\alpha_0,\alpha_1$</strong>ã€‚replay è®©è·¯ç”±ä¸åŒ¹é…ä¸å†æ˜¾å¼å‡ºç°åœ¨ loss ä¸­ï¼Œä½†ä¸åŒ¹é…åœ¨çœŸå®ç­–ç•¥è·ç¦»é‡Œä»ç„¶å­˜åœ¨ã€‚</li>
  <li><strong>å®è·µä¸Šæ˜¯åœ¨â€œç”¨åå·®æ¢æ–¹å·®â€</strong>ï¼šå›æ”¾å¾€å¾€æ˜¾è‘—é™ä½æ–¹å·®ã€æå‡ç¨³å®šæ€§ï¼Œä½†ä¹Ÿå¯èƒ½é™åˆ¶äº† MoE åœ¨ RL ç›®æ ‡ä¸‹å­¦åˆ°æ›´ä¼˜çš„è·¯ç”±æ¨¡å¼ã€‚</li>
</ol>

<p>æ‰€ä»¥ï¼Œä»ä¸‰ç­–ç•¥ TRPO çš„è§†è§’ï¼Œæ›´å‡†ç¡®çš„ç†è§£æ˜¯ï¼š</p>

<blockquote>
  <p><strong>routing replay æ˜¯ä¸€ç§ surrogate objective çš„æ”¹å†™ï¼Œè€Œä¸æ˜¯å¯¹ $\alpha_0$ æˆ– $\alpha_1$ çš„ç›´æ¥å®ç°ã€‚</strong></p>
</blockquote>

<h2 id="å°ç»“">å°ç»“</h2>

<p>å¦‚æœæŠŠè¿™ç¯‡æ–‡ç« å‹ç¼©æˆä¸€å¥è¯ï¼Œå°±æ˜¯ï¼š</p>

<blockquote>
  <p><strong>è®¸å¤šâ€œå¤§æ¨¡å‹ RL è®­æ¨ä¸ä¸€è‡´â€å’Œâ€œå¼‚æ­¥è®­ç»ƒâ€é—®é¢˜ï¼Œåœ¨æœ¬æ–‡çš„è§†è§’ä¸‹ï¼Œå…¶å®éƒ½å¯ä»¥ç†è§£ä¸ºï¼šåœ¨ TRPO æ¡†æ¶ä¸‹ï¼Œå½“è¡Œä¸ºç­–ç•¥ $\mu$ å’Œå‚è€ƒç­–ç•¥ $\pi_{\theta_{\text{old}}}$ ä¸ä¸€è‡´æ—¶ï¼ŒäºŒè€…ä¹‹é—´çš„åç§»ï¼ˆ$\alpha_1$ï¼‰è¢«ä¸¥é‡ä½ä¼°äº†ã€‚</strong></p>
</blockquote>

<p>ä»ä¸¤ç­–ç•¥åˆ°ä¸‰ç­–ç•¥ï¼Œæˆ‘ä»¬åšçš„äº‹æƒ…å…¶å®å¾ˆç®€å•ï¼š</p>

<ul>
  <li>æŠŠ TRPO çš„ä¸‹ç•Œä»â€œæ—§ç­–ç•¥ vs æ–°ç­–ç•¥â€çš„å™è¿°ï¼Œæ”¹å†™æˆâ€œ<strong>è¡Œä¸ºç­–ç•¥ â€“ å‚è€ƒç­–ç•¥ â€“ ç›®æ ‡ç­–ç•¥</strong>â€ä¸‰è€…çš„å…³ç³»ï¼›</li>
  <li>æ˜¾å¼åœ°æ‹†å‡ºäº†ä¸¤ä¸ª TV è·ç¦»ï¼š
    <ul>
      <li><strong>çº¦æŸ 1ï¼šå‚è€ƒ vs ç›®æ ‡</strong> $\alpha_0$ï¼Œå¯¹åº” PPO / GRPO / GSPO ç­‰å·¥ä½œé‡Œæœ€å¸¸è§çš„ KL / clip / trust regionï¼›</li>
      <li><strong>çº¦æŸ 2ï¼šè¡Œä¸º vs å‚è€ƒ</strong> $\alpha_1$ï¼Œå¯¹åº”å¼‚æ­¥æ¡†æ¶ã€è®­æ¨å·®å¼‚ã€MoE è·¯ç”±ã€kernel éç¡®å®šæ€§ç­‰ç°å®å› ç´ ï¼›</li>
    </ul>
  </li>
  <li>å¾—åˆ°äº†ä¸€ä¸ªéå¸¸ç›´æ¥çš„ç»“è®ºï¼š
æ›¿ä»£ç›®æ ‡ $L_\mu(\pi_\theta)$ å’ŒçœŸå®æ€§èƒ½ $\mathcal{J}(\pi_\theta)$ çš„ gap æ­£æ¯”äº $\alpha_0 + \alpha_1$ã€‚</li>
</ul>

<p>åœ¨è¿™ä¸ªè§†è§’ä¸‹ï¼ˆå½“ç„¶è¿™åªæ˜¯ä¼—å¤šå¯èƒ½è§†è§’ä¹‹ä¸€ï¼‰ï¼š</p>

<ul>
  <li>Decoupled PPO / AReaL å¯ä»¥è¢«çœ‹ä½œæ˜¯åœ¨<strong>å½¢å¼ä¸Šæ‰¿è®¤â€œä¸‰ç­–ç•¥å­˜åœ¨â€</strong>ï¼Œå¹¶å°è¯•åœ¨ç›®æ ‡å‡½æ•°ä¸Šå°†â€œè¡Œä¸ºåˆ†å¸ƒâ€å’Œâ€œå‚è€ƒç­–ç•¥â€è§£è€¦ï¼›</li>
  <li>TISã€IcePopã€MISã€WTRS åˆ™æ˜¯é€šè¿‡ IS æˆ–è€…æ©ç æœºåˆ¶åœ¨æ ·æœ¬å±‚é¢å®æ–½â€çº¦æŸ 2â€ï¼š
    <ul>
      <li>TISï¼šç”¨ token-level æˆªæ–­æƒé‡å‰Šå¼±æ¯”ç‡è¿‡å¤§æ ·æœ¬çš„å½±å“ï¼›</li>
      <li>IcePopï¼šåœ¨ MoE åœºæ™¯ä¸‹ç”¨ token-level åŒä¾§æ©ç ç¡¬æ€§ä¸¢å¼ƒâ€æç«¯ä¸ä¸€è‡´â€çš„ tokenï¼›</li>
      <li>MISï¼šåœ¨ sequence-level ç›´æ¥å±è”½æ•´æ¡â€æ¯”ç‡è¿‡å¤§â€çš„è½¨è¿¹ï¼›</li>
      <li>WTRSï¼šåœ¨ token-level æ£€æµ‹æ¯”ç‡è¿‡å°çš„ä¿¡å·ï¼Œä¸€æ—¦å‘ç°åˆ™åœ¨ sequence-level æ‹’ç»æ•´æ¡è½¨è¿¹ï¼›</li>
    </ul>
  </li>
  <li><strong>routing replayï¼ˆè·¯ç”±å›æ”¾ï¼‰åœ¨ä¸‰ç­–ç•¥ TRPO çš„è§†è§’ä¸‹æ›´åƒæ˜¯â€œæ”¹å†™ surrogate objectiveâ€è€Œéâ€œç›´æ¥å®ç°çº¦æŸâ€</strong>ï¼šæ— è®ºå›æ”¾è¡Œä¸ºè·¯ç”±ï¼ˆR3 ç±»ï¼‰è¿˜æ˜¯å›æ”¾å‚è€ƒè·¯ç”±ï¼Œå®ƒä»¬éƒ½æŠŠåŸæœ¬çš„ $L_{\mu}(\pi_{\theta})$ æ”¹æˆäº†ä¸€ä¸ªè·¯ç”±è¢«æ¡ä»¶åŒ–/æ›¿æ¢åçš„ surrogate objectiveï¼Œç”¨<strong>ä¸€å®šçš„ç›®æ ‡åå·®ä¸è·¯ç”±å­¦ä¹ è‡ªç”±åº¦çš„æ”¶ç¼©</strong>æ¢å–<strong>é™ä½æ–¹å·®ä¸æå‡ç¨³å®šæ€§</strong>ã€‚å› æ­¤å®ƒå¹¶ä¸ä¼šçœŸæ­£æ”¶ç¼© $\alpha_0$ æˆ– $\alpha_1$ï¼Œè€Œæ˜¯è®©è·¯ç”±ä¸ä¸€è‡´åœ¨ loss ä¸­â€œä¸å¯è§â€ï¼›</li>
  <li>ã€ŠRL è€è®­å´©ï¼Ÿè®­æ¨å·®å¼‚æ˜¯åŸºçŸ³ã€‹ã€ä»¥åŠå‰æ–‡æåˆ°çš„ <em>Defeating Nondeterminism in LLM Inference</em> ç­‰å·¥ç¨‹ç»éªŒï¼Œåˆ™å¯ä»¥ç†è§£ä¸ºåœ¨<strong>ç³»ç»Ÿä¾§å’Œæ•°å€¼å®ç°ä¾§</strong>ï¼Œå°½å¯èƒ½æŠŠ $\alpha_1$ å‹ä½ï¼Œè®©ç®—æ³•å±‚çš„å‡è®¾ä¸è‡³äºå®Œå…¨å¤±æ•ˆã€‚</li>
</ul>

<p>ä»è¿™ä¸ªç»Ÿä¸€è§†è§’å‡ºå‘ï¼Œä¹Ÿè®¸æœ‰åŠ©äºå›ç­”å‡ ä¸ªå®é™…é—®é¢˜ï¼ˆè¿™é‡Œåªæ˜¯æŠ›å‡ ä¸ªå¼€æ”¾æ€§é—®é¢˜ï¼‰ï¼š</p>

<ul>
  <li>åœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬è¿˜èƒ½æŠŠâ€œå¤§æ¨¡å‹ RL è®­ç»ƒâ€ç†è§£æˆæŸç§æ„ä¹‰ä¸Šçš„â€œè¿‘ä¼¼ TRPO / PPOâ€ï¼Ÿ</li>
  <li>å¯¹ä¸€ä¸ªå…·ä½“çš„ RL ç³»ç»Ÿï¼Œæˆ‘ä»¬ç©¶ç«Ÿåº”è¯¥æŠŠä¸»è¦ç²¾åŠ›èŠ±åœ¨ï¼š
    <ul>
      <li>æ”¶ç´§ $\alpha_0$ï¼ˆæ›´å¼ºçš„ KL / æ›´ç¨³çš„ sequence-level ç›®æ ‡ï¼‰ï¼Œè¿˜æ˜¯</li>
      <li>å‹ä½ $\alpha_1$ï¼ˆæ›´ä¸€è‡´çš„è®­æ¨æ¡†æ¶ã€æ›´æ¿€è¿›çš„ MIS / TIS / IcePopï¼‰ï¼Ÿ</li>
    </ul>
  </li>
  <li>åœ¨ MoEã€å¼‚æ­¥é‡‡æ ·ã€å¤æ‚ agent workflow è¿™äº›ç°å®è®¾å®šä¸‹ï¼Œæˆ‘ä»¬è¿˜èƒ½å®‰å…¨åœ°å‡è£…â€œ$\mu \approx \pi_{\theta_{\text{old}}}$â€å¤šä¹…ï¼Ÿ</li>
</ul>

<p>æœ¬æ–‡åªæ˜¯åœ¨ TRPO è¿™ä¸ªè€æ¡†æ¶ä¸Šåšäº†ä¸€ä¸ªéå¸¸â€œ<strong>æœ€å°åŒ–</strong>â€çš„å»¶å±•ï¼ŒæŠŠâ€œä¸‰ç­–ç•¥â€æ˜¾å¼å†™å‡ºæ¥ï¼Œå¹¶ç”¨å®ƒæ¥æ•´ç†ç°æœ‰çš„ä¸€äº›å·¥ä½œã€‚éš¾å…æœ‰ç†è§£åå·®æˆ–é—æ¼ä¹‹å¤„ï¼Œå¦‚æœä½ ä¹Ÿå…³æ³¨å®é™…å¤§æ¨¡å‹ RL è®­ç»ƒçš„æƒ…å†µï¼Œæ¬¢è¿æŠŠä½ è‡ªå·±çš„è®¾å®šæŠ½è±¡æˆâ€œ$\mu,\pi_{\theta_{\text{old}}},\pi_\theta$ ä¸‰è€…çš„å…³ç³»â€ï¼Œå†å›å¤´çœ‹çœ‹ Theorem 2 é‡Œçš„é‚£æ¡ä¸ç­‰å¼ï¼Œæˆ–è®¸ä¼šæœ‰ä¸ä¸€æ ·çš„ç›´è§‚æ„Ÿå—ã€‚</p>

<p><a href="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html">è‹±æ–‡ç‰ˆ From Two Policies to Three: Extending TRPO under Behaviorâ€“Reference Policy Mismatch in LLM RL</a></p>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[ç°ä»£ LLM RL æµç¨‹å¸¸å¸¸åœ¨"æ—§ç­–ç•¥"æ‚„ç„¶åç¦»å®é™…ç”Ÿæˆ rollout çš„è¡Œä¸ºç­–ç•¥æ—¶è¿›è¡Œè®­ç»ƒï¼Œç ´åäº†é€šå¸¸çš„åŒç­–ç•¥å‡è®¾ã€‚æœ¬æ–‡å°†ç»å…¸çš„ TRPO ä¸‹ç•Œæ”¹å†™ä¸ºä¸‰ç­–ç•¥å½¢å¼â€”â€”è¡Œä¸ºç­–ç•¥ã€å‚è€ƒç­–ç•¥å’Œç›®æ ‡ç­–ç•¥â€”â€”ä½¿å¾—æ€§èƒ½å·®è·å¯ä»¥åˆ†è§£ä¸ºä¸¤ä¸ªå¯ä»¥æ¨ç†å’Œæ§åˆ¶çš„ TV è·ç¦»ã€‚åœ¨è¿™ä¸€è§†è§’ä¸‹ï¼ŒDecoupled PPOã€AReaLã€TISã€IcePopã€sequence-level MISã€æœ€å Token æ‹’ç»é‡‡æ · (WTRS)ã€MoE è·¯ç”±å›æ”¾ç­‰æ–¹æ³•ï¼Œä»¥åŠå¸¸è§çš„è®­æ¨å¯¹é½å·¥ç¨‹æŠ€å·§ï¼Œéƒ½å¯ä»¥çœ‹ä½œæ˜¯ç¼©å°è¿™ä¸¤ä¸ªåå·®çš„ä¸åŒæ–¹å¼ã€‚]]></summary></entry><entry><title type="html">From Two Policies to Three: Extending TRPO under Behaviorâ€“Reference Policy Mismatch in LLM RL</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html" rel="alternate" type="text/html" title="From Two Policies to Three: Extending TRPO under Behaviorâ€“Reference Policy Mismatch in LLM RL" /><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html"><![CDATA[<ul>
  <li>TOC</li>
</ul>

<p><a href="/reinforcement-learning/2025/11/15/three-policy-cn.html">ä¸­æ–‡ç‰ˆæœ¬</a> | <a href="https://zhuanlan.zhihu.com/p/1973206684907365344">çŸ¥ä¹ç‰ˆæœ¬ <img src="https://static.zhihu.com/heifetz/favicon.ico" alt="Zhihu" /></a></p>

<p><img src="/assets/img/three-policy-mini-class-en.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<h2 id="traininginference-mismatch-and-asynchronous-frameworks">Trainingâ€“Inference Mismatch and Asynchronous Frameworks</h2>

<p>Recently Iâ€™ve seen quite a lot of discussion around <em>trainingâ€“inference mismatch</em> and <em>asynchronous RL frameworks</em> for large language models. My intuition is that many of these seemingly diverse and complicated issues are, in fact, manifestations of a more fundamental tension: a mismatch between the <strong>behavior policy</strong> and the <strong>reference policy</strong>.</p>

<p>In this post, Iâ€™ll first briefly summarize the related work Iâ€™ve come across, and then try to connect them through the lens of â€œbehavior policy vs. reference policy,â€ as a complementary way to look at the problem.</p>

<p>Throughout the post Iâ€™ll use:</p>

<ul>
  <li>
    <p><strong>Behavior policy</strong> $\mu$: the policy that <em>actually</em> generates rollouts, i.e., â€œunder which distribution your data are sampled.â€ In modern LLM RL systems this typically corresponds to the implementation inside the inference engine (vLLM, SGLang, etc.), and under asynchronous frameworks it is often a <strong>mixture distribution over multiple worker policies</strong>.</p>
  </li>
  <li>
    <p><strong>Reference policy</strong> $\pi_{\theta_{\text{old}}}$: the policy used in the training objective for importance sampling, clipping, or KL constraints â€” typically the â€œold policyâ€ in PPO / GRPO.</p>
  </li>
  <li>
    <p><strong>Target policy</strong> $\pi_\theta$: the policy we optimize in the training objective, i.e., â€œwhat we want the model to becomeâ€ â€” typically the â€œnew policyâ€ in PPO / GRPO.</p>
  </li>
</ul>

<p>In the classical idealized setup, we usually <strong>implicitly assume</strong> $\mu = \pi_{\theta_{\text{old}}}$. In real systems, however, asynchronous updates, different inference / training backends, MoE routing fluctuations, and even hardware-level numerical differences cause these two policies to deviate to varying degrees.</p>

<h2 id="related-work">Related Work</h2>

<p>Below is a rough timeline of the works that left a strong impression on me (this is only a partial and biased subset of the literature Iâ€™ve seen):</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2110.00641">Decoupled PPO</a> was among the first to point out that in trust-region policy optimization methods (TRPO and PPO), the â€œold policyâ€ actually plays two distinct roles:
    <ol>
      <li>
        <p>It is used for importance sampling to perform off-policy correction. In this sense, the â€œold policyâ€ is meant to represent the <strong>behavior policy</strong> that generated the training data.</p>
      </li>
      <li>
        <p>It is also used to limit the update step size of the new policy. In this sense, the â€œold policyâ€ acts as a baseline to measure how much the new and old policies differ, i.e., a <strong>proximal policy</strong> (what I call the reference policy here).</p>
      </li>
    </ol>

    <p>The paper points out that these two roles do <em>not</em> have to be played by the same policy, and proposes the Decoupled PPO objective, which explicitly decouples â€œwho generates the dataâ€ from â€œwho defines the trust regionâ€ at the level of the optimization objective.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2505.24298">AReaL</a> focuses on the mismatch between behavior and reference policies under asynchronous training frameworks: rollouts are often generated by <strong>stale parameter versions</strong> or <strong>different workers</strong>. The paper adopts a Decoupled-PPO-style objective in the asynchronous setting, explicitly separating the behavior distribution from the reference policy, while still maintaining PPO-like optimization properties in this asynchronous regime.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2507.18071">GSPO</a> starts from stability issues of GRPO on long sequences and MoE models. It shows that token-level PPO / GRPO can become highly unstable when MoE expert routing is extremely volatile (especially when routing differs significantly between old and new policies), leading to large variance and training collapse. GSPO proposes a <strong>sequence-level</strong> PPO-style objective and ratio constraint, using the ratio over entire sequences to control updates. This substantially mitigates training collapse in MoE scenarios caused by routing instability and token-level noise.</p>
  </li>
  <li>
    <p><a href="https://fengyao.notion.site/off-policy-rl#28b721e3f6c480c3a756f8fb319e860d">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training</a> observes that in existing LLM RL frameworks (such as VeRL), the inference stack and the training stack often differ across multiple functional modules (e.g., vLLM vs. FSDP / Megatron kernels and operators). This makes the behavior policy $\mu$ differ from the reference policy $\pi_{\theta_{\text{old}}}$, so what is <em>assumed</em> to be on-policy training actually becomes off-policy training with nontrivial bias. The article summarizes two existing ways to handle this: PPO-IS and vanilla-IS, and further proposes <strong>token-level truncated importance sampling (TIS)</strong> to downweight samples with severe trainingâ€“inference mismatch. The author also wrote two more foundational notes analyzing trainingâ€“inference mismatch from basic principles: <a href="https://fengyao.notion.site/pg-seq-token-part1-basics">Part I</a> and <a href="https://fengyao.notion.site/pg-seq-token-part2-mismatch">Part II</a>.</p>
  </li>
  <li>
    <p><a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference">Defeating Nondeterminism in LLM Inference</a> points out that the lack of <strong>batch-size invariance</strong> is a core source of randomness in LLM inference: the same input can yield noticeably different probability distributions under different batch compositions and kernel paths. This means that even when you â€œnominallyâ€ have a single set of parameters, the <strong>behavior policy</strong> $\mu$ realized in practice can fluctuate with system load and scheduling, further exacerbating trainingâ€“inference mismatch.</p>
  </li>
  <li>
    <p><a href="https://ringtech.notion.site/icepop">Small Leak Can Sink a Great Shipâ€”Boost RL Training on MoE with ğ‘°ğ’„ğ’†ğ‘·ğ’ğ’‘!</a> observes that the above mismatch issues are further amplified in MoE models: routing itself is highly sensitive to small perturbations, and stacked with inference / training implementation differences and asynchronous sampling, it is easy to magnify bias and instability. The paper proposes IcePop: at the <strong>token level</strong>, it computes importance sampling ratios and applies <strong>two-sided masking</strong> to discard tokens whose ratios are either too large or too small. This removes â€œvery noisyâ€ data from the gradient, stabilizing RL training on MoE models.</p>
  </li>
  <li>
    <p><a href="https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda">When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch</a> gives a systematic analysis of the causes of trainingâ€“inference mismatch, including large amounts of out-of-distribution and low-probability content introduced by agent workflows, hardware and kernel-level numerical uncertainty, and how <strong>token-level</strong> importance sampling can introduce severe bias on long sequences. It further proposes <strong>sequence-level</strong> masked importance sampling (sequence-level MIS): compute an IS ratio at the sequence level and discard only those sequences whose overall ratio is too large, thereby controlling bias while strongly suppressing training collapse caused by extreme samples. The paper provides reasonably complete theoretical derivations and extensive experimental evidence.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2510.11370">Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</a> focuses on the MoE-specific problem of <strong>routing inconsistency</strong>. The paper finds that even for identical inputs, inference and training can route tokens to different experts due to small differences in operator implementations or parallelism. This â€œphysical-pathâ€ mismatch makes the gap between the behavior policy $\mu$ and the reference policy $\pi_{\theta_{\text{old}}}$ much larger than expected and can easily cause training collapse. To address this, the paper proposes <strong>Rollout Routing Replay (R3)</strong>: during rollout it records, for each token, the actual expert indices selected by the inference router, and during training it <strong>replays</strong> these routing decisions instead of recomputing them. In effect, R3 forces the training and inference stacks to share the same routing paths in the MoE topology, aligning the two sides at the level of the computation graph.</p>
  </li>
  <li>
    <p><a href="https://zhuanlan.zhihu.com/p/1959976628290590602">RL è€è®­å´©ï¼Ÿè®­æ¨å·®å¼‚æ˜¯åŸºçŸ³</a> approaches the problem more from a practical perspective, sharing experience on how to engineer for near trainingâ€“inference consistency: choosing consistent operators and precision settings, monitoring and constraining the log-prob gap between training and inference, etc. The focus is on framework-level engineering practices that can mitigate trainingâ€“inference difference at the root.</p>
  </li>
  <li>
    <p><a href="https://verl.readthedocs.io/en/latest/algo/rollout_corr.html">verl Rollout Importance Sampling</a> introduces a <strong>Token Veto</strong> mechanism in its rollout correction module: it computes <strong>token-level</strong> importance ratios $\rho_t^{(\text{ref}\leftarrow\text{beh})}$, and if any token in a trajectory satisfies $\min_t \rho_t &lt; \tau_{\text{veto}}$, the entire sequence is discarded from training. This â€œtoken-level detection, sequence-level vetoâ€ design embodies a conservative â€œone-vote vetoâ€ strategy.</p>
  </li>
  <li><a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf">INTELLECT-3 Technical Report</a> adopts a similar rejection sampling strategy in its asynchronous distributed RL training framework. INTELLECT-3 computes <strong>token-level</strong> importance ratios for each rollout; if any tokenâ€™s ratio falls below a threshold ($10^{-5}$ in the paper), the entire trajectory is masked.</li>
</ul>

<h2 id="a-minimally-unified-view-from-a-three-policy-trpo-perspective">A Minimally Unified View from a Three-Policy TRPO Perspective</h2>

<p>At first glance, the works listed above seem to tackle different aspects:</p>

<ul>
  <li><strong>Algorithmic level</strong>: how to formulate PPO / GRPO objectives, token-level vs. sequence-level, clip vs. mask, etc.</li>
  <li><strong>Systems level</strong>: how to align inference and training stacks.</li>
  <li><strong>Model level</strong>: how MoE routing amplifies instability, and so on.</li>
</ul>

<p>However, if we align everything along a single axis â€” <strong>behavior policy vs. reference policy</strong> â€” a large fraction of these issues can be placed in a relatively simple theoretical framework: a <strong>three-policy TRPO</strong>.</p>

<p>In the next section Iâ€™ll unpack this three-policy TRPO in as simple math as I can. You can think of it as â€œTRPO + triangle inequalityâ€ â€” a very small extension conceptually, but surprisingly handy when analyzing trainingâ€“inference mismatch in LLM RL:</p>

<ul>
  <li>On the one hand, it helps us understand what exactly â€œtrainingâ€“inference mismatchâ€ and â€œasynchronous training frameworksâ€ are harming within the TRPO view.</li>
  <li>On the other hand, it offers a unifying way to interpret TIS, IcePop, sequence-level MIS, etc. In the view of this post, they can all be seen as different incarnations of <strong>Constraint 2</strong> introduced below.</li>
</ul>

<h3 id="three-policies">Three Policies</h3>

<p>We stick to the notation from above and consider a discounted MDP with discount factor $\gamma \in (0,1)$:</p>

<ul>
  <li>States $s \in \mathcal{S}$, actions $a \in \mathcal{A}$.</li>
  <li>Policy $\pi(a \mid s)$.</li>
  <li>Discounted state distribution:
\(d_\pi(s) := (1-\gamma)\sum_{t=0}^\infty \gamma^t \Pr_\pi(s_t = s).\)</li>
  <li>Return (episodic view):
\(\mathcal{J}(\pi) := \mathbb{E}_\pi\Big[\sum_{t=0}^\infty \gamma^t r_t\Big].\)</li>
  <li>Value / Q / advantage functions:
\(V_\pi(s),\quad Q_\pi(s,a),\quad A_\pi(s,a) := Q_\pi(s,a) - V_\pi(s).\)</li>
</ul>

<p>Itâ€™s worth spelling out that in the three-policy setup we have:</p>

<ul>
  <li>
    <p><strong>Behavior policy</strong> $\mu$: the policy that actually generates rollouts. Data $(s,a,r,\dots)$ are sampled from it.</p>
  </li>
  <li>
    <p><strong>Reference policy</strong> $\pi_{\theta_{\text{old}}}$: the â€œold policyâ€ used in the optimization objective for importance sampling ratios, clipping, or KL constraints.</p>
  </li>
  <li>
    <p><strong>Target policy</strong> $\pi_\theta$: the policy we are optimizing in this update.</p>
  </li>
</ul>

<p>In the ideal setup we assume $\mu = \pi_{\theta_{\text{old}}}$; in real systems they are often unequal. This is the mathematical shadow of â€œtrainingâ€“inference mismatch.â€</p>

<h3 id="two-policy-trpo">Two-Policy TRPO</h3>

<blockquote>
  <p>If youâ€™re already familiar with TRPO, feel free to skip ahead to the â€œThree-Policy TRPOâ€ subsection.</p>
</blockquote>

<p>All the theoretical guarantees in TRPO are stated <strong>with respect to the advantage function of some baseline policy</strong>. Since the only advantage we can estimate reliably in practice is $A_\mu$ (data are sampled under $\mu$), we may as well treat $\mu$ as the baseline policy.</p>

<p>A classical result is the <strong>Performance Difference Lemma</strong>:</p>

<blockquote>
  <p>For any two policies $\mu$ and $\pi_\theta$, we have</p>

\[\mathcal{J}(\pi_\theta) - \mathcal{J}(\mu)
= \frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d_{\pi_\theta},\, a\sim\pi_\theta}[A_\mu(s,a)].\]
</blockquote>

<p>The intuition is simple:</p>

<ul>
  <li>$A_\mu(s,a)$ says: â€œif I deviate from what $\mu$ would do at state $s$ and instead take action $a$, how much will the long-term return change?â€</li>
  <li>Summing that â€œgainâ€ across all time steps, states, and actions gives the total improvement of the new policy over the behavior policy.</li>
</ul>

<p>The challenge in TRPO is that we cannot compute</p>

\[\mathbb{E}_{s\sim d_{\pi_\theta}, a\sim\pi_\theta}[A_\mu(s,a)]\]

<p>exactly, because $d_{\pi_\theta}$ is the state distribution of the <em>new</em> policy, under which we do not have samples.</p>

<p>So TRPO introduces a surrogate objective by replacing the state distribution with that of the behavior policy:</p>

\[L_\mu(\pi_\theta)
:= \mathcal{J}(\mu) + \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_\mu,\,a\sim \pi_\theta}[A_\mu(s,a)].\]

<p>Intuitively, $L_\mu$ asks the following question: â€œUnder the states visited by the behavior policy, how good is the new policy if we just let it pick the actions?â€</p>

<p>Starting from the Performance Difference Lemma, the difference between the true objective and the surrogate is:</p>

\[\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)
= \frac{1}{1-\gamma}\;
  \sum_s \big(d_{\pi_\theta}(s) - d_\mu(s)\big)
  \,\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[A_\mu(s,a)].\]

<p>If we define</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|,\]

<p>we immediately get the following upper bound:</p>

<blockquote>
  <p><strong>Lemma 1</strong></p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\;
    \|d_{\pi_\theta} - d_\mu\|_1.\]
</blockquote>

<p>This reveals the first key quantity:</p>

<blockquote>
  <p><strong>State distribution shift</strong> $|d_{\pi_\theta} - d_\mu|_1$, i.e., â€œhow differently the new policy sees the world, compared to the behavior policy.â€</p>
</blockquote>

<p>We usually do <em>not</em> directly impose constraints on $|d_{\pi_\theta} - d_\mu|_1$. Instead, we constrain the per-timestep action distribution difference â€” via trust regions, KL penalties, clipping, etc.</p>

<p>Define the total variation (TV) distance:</p>

\[D_{\mathrm{TV}}(p,q) := \frac{1}{2}\|p-q\|_1.\]

<p>Assume there is a constant $\beta$ such that</p>

<blockquote>
  <p>For all $s$, the TV distance between the behavior and target policies is bounded:</p>

\[D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big) \le \beta.\]
</blockquote>

<p>Intuitively: in any state, the action distribution of the â€œnew policyâ€ cannot deviate too much from that of the policy that generated the data.</p>

<p>A standard result (provable via coupling) is:</p>

<blockquote>
  <p><strong>Lemma 2</strong>
Under the assumption above,</p>

\[\|d_{\pi_\theta} - d_\mu\|_1
\le \frac{2\gamma}{1-\gamma}\,\beta.\]
</blockquote>

<p>Combining Lemma 1 and Lemma 2, we obtain</p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\; \frac{2\gamma}{1-\gamma}\,\beta
= \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\beta.\]

<p>This gives a compact <strong>two-policy TRPO lower bound (baseline = behavior policy)</strong>:</p>

<blockquote>
  <p><strong>Theorem 1 (Two-Policy TRPO)</strong></p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
\frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\beta.\]
</blockquote>

<p>This suggests:</p>

<ul>
  <li><strong>What really matters for the tightness of $L_\mu(\pi_\theta)$ as a surrogate for $\mathcal{J}(\pi_\theta)$ is how far the behavior policy $\mu$ and the target policy $\pi_\theta$ drift apart:</strong>
\(\beta = \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big).\)</li>
</ul>

<p>If you can directly control this $\beta$, you can essentially port TRPOâ€™s monotonic improvement guarantees to the behavior-policy view.</p>

<h3 id="three-policy-trpo">Three-Policy TRPO</h3>

<p>In practice, especially in large-scale LLM RL, <strong>we often cannot directly control $\beta$ itself.</strong></p>

<p>In most PPO / GRPO / GSPO / RLHF-style frameworks, the actual situation is:</p>

<ul>
  <li>Rollout data are generated by some <strong>behavior policy</strong> $\mu$ (some particular parameter version plus system details inside the inference engine).</li>
  <li>During updates, we would like to leverage a <strong>reference policy</strong> $\pi_{\theta_{\text{old}}}$ to limit the update of the <strong>target policy</strong> $\pi_\theta$.</li>
</ul>

<p>In other words, what we can actually touch and control are two quantities:</p>

<ol>
  <li>
    <p><strong>Reference vs. target</strong>: via KL penalties, clipping, etc., we constrain</p>

\[D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big).\]
  </li>
  <li>
    <p><strong>Behavior vs. reference</strong>: we would <em>like</em> to keep
\(D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)\)
small as well â€” this is where trainingâ€“inference mismatch and asynchronous execution come in.</p>
  </li>
</ol>

<p>This motivates defining two â€œproxy gapsâ€:</p>

<ul>
  <li>
    <p><strong>Constraint 1: reference vs. target</strong></p>

\[\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big);\]
  </li>
  <li>
    <p><strong>Constraint 2: behavior vs. reference</strong>
\(\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big).\)</p>
  </li>
</ul>

<p>Intuitively:</p>

<ul>
  <li>$\alpha_0$: how far the new policy is from the â€œold policyâ€ you are using in the loss â€” this is the trust-region part.</li>
  <li>$\alpha_1$: how far the reference policy used in training is from the <em>actual</em> behavior policy that generated the data â€” this is the footprint of trainingâ€“inference mismatch and asynchrony.</li>
</ul>

<p>Now we can plug these two quantities back into the TRPO lower bound.</p>

<p>For any state $s$, by the triangle inequality we have</p>

\[\begin{aligned}
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
&amp;\le
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)
\\
&amp;\quad +
D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big).
\end{aligned}\]

<p>Taking the supremum over $s$ gives</p>

\[\beta
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
\;\le\;
\alpha_1 + \alpha_0.\]

<p>Plugging this inequality into the two-policy TRPO bound (Theorem 1), and denoting</p>

\[C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2},\]

<p>we obtain</p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,\beta
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,(\alpha_0 + \alpha_1).\]

<p>This yields a very direct <strong>three-policy TRPO lower bound</strong>:</p>

<blockquote>
  <p><strong>Theorem 2 (Three-Policy TRPO)</strong>
Let</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|,\quad
C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2},\]

  <p>and</p>

\[\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big),
\quad
\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big).\]

  <p>Then for any target policy $\pi_\theta$,</p>

\[\boxed{
\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\; C\,(\alpha_0 + \alpha_1)
}\]

  <p>where</p>

\[L_\mu(\pi_\theta)
:=
\mathcal{J}(\mu) + \frac{1}{1-\gamma}
  \mathbb{E}_{s\sim d_\mu,a\sim\pi_\theta}[A_\mu(s,a)].\]
</blockquote>

<p>The meaning of this bound is quite straightforward:</p>

<ul>
  <li><strong>The gap between the surrogate objective $L_\mu(\pi_\theta)$ and the true performance $\mathcal{J}(\pi_\theta)$ decomposes into two pieces:</strong>
    <ul>
      <li>The deviation between reference and target policies, $\alpha_0$.</li>
      <li>The deviation between behavior and reference policies, $\alpha_1$.</li>
    </ul>
  </li>
</ul>

<p>As long as both terms are small, <strong>optimizing $L_\mu$ is likely to improve $\mathcal{J}$</strong>.</p>

<h3 id="how-to-control-these-two-deviations-in-practice">How to Control These Two Deviations in Practice?</h3>

<p>We can now revisit various practical methods through the lens of Theorem 2:</p>

<ul>
  <li>Most PPO / GRPO / GSPO-style work focuses on controlling <strong>Constraint 1: $\alpha_0$</strong>.</li>
  <li>Most TIS / IcePop / MIS-style work, in the view of this post, can be understood as primarily targeting <strong>Constraint 2: $\alpha_1$</strong>.</li>
</ul>

<p>In the remainder of this post I will focus on <strong>Constraint 2</strong>.</p>

<p>The goal of Constraint 2 is: <strong>ensure that the data used in training come (effectively) from a behavior policy that is close to the reference policy.</strong></p>

<p>In practice, this usually involves both <strong>system-level mechanisms</strong> and <strong>algorithmic mechanisms (importance sampling)</strong>.</p>

<ol>
  <li><strong>System level: keep the behavior policy from drifting too far</strong>
    <ul>
      <li>
        <p>Asynchronous frameworks:
Tag each sample with a policy version, and only use data generated by parameter versions that are close enough to $\pi_{\theta_{\text{old}}}$.</p>
      </li>
      <li>
        <p>Trainingâ€“inference alignment:
Use consistent precision, operators, and similar kernel behavior between the training and inference stacks.</p>
      </li>
    </ul>

    <p>These mechanisms act â€œoutsideâ€ the algorithm to make $\mu$ closer to $\pi_{\theta_{\text{old}}}$, thereby shrinking $\alpha_1$.</p>
  </li>
  <li>
    <p><strong>Algorithmic level: sample-wise correction</strong></p>

    <p>At the algorithmic level, we no longer attempt to â€œfixâ€ the entire behavior policy. Instead, we use importance sampling ratios to correct at the <strong>sample level</strong>: we filter or reweight samples so that the behavior policy is close to the reference policy <em>on the subset of data that actually participates in training</em>, or at least reduce the influence of samples with large mismatch.</p>

    <p>Concretely, this gives rise to methods like TIS, IcePop, and MIS, which can be seen as different ways of implementing Constraint 2 at the sample level.</p>
  </li>
</ol>

<h2 id="importance-sampling-and-masking-four-implementations-of-constraint-2">Importance Sampling and Masking: Four Implementations of Constraint 2</h2>

<p>In this section Iâ€™ll reuse the notation introduced above to write down the objectives of these three methods, focusing only on the design choices related to â€œbehavior vs. reference policy.â€ Let the token-level PPO / GRPO-style update term be</p>

\[g_\theta(t)
= \min\big(r_t(\theta) A_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon) A_t\big),\]

<p>where</p>

\[r_t(\theta) = \frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)},
\quad (s_t,a_t)\sim\mu,\quad A_t := A_\mu(s_t,a_t).\]

<p>Here:</p>

<ul>
  <li>$r_t(\theta)$ is the <strong>target vs. reference</strong> ratio (corresponding to Constraint 1).</li>
  <li>$A_t$ is the advantage estimated from data sampled under the behavior policy.</li>
</ul>

<p>To connect token-level $(s_t,a_t)$ with sequence-level $(x,y)$ notation, consider the RLHF setting (reinforcement learning from human feedback) for LLMs:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Prompts are denoted by $x$, and responses by $y = (y_1,\dots,y_{</td>
          <td>y</td>
          <td>})$.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Token-level states and actions are defined as $s_t := (x,y_{&lt;t})$, $a_t := y_t$.</li>
  <li>The behavior and reference policies on sequences can then be written as
\(\mu(y\mid x) = \prod_{t=1}^{|y|}\mu(a_t=y_t\mid s_t),\quad
\pi_{\theta_{\text{old}}}(y\mid x) = \prod_{t=1}^{|y|}\pi_{\theta_{\text{old}}}(a_t=y_t\mid s_t).\)</li>
</ul>

<p>To quantify the deviation between reference and behavior policies, we can define the token-level importance ratio:</p>

\[\rho_t^{(\text{ref}\leftarrow\text{beh})} :=
\frac{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}{\mu(a_t\mid s_t)},\]

<p>and its sequence-level counterpart:</p>

\[\rho(y\mid x) := \frac{\pi_{\theta_{\text{old}}}(y\mid x)}{\mu(y\mid x)}
= \prod_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})}.\]

<p>The difference between TIS, IcePop, and MIS lies in <strong>how they use $\rho$ to implement Constraint 2</strong>.</p>

<h3 id="1-tis-token-level-truncated-importance-sampling">1. TIS: Token-Level Truncated Importance Sampling</h3>

<p>TIS directly truncates the token-level ratio $\rho_t^{(\text{ref}\leftarrow\text{beh})}$; define</p>

\[\color{blue}{w_t = \min\big(\rho_t^{(\text{ref}\leftarrow\text{beh})},\ C_{\text{IS}}\big)}.\]

<p>The update objective becomes</p>

\[L_{\text{TIS}}(\theta)
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{w_t}\; g_\theta(t)\big].\]

<ul>
  <li>The blue $\color{blue}{w_t}$ is the truncated IS weight: extremely large ratios are capped at a constant $C_{\text{IS}}$.</li>
  <li>From the three-policy TRPO perspective, this is a <em>soft</em> way to downweight tokens where behavior and reference policies differ significantly, effectively reducing their contribution to $\alpha_1$ in the gradient.</li>
</ul>

<h3 id="2-icepop-token-level-two-sided-masking-in-moe">2. IcePop: Token-Level Two-Sided Masking in MoE</h3>

<p>IcePop also uses $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ as a discrepancy measure, but opts for <strong>two-sided masking</strong>:</p>

\[\color{blue}{m_t = \mathbf{1}\big[C_{\text{low}} \le \rho_t^{(\text{ref}\leftarrow\text{beh})} \le C_{\text{high}}\big]}.\]

<p>The update objective becomes</p>

\[L_{\text{IcePop}}(\theta)
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{m_t}\; g_\theta(t)\big].\]

<ul>
  <li>The blue $\color{blue}{m_t}$ decides whether a token participates in the update: tokens with ratios that are too large or too small are dropped entirely.</li>
  <li>This is a <em>hard</em> sample selection scheme: only tokens where behavior and reference policies are reasonably aligned (ratios within $[C_{\text{low}}, C_{\text{high}}]$) are kept, implementing a stricter version of Constraint 2 at the token level.</li>
</ul>

<h3 id="3-sequence-level-mis-masked-importance-sampling-over-entire-sequences">3. Sequence-Level MIS: Masked Importance Sampling Over Entire Sequences</h3>

<p>The core operation in sequence-level MIS is to <strong>retain only sequences whose sequence-level IS ratio is below a threshold $C$</strong>, zeroing out the loss for all other sequences:</p>

\[\color{blue}{
\rho(y\mid x)
\leftarrow
\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}
}\]

<p>In a unified loss form, this can be written as</p>

\[L_{\text{MIS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}}
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big].\]

<p>In words:</p>

<ul>
  <li>For <strong>sequences with small IS ratios</strong>, the full weight $\rho(y\mid x)$ is retained for off-policy correction.</li>
  <li>For <strong>sequences whose ratios exceed the threshold $C$</strong>, the entire policy loss is masked out (weight set to $0$).</li>
</ul>

<p>From the three-policy TRPO viewpoint, sequence-level MIS no longer truncates at the token level. Instead, it performs <strong>trajectory-level</strong> filtering: it drops trajectories where behavior and reference policies diverge too much, and only optimizes on the subset with $\rho(y\mid x)\le C$. This implements Constraint 2 at the sequence level.</p>

<h3 id="4-worst-token-reject-sampling-rejecting-entire-sequences-based-on-the-worst-token">4. Worst Token Reject Sampling: Rejecting Entire Sequences Based on the Worst Token</h3>

<p>The verl Token Veto mechanism and INTELLECT-3 both adopt a rejection sampling strategy that can be collectively called <strong>Worst Token Reject Sampling (WTRS)</strong>:</p>

<ul>
  <li>
    <p><strong>verl Token Veto</strong>: In its rollout correction module, if any token in a trajectory has $\min_t \rho_t &lt; \tau_{\text{veto}}$, the entire sequence is discarded via response<em>mask. The threshold $\tau</em>{\text{veto}}$ is user-configurable.</p>
  </li>
  <li>
    <p><strong>INTELLECT-3 Token Masking</strong>: In its asynchronous distributed RL framework, if any tokenâ€™s ratio is below $10^{-5}$, the entire trajectory is masked.</p>
  </li>
</ul>

<p>The core operation is identical: <strong>if any token in a trajectory has an IS ratio below a threshold $\tau$, the entire sequence is rejected from training.</strong> This can be written as:</p>

\[\color{blue}{
m(y\mid x) = \mathbf{1}\Big\{\min_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})} \ge \tau\Big\}
}\]

<p>In a unified loss form:</p>

\[L_{\text{WTRS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{m(y\mid x)}
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big].\]

<p>In words:</p>

<ul>
  <li>For <strong>sequences where all tokens have IS ratios $\ge \tau$</strong>: participate in training normally.</li>
  <li>For <strong>sequences where any token has an IS ratio $&lt; \tau$</strong>: the entire sequenceâ€™s policy loss is masked out.</li>
</ul>

<p>From the three-policy TRPO perspective, WTRS adopts a hybrid â€œtoken-level detection, sequence-level vetoâ€ strategy: it detects extreme mismatch signals at the <strong>token level</strong>, and once detected, rejects at the <strong>sequence level</strong>. This â€œone-vote vetoâ€ design reflects a conservative philosophy â€” when a trajectory contains a token that â€œthe behavior policy generated but the reference policy would almost never generate,â€ <strong>the credibility of the entire trajectory is called into question</strong>, thereby implementing control over Constraint 2 ($\mu$ vs. $\pi_{\theta_{\text{old}}}$ deviation) at the trajectory granularity.</p>

<h2 id="moe-routing-replay-what-does-it-actually-do-in-three-policy-trpo">MoE Routing Replay: What Does It Actually Do in Three-Policy TRPO?</h2>

<p>In MoE (Mixture-of-Experts) models, trainingâ€“inference mismatch often first appears as <strong>routing inconsistency</strong>: even with identical parameters, the inference and training stacks may route tokens to different experts because of small differences in operators, parallelism, or numerics. A natural engineering response is <strong>routing replay</strong>: during rollout (inference), record the actual expert paths, and during training, force the model to reuse these routing decisions.</p>

<p>These methods are often intuitively described as â€œimplementing Constraint 2 and shrinking $\alpha_1$.â€ From the three-policy TRPO perspective, a more precise statement is:</p>

<blockquote>
  <p><strong>Routing replay does not tighten the original surrogate objective via a constraint; instead, it rewrites the surrogate objective into one that is conditioned on / replaces the routing.</strong>
It makes routing mismatch invisible in the loss, but it does not actually shrink the true policy distances $\alpha_0$ or $\alpha_1$.</p>
</blockquote>

<p>Below Iâ€™ll sketch a <strong>minimal</strong> abstraction that is sufficient to make this concrete.</p>

<h3 id="surrogate-objective-in-moe-separating-routing-and-token-generation">Surrogate Objective in MoE: Separating Routing and Token Generation</h3>

<p>Abstract an MoE model as a two-stage stochastic decision: â€œfirst choose an expert $z$, then generate token $a$ conditioned on that expert.â€ The target policy can be factorized as</p>

\[\pi_\theta(a,z\mid s)=\omega_\theta(z\mid s)\,\pi_\theta(a\mid s,z),\]

<p>where:</p>

<ul>
  <li>$\omega_\theta(z\mid s)$ is the router distribution.</li>
  <li>$\pi_\theta(a\mid s,z)$ is the token distribution conditioned on expert $z$.</li>
</ul>

<p>In the three-policy TRPO setting, the surrogate objective we actually want to optimize can be written as</p>

\[L_\mu(\pi_\theta) = \mathcal{J}(\mu) + \frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_z \omega_\theta(z\mid s)\,F_\theta(s,z)
\bigg],\]

<p>where I use</p>

\[F_\theta(s,z)
:=
\sum_a \pi_\theta(a\mid s,z)\,A_\mu(s,a,z)\]

<p>to denote the expert-level aggregation of advantages.</p>

<p>The key point is that <strong>in the original $L_\mu(\pi_\theta)$, the routing distribution is precisely the current router $\omega_\theta$ that we are updating</strong>. In other words, RL on MoE is updating not only the token-generation distribution but also the router itself.</p>

<h3 id="1-replaying-behavior-policy-routing-behavior-router-replay--r3-style">(1) Replaying Behavior-Policy Routing (Behavior-Router Replay / R3-Style)</h3>

<p>R3-style methods record, during rollout, the set of experts $M_\mu(s)$ actually selected by the behavior policy on the inference side, and during training force the current policy to <strong>route only within this set</strong>. This can be written as a â€œconditional projectionâ€ of the routing distribution:</p>

\[\omega_\theta^{\text{R3}}(z\mid s)
:=
\frac{\omega_\theta(z\mid s)\,\mathbf{1}\{z\in M_\mu(s)\}}
     {\sum_{z'\in M_\mu(s)}\omega_\theta(z'\mid s)} .\]

<p>The surrogate objective that is actually optimized during training becomes</p>

\[L_\mu^{\text{R3}}(\pi_\theta) =
\mathcal{J}(\mu) +
\frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_{z\in M_\mu(s)} \omega_\theta^{\text{R3}}(z\mid s)\,F_\theta(s,z)
\bigg].\]

<p>Compared to the original $L_\mu(\pi_\theta)$, R3 does <em>not</em> push $\omega_\theta$ closer to $\omega_{\text{old}}$ or $\omega_\mu$. Instead, it:</p>

<ul>
  <li><strong>replaces the expectation over $z\sim\omega_\theta$ by a conditional expectation over $z\sim\omega_\theta(\cdot\mid z\in M_\mu(s))$</strong>, and</li>
  <li>equivalently, <strong>shrinks the feasible routing support to $M_\mu(s)$</strong>.</li>
</ul>

<p>So R3 is optimizing a â€œbehavior-routing-conditioned surrogate objective,â€ rather than the original $L_\mu(\pi_\theta)$. The benefit is substantially reduced variance and improved stability; the cost is that <strong>the routerâ€™s exploration and update freedom is constrained at every state</strong>.</p>

<h3 id="2-replaying-reference-policy-routing-reference-router-replay">(2) Replaying Reference-Policy Routing (Reference-Router Replay)</h3>

<p>Another class of routing-replay schemes instead reuses the reference policyâ€™s router $\omega_{\text{old}}$. This is equivalent to training a hybrid policy</p>

\[\hat\pi_\theta(a,z\mid s)
:=
\omega_{\text{old}}(z\mid s)\,\pi_\theta(a\mid s,z),\]

<p>with surrogate objective</p>

\[L_\mu^{\text{ref-replay}}(\pi_\theta) =
\mathcal{J}(\mu) +
\frac{1}{1-\gamma}
\mathbb{E}_{s\sim d_\mu}
\bigg[
\sum_z \omega_{\text{old}}(z\mid s)\,F_\theta(s,z)
\bigg].\]

<p>This has the effect that:</p>

<ul>
  <li>In the surrogate objective, the router is <strong>frozen to the old router</strong> $\omega_{\text{old}}$, so the â€œreference vs. targetâ€ discrepancy in routing is simply removed from the loss.</li>
  <li>Training becomes insensitive to how far the <em>new</em> router $\omega_\theta$ drifts from $\omega_{\text{old}}$, thereby sidestepping the instabilities caused by routing mismatch.</li>
</ul>

<p>Again, this is fundamentally a <strong>change of objective</strong>:</p>

<ul>
  <li>The deviation $\alpha_0$ in the true policy space is not reduced; it is merely rendered invisible by redefining the surrogate in terms of the old router.</li>
  <li>Learning of the router is effectively frozen or heavily suppressed.</li>
</ul>

<h3 id="routing-replay-as-a-change-of-surrogate-objective">Routing Replay as a Change of Surrogate Objective</h3>

<p>Putting these replay variants side by side, they share several properties:</p>

<ol>
  <li><strong>They optimize not the original $L_\mu(\pi_\theta)$, but a surrogate where routing has been conditioned or replaced.</strong></li>
  <li><strong>They do not directly shrink the three-policy TRPO boundâ€™s $\alpha_0$ or $\alpha_1$</strong>. Routing mismatch is removed from the loss, but it still exists in the true policy distances.</li>
  <li><strong>In practice they trade bias for variance</strong>: replay typically lowers variance and improves stability, but may also limit the routerâ€™s ability to learn routing patterns that are optimal for the RL objective.</li>
</ol>

<p>So, in the three-policy TRPO view, a more accurate characterization is:</p>

<blockquote>
  <p><strong>Routing replay is best thought of as a rewrite of the surrogate objective, not as a direct implementation of a constraint on $\alpha_0$ or $\alpha_1$.</strong></p>
</blockquote>

<h2 id="conclusion">Conclusion</h2>

<p>If I had to compress this post into a single sentence, it would be:</p>

<blockquote>
  <p><strong>Many issues around â€œtrainingâ€“inference mismatchâ€ and â€œasynchronous trainingâ€ in large-scale LLM RL can be understood, in the TRPO framework, as severely underestimating the deviation between the behavior policy $\mu$ and the reference policy $\pi_{\theta_{\text{old}}}$ â€” i.e., the term $\alpha_1$.</strong></p>
</blockquote>

<p>From two policies to three, what we did is conceptually very small:</p>

<ul>
  <li>
    <p>We rewrote the TRPO lower bound from an â€œold vs. new policyâ€ narrative into a â€œ<strong>behaviorâ€“referenceâ€“target</strong>â€ three-policy relationship.</p>
  </li>
  <li>We explicitly separated two TV distances:
    <ul>
      <li><strong>Constraint 1: reference vs. target</strong>, $\alpha_0$, corresponding to the KL / clip / trust-region style constraints in PPO / GRPO / GSPO.</li>
      <li><strong>Constraint 2: behavior vs. reference</strong>, $\alpha_1$, capturing real-world factors like asynchronous frameworks, trainingâ€“inference mismatch, MoE routing volatility, kernel-level nondeterminism, etc.</li>
    </ul>
  </li>
  <li>This leads to a simple conclusion:
The gap between the surrogate $L_\mu(\pi_\theta)$ and the true performance $\mathcal{J}(\pi_\theta)$ scales with $\alpha_0 + \alpha_1$.</li>
</ul>

<p>Under this lens (which is of course only one of many possible perspectives):</p>

<ul>
  <li>
    <p>Decoupled PPO / AReaL can be viewed as <strong>formally acknowledging the existence of three policies</strong> and explicitly decoupling the behavior distribution from the reference policy in the objective.</p>
  </li>
  <li>TIS, IcePop, MIS, and WTRS can be seen as different ways of implementing <strong>Constraint 2</strong> using importance sampling truncation / masking:
    <ul>
      <li>TIS: token-level truncation of IS weights to soften the influence of extreme samples.</li>
      <li>IcePop: token-level two-sided masking in MoE to hard-drop tokens with severe mismatch.</li>
      <li>MIS: sequence-level masking to ignore entire trajectories whose behaviorâ€“reference mismatch is too large.</li>
      <li>WTRS: token-level detection of extremely small ratios, rejecting the entire trajectory once such a signal is found.</li>
    </ul>
  </li>
  <li>
    <p><strong>Routing replay</strong> (whether replaying behavior routing in R3-style schemes or replaying reference routing) is better viewed as <strong>changing the surrogate objective</strong> rather than directly implementing a constraint: both variants replace the original $L_\mu(\pi_\theta)$ with a routing-conditioned / routing-frozen surrogate, trading off some objective bias and reduced routing learning freedom for lower variance and greater stability, without actually shrinking $\alpha_0$ or $\alpha_1$â€”they simply make routing mismatch invisible in the loss.</p>
  </li>
  <li>Engineering advice such as in <em>RL è€è®­å´©ï¼Ÿè®­æ¨å·®å¼‚æ˜¯åŸºçŸ³</em> and system-level work like <em>Defeating Nondeterminism in LLM Inference</em> can be interpreted as efforts to <strong>reduce $\alpha_1$ on the systems and numerical side</strong>, so that the assumptions underlying the algorithms do not break too badly.</li>
</ul>

<p>From this unified perspective, it may also be easier to think about the following practical questions (these are completely open and I donâ€™t have definitive answers):</p>

<ul>
  <li>
    <p>Under what conditions can we still reasonably interpret â€œLLM RL trainingâ€ as some approximate form of TRPO / PPO?</p>
  </li>
  <li>For a concrete RL system, where should we invest more effort:
    <ul>
      <li>tightening $\alpha_0$ (stronger KL control, more stable sequence-level objectives), or</li>
      <li>reducing $\alpha_1$ (better trainingâ€“inference alignment, more aggressive MIS / TIS / IcePop)?</li>
    </ul>
  </li>
  <li>In the presence of MoE, asynchronous sampling, and complex agent workflows, how long can we safely pretend that â€œ$\mu \approx \pi_{\theta_{\text{old}}}$â€?</li>
</ul>

<p>This post is just a very <strong>minimal</strong> extension of the classic TRPO framework, making the â€œthree policiesâ€ explicit and using them to organize some existing work. There are inevitably misunderstandings and omissions. If you also care about how RL training actually behaves in large LLM systems, Iâ€™d be very interested to see how your own setup can be abstracted into a relationship between $\mu$, $\pi_{\theta_{\text{old}}}$, and $\pi_\theta$, and then re-examined through the inequality in Theorem 2. It might give a slightly different intuitive feel for what your system is really optimizing.</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025ThreePolicyTRPO</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{From Two Policies to Three: Extending TRPO under Behavior-Reference Policy Mismatch in LLM RL}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html}</span><span class="p">,</span>
  <span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-11-23}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[Modern LLM RL pipelines often train under an "old policy" that silently drifts away from the behavior policy that actually generates rollouts, breaking the usual on-policy assumptions. This post rewrites the classic TRPO lower bound in a three-policy form â€” behavior, reference, and target â€” so that the performance gap cleanly decomposes into two TV distances that we can reason about and control. Seen through this lens, methods like Decoupled PPO, AReaL, TIS, IcePop, sequence-level MIS, Worst Token Reject Sampling (WTRS), MoE routing replay, and common engineering tricks for trainingâ€“inference alignment all become different ways of shrinking these two deviations.]]></summary></entry></feed>