<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://xihuai18.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xihuai18.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2026-01-08T00:10:42+00:00</updated><id>https://xihuai18.github.io/feed.xml</id><title type="html">Xihuai Wang’s Page</title><subtitle>Xihuai&apos;s personal page.
</subtitle><entry xml:lang="en"><title type="html">Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html" rel="alternate" type="text/html" title="Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees" /><published>2025-12-17T00:00:00+00:00</published><updated>2025-12-17T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html"><![CDATA[<h2 id="introduction-why-should-we-care-about-off-policy">Introduction: Why Should We Care About “Off-Policy”?</h2>

<p>Consider the following scenario: you are training a large language model with reinforcement learning to improve its question-answering capabilities. Ideally, each time the model generates a batch of responses, you would immediately update the model with this data, then use the updated model to generate new data, and so on. This approach of “updating with data from the same policy that generated it” is called <strong>on-policy</strong> training.</p>

<p>Reality, however, is not so simple. In large-scale distributed training, hundreds of GPUs generate data in parallel, while model updates take time. When a new model is deployed, much data generated by “older versions” of the model remains unused—discarding it seems wasteful, yet using it raises concerns about whether “stale data” might harm training effectiveness.</p>

<p>This is the core problem faced by <strong>off-policy</strong> training: <strong>Can we guarantee continued performance improvement when using data collected by older policies to update newer policies?</strong></p>

<p>This article systematically addresses this question. Starting from foundational theory, we progressively derive actionable conditions that specify when mixing data from multiple policy versions can still guarantee monotonic training improvement.</p>

<h2 id="part-i-theoretical-foundations">Part I: Theoretical Foundations</h2>

<h3 id="11-basic-setup">1.1 Basic Setup</h3>

<p>We consider a standard Markov Decision Process (MDP) comprising a state space ⟦INLMATH29MATHEND⟧, action space ⟦INLMATH30MATHEND⟧, transition probability ⟦INLMATH31MATHEND⟧, reward function ⟦INLMATH32MATHEND⟧, initial distribution ⟦INLMATH33MATHEND⟧, and discount factor ⟦INLMATH34MATHEND⟧.</p>

<p>The <strong>expected cumulative discounted return</strong> of policy ⟦INLMATH35MATHEND⟧ is:</p>

<p>⟦DISPMATH9MATHEND⟧</p>

<h4 id="discounted-state-visitation-distribution">Discounted State Visitation Distribution</h4>

<p>Represents the weighted frequency of visiting each state during long-term policy execution:</p>

<p>⟦DISPMATH10MATHEND⟧</p>

<h4 id="advantage-function">Advantage Function</h4>

<p>Measures how much better action ⟦INLMATH36MATHEND⟧ is compared to the policy’s average:</p>

<p>⟦DISPMATH11MATHEND⟧</p>

<h4 id="total-variation-distance-tv-distance">Total Variation Distance (TV Distance)</h4>

<p>Measures the difference between two policies’ action distributions at state ⟦INLMATH37MATHEND⟧:</p>

<p>⟦DISPMATH12MATHEND⟧</p>

<p>Throughout, we use ⟦INLMATH38MATHEND⟧ for conditional probability (e.g., ⟦INLMATH39MATHEND⟧) and reserve ⟦INLMATH40MATHEND⟧ for norms.</p>

<h3 id="12-core-tool-policy-performance-difference-lemma">1.2 Core Tool: Policy Performance Difference Lemma</h3>

<p>The cornerstone of the entire theory is this elegant result:</p>

<blockquote>
  <p><strong>Lemma 1.1 (Policy Performance Difference Lemma)</strong></p>

  <p>For any policies ⟦INLMATH41MATHEND⟧ (old) and ⟦INLMATH42MATHEND⟧ (new), the performance difference can be expressed as:</p>

  <p>⟦DISPMATH1MATHEND⟧</p>
</blockquote>

<p><strong>Intuitive understanding</strong>: How much better the new policy is than the old equals the “average advantage” obtained by selecting actions according to the new policy under the state distribution visited by the new policy.</p>

<h2 id="part-ii-performance-improvement-bounds-for-single-policy-sampling">Part II: Performance Improvement Bounds for Single-Policy Sampling</h2>

<h3 id="21-distribution-mismatch-and-controlling-state-shift">2.1 Distribution Mismatch and Controlling State Shift</h3>

<p>The Policy Performance Difference Lemma has a practical issue: the expectation on the right-hand side is computed under ⟦INLMATH43MATHEND⟧ (the new policy’s state distribution), while we can only sample from ⟦INLMATH44MATHEND⟧ (the old policy).</p>

<p>The solution is to decompose the expectation into “expectation under the old distribution + bias term,” then control the bias. The key question is: <strong>What is the quantitative relationship between the difference in state distributions and the difference in policies?</strong></p>

<h4 id="controlling-state-distribution-differences">Controlling State Distribution Differences</h4>

<blockquote>
  <p><strong>Lemma 1.2 (Relationship Between State Distribution Difference and Policy TV Distance)</strong></p>

  <p>⟦DISPMATH2MATHEND⟧</p>
</blockquote>

<h4 id="physical-interpretation">Physical Interpretation</h4>

<p>Small differences in policies in action space are “amplified” through environment dynamics into differences in state visitation distributions. The coefficient ⟦INLMATH45MATHEND⟧ reflects the <strong>temporal accumulation effect</strong>—in long-horizon tasks (⟦INLMATH46MATHEND⟧ close to 1), the amplification is stronger.</p>

<h4 id="proof-sketch">Proof Sketch</h4>

<p>By deriving the fixed-point equation for discounted visitation distributions and exploiting the ⟦INLMATH47MATHEND⟧ non-expansiveness of stochastic matrices, one can show that state distribution differences are amplified by policy differences through transition dynamics, with the amplification factor being precisely ⟦INLMATH48MATHEND⟧.</p>

<h3 id="22-policy-performance-improvement-lower-bound">2.2 Policy Performance Improvement Lower Bound</h3>

<blockquote>
  <p><strong>Theorem 1.1 (Policy Performance Improvement Lower Bound)</strong></p>

  <p>Define the expected advantage upper bound constant ⟦INLMATH49MATHEND⟧. Then:</p>

  <p>⟦DISPMATH3MATHEND⟧</p>

  <p>where the <strong>surrogate objective</strong> is:</p>

  <p>⟦DISPMATH4MATHEND⟧</p>
</blockquote>

<p>This lower bound consists of two parts:</p>

<ol>
  <li>
    <p><strong>Surrogate objective</strong> ⟦INLMATH50MATHEND⟧: Can be directly estimated from old policy data via importance sampling; this is the optimization objective of TRPO/PPO.</p>
  </li>
  <li>
    <p><strong>Policy shift penalty</strong>: Increases with the TV distance between new and old policies, explaining why PPO needs to constrain update magnitude.</p>
  </li>
</ol>

<p><strong>Core conclusion</strong>: Maximizing the surrogate objective while controlling policy shift guarantees performance improvement.</p>

<h2 id="part-iii-multi-policy-static-mixture-sampling">Part III: Multi-Policy Static Mixture Sampling</h2>

<h3 id="31-setup-and-unified-modeling-static-mixture">3.1 Setup and Unified Modeling (Static Mixture)</h3>

<p>In practice, a batch of data may come from multiple policy versions ⟦INLMATH51MATHEND⟧, with respective proportions ⟦INLMATH52MATHEND⟧. How do we extend Theorem 1.1 to this setting?</p>

<p><strong>Core idea: augmented state space</strong></p>

<p>The solution is an elegant modeling technique: <strong>treat the policy version index as part of the state</strong>.</p>

<p>Define the augmented state space ⟦INLMATH53MATHEND⟧, where ⟦INLMATH54MATHEND⟧ is the policy index set. Under augmented state ⟦INLMATH55MATHEND⟧, the <strong>mixture behavior policy</strong> is defined as ⟦INLMATH56MATHEND⟧.</p>

<p>The evolution of indices is characterized by the <strong>index transition kernel</strong> ⟦INLMATH57MATHEND⟧. The augmented MDP inherits the original MDP’s rewards and environment transitions, with indices evolving independently according to ⟦INLMATH58MATHEND⟧.</p>

<p>This technique works because the new policy ⟦INLMATH59MATHEND⟧’s return in the augmented MDP equals its return in the original MDP, allowing direct application of Theorem 1.1.</p>

<h3 id="32-trajectory-level-mixture-simplification-and-improvement-bound">3.2 Trajectory-Level Mixture: Simplification and Improvement Bound</h3>

<p>The most common scenario is <strong>using a single old policy per trajectory</strong>: at trajectory start, sample index ⟦INLMATH60MATHEND⟧, and use ⟦INLMATH61MATHEND⟧ throughout. In this case, the index transition kernel is the identity: ⟦INLMATH62MATHEND⟧.</p>

<p>From an engineering perspective, in many <strong>actor-learner asynchronous training</strong> setups (when sampling and training organize data by “entire trajectories/complete episodes belonging to a certain policy version”), this approximately corresponds to what we call <strong>trajectory-level mixture</strong>: actors use a fixed policy snapshot within a sampling unit to generate data, while learners mix trajectories from different versions for updates. We say “approximately” because different systems may not have identical boundaries for “trajectory/sampling unit.”</p>

<blockquote>
  <p><strong>Lemma 2.1 (Structural Simplification for Trajectory-Level Mixture)</strong></p>

  <p>(a) The augmented state visitation distribution decomposes as: ⟦INLMATH63MATHEND⟧</p>

  <p>(b) The advantage function reduces to: ⟦INLMATH64MATHEND⟧</p>
</blockquote>

<p><strong>Intuition for (b)</strong>: Since the index never changes, <strong>all future trajectories</strong> starting from augmented state ⟦INLMATH65MATHEND⟧ are generated by the same policy ⟦INLMATH66MATHEND⟧. Therefore, future cumulative returns are entirely determined by ⟦INLMATH67MATHEND⟧, and value functions and advantage functions naturally reduce to their ⟦INLMATH68MATHEND⟧ counterparts.</p>

<p>Consequently, the mixture policy’s return is the weighted average of individual old policies’ returns: ⟦INLMATH69MATHEND⟧.</p>

<p><strong>Improvement bound</strong></p>

<blockquote>
  <p><strong>Corollary 2.1 (Performance Improvement Lower Bound for Trajectory-Level Mixture)</strong></p>

  <p>⟦DISPMATH5MATHEND⟧</p>
</blockquote>

<p>This result shows that when mixing trajectories from multiple old policy versions for training, if we construct the loss using importance ratios corresponding to each trajectory’s source policy while controlling the new policy’s deviation from each old policy, the new policy’s performance has a clear improvement lower bound.</p>

<h2 id="part-iv-dynamic-mixture-sampling-and-monotonic-improvement-conditions">Part IV: Dynamic Mixture Sampling and Monotonic Improvement Conditions</h2>

<h3 id="41-problem-and-unified-modeling-dynamic-mixture">4.1 Problem and Unified Modeling (Dynamic Mixture)</h3>

<p>Part III discussed <strong>static mixture</strong>—where mixture weights ⟦INLMATH70MATHEND⟧ remain fixed. This section considers the more general <strong>dynamic mixture</strong>—where sampling gradually transitions to the new policy after it is released.</p>

<p>The previous results characterize improvement of “the new policy relative to the mixture behavior policy.” However, in actual training, what we truly care about is: <strong>Does the latest policy ⟦INLMATH71MATHEND⟧ after each update monotonically improve over the previous latest policy ⟦INLMATH72MATHEND⟧?</strong></p>

<p>⟦DISPMATH13MATHEND⟧</p>

<h4 id="unified-modeling-framework">Unified Modeling Framework</h4>

<p>Two typical forms of dynamic mixture sampling can be uniformly characterized by the index transition kernel ⟦INLMATH73MATHEND⟧:</p>

<p><strong>Trajectory-level mixture</strong> (can be viewed as an abstraction of conventional asynchronous training; identity index transition): ⟦INLMATH74MATHEND⟧</p>

<p><strong>Step/segment-level mixture</strong> (an abstraction of partial rollout / segment-based sampling; allows switching): ⟦INLMATH75MATHEND⟧</p>

<p>where ⟦INLMATH76MATHEND⟧ is the switching probability and ⟦INLMATH77MATHEND⟧ is the target index distribution.</p>

<h3 id="42-decomposition-and-monotonic-improvement-bound">4.2 Decomposition and Monotonic Improvement Bound</h3>

<p>By introducing the mixture return ⟦INLMATH78MATHEND⟧ as an intermediate bridge, the performance difference decomposes as:</p>

<p>⟦DISPMATH14MATHEND⟧</p>

<p>The first term can be handled using Theorem 1.1. The second term is the <strong>mixture bias term</strong>, which can be shown to satisfy:</p>

<p>⟦DISPMATH15MATHEND⟧</p>

<h4 id="monotonic-improvement-bound">Monotonic Improvement Bound</h4>

<p>Combining the above results yields the core theorem:</p>

<blockquote>
  <p><strong>Theorem 3.1 (Monotonic Improvement Lower Bound Under Dynamic Mixture Sampling)</strong></p>

  <p>⟦DISPMATH6MATHEND⟧</p>
</blockquote>

<p>Here ⟦INLMATH79MATHEND⟧ denotes the surrogate objective relative to the behavior policy ⟦INLMATH80MATHEND⟧ (the same form as ⟦INLMATH81MATHEND⟧ in Part II, but with the behavior policy generalized from a single ⟦INLMATH82MATHEND⟧ to the mixture ⟦INLMATH83MATHEND⟧).</p>

<p>More explicitly, one can write
⟦DISPMATH16MATHEND⟧</p>

<p>Similarly, define
⟦DISPMATH17MATHEND⟧</p>

<p>This lower bound reveals the necessity of <strong>dual control</strong>:</p>
<ul>
  <li><strong>Update shift penalty</strong>: Deviation of the new policy ⟦INLMATH84MATHEND⟧ from the sampling source policy ⟦INLMATH85MATHEND⟧</li>
  <li><strong>Sampling staleness penalty</strong>: Staleness of the sampling source policy ⟦INLMATH86MATHEND⟧ relative to the current policy ⟦INLMATH87MATHEND⟧</li>
</ul>

<h3 id="43-why-direct-constraints-are-infeasible-triangle-inequality-decomposition">4.3 Why Direct Constraints Are Infeasible: Triangle Inequality Decomposition</h3>

<p>The update shift penalty term in Theorem 3.1 might appear controllable by constraining ⟦INLMATH88MATHEND⟧, but this is actually <strong>infeasible</strong>:</p>

<blockquote>
  <p><strong>Observation 3.1 (Infeasibility of Update Shift Constraints)</strong></p>

  <p>Suppose the mixture sampling includes two old policies ⟦INLMATH89MATHEND⟧ and ⟦INLMATH90MATHEND⟧. If there exists some state ⟦INLMATH91MATHEND⟧ such that ⟦INLMATH92MATHEND⟧, then no policy ⟦INLMATH93MATHEND⟧ can simultaneously satisfy ⟦INLMATH94MATHEND⟧ and ⟦INLMATH95MATHEND⟧.</p>
</blockquote>

<h4 id="proof">Proof</h4>

<p>By the triangle inequality, if both constraints were satisfied, then ⟦INLMATH96MATHEND⟧, a contradiction.</p>

<h4 id="root-cause">Root Cause</h4>

<p>The update shift penalty directly couples ⟦INLMATH97MATHEND⟧ with the historical policy family ⟦INLMATH98MATHEND⟧, whose internal structure is a product of historical training and not controllable by the current update.</p>

<h4 id="triangle-inequality-decomposition">Triangle Inequality Decomposition</h4>

<p>The solution leverages the triangle inequality of TV distance:</p>

<p>⟦DISPMATH18MATHEND⟧</p>

<p>This decomposes the coupled constraint into two independent parts:</p>

<ul>
  <li><strong>Update increment shift</strong> ⟦INLMATH99MATHEND⟧: Deviation of the new policy from the current policy, <strong>controllable by the optimization side</strong></li>
  <li><strong>Sampling staleness</strong> ⟦INLMATH100MATHEND⟧: Deviation of the current policy from each old policy, <strong>must be controlled by the sampling side</strong></li>
</ul>

<p>Define:</p>

<p>⟦DISPMATH19MATHEND⟧</p>

<blockquote>
  <p><strong>Corollary 3.2 (Decomposed Monotonic Improvement Lower Bound)</strong></p>

  <p>⟦DISPMATH7MATHEND⟧</p>
</blockquote>

<h4 id="why-does-decomposition-solve-the-problem">Why Does Decomposition Solve the Problem?</h4>

<p>The key is that after decomposition, ⟦INLMATH101MATHEND⟧ only involves the new policy ⟦INLMATH102MATHEND⟧ and the current policy ⟦INLMATH103MATHEND⟧, <strong>completely independent of the structure of the old policy family ⟦INLMATH104MATHEND⟧</strong>. Therefore, regardless of how different the old policies are from each other, constraining ⟦INLMATH105MATHEND⟧ is always feasible—this is precisely the resolution to the infeasibility issue revealed in Observation 3.1.</p>

<p>This reveals an important engineering principle—<strong>separation of concerns</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Control Term</th>
      <th>Responsible Party</th>
      <th>Control Mechanism</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>⟦INLMATH106MATHEND⟧ (update increment shift)</td>
      <td>Optimization algorithm</td>
      <td>Policy clipping</td>
    </tr>
    <tr>
      <td>⟦INLMATH107MATHEND⟧ (sampling staleness)</td>
      <td>Sampling system</td>
      <td>Data filtering, version window</td>
    </tr>
  </tbody>
</table>

<h2 id="part-v-theoretical-foundations-of-clipping-mechanisms">Part V: Theoretical Foundations of Clipping Mechanisms</h2>

<h3 id="51-from-tv-distance-to-sample-controllable-quantities">5.1 From TV Distance to Sample-Controllable Quantities</h3>

<p>Corollary 3.2 tells us that to guarantee monotonic improvement, we need to control the update increment shift ⟦INLMATH108MATHEND⟧. However, TV distance is a distribution-level quantity—how can we control it using samples?</p>

<p>The key bridge is the following identity:</p>

<blockquote>
  <p><strong>Lemma 3.3 (Ratio Difference Representation of TV Distance)</strong></p>

  <p>Suppose policy ⟦INLMATH109MATHEND⟧’s support covers the supports of ⟦INLMATH110MATHEND⟧ and ⟦INLMATH111MATHEND⟧. Then for any state distribution ⟦INLMATH112MATHEND⟧:</p>

  <p>⟦DISPMATH8MATHEND⟧</p>
</blockquote>

<h4 id="intuitive-understanding">Intuitive Understanding</h4>

<p>The left side is the TV distance between two distributions (requiring enumeration over all actions), while the right side is the absolute difference of two importance ratios when sampling under ⟦INLMATH113MATHEND⟧. This enables us to estimate and control TV distance using samples.</p>

<h4 id="sample-representation-of-inlmath114mathend">Sample Representation of ⟦INLMATH114MATHEND⟧</h4>

<p>Using Lemma 3.3, setting ⟦INLMATH115MATHEND⟧, ⟦INLMATH116MATHEND⟧, ⟦INLMATH117MATHEND⟧ (the sampling source policy), we obtain:</p>

<p>⟦DISPMATH20MATHEND⟧</p>

<p>Denoting ⟦INLMATH118MATHEND⟧ and ⟦INLMATH119MATHEND⟧, we have:</p>

<p>⟦DISPMATH21MATHEND⟧</p>

<p>This means: <strong>If we can ensure ⟦INLMATH120MATHEND⟧ for each sample, we can guarantee ⟦INLMATH121MATHEND⟧</strong>.</p>

<h3 id="52-constraining-inlmath122mathend-two-clipping-options">5.2 Constraining ⟦INLMATH122MATHEND⟧: Two Clipping Options</h3>

<h4 id="method-1-direct-constraint-on-ratio-difference">Method 1: Direct Constraint on Ratio Difference</h4>

<p>For each sample ⟦INLMATH123MATHEND⟧, require:</p>

<p>⟦DISPMATH22MATHEND⟧</p>

<p>The clipping interval is ⟦INLMATH124MATHEND⟧, with <strong>clipping center at ⟦INLMATH125MATHEND⟧ rather than 1</strong>.</p>

<h4 id="method-2-constraint-on-incremental-ratio">Method 2: Constraint on Incremental Ratio</h4>

<p>Noting that ⟦INLMATH126MATHEND⟧, we have:</p>

<p>⟦DISPMATH23MATHEND⟧</p>

<p>If we constrain ⟦INLMATH127MATHEND⟧, since ⟦INLMATH128MATHEND⟧, one can show ⟦INLMATH129MATHEND⟧.</p>

<p>This method clips ⟦INLMATH130MATHEND⟧ with center at 1, meaning the <strong>clipping constraint itself does not depend on the old policy family ⟦INLMATH131MATHEND⟧</strong>. However, if we use the weighted advantage ⟦INLMATH132MATHEND⟧ below, we still need per-sample behavior probabilities (or recorded logprobs) to compute ⟦INLMATH133MATHEND⟧.</p>

<h4 id="objective-functions-three-clipping-mechanisms">Objective Functions (Three Clipping Mechanisms)</h4>

<p>For comparison, we present the complete objective functions for three clipping mechanisms. Suppose the current sample comes from old policy ⟦INLMATH134MATHEND⟧, and denote:</p>
<ul>
  <li>⟦INLMATH135MATHEND⟧ (new policy’s ratio relative to sampling policy)</li>
  <li>⟦INLMATH136MATHEND⟧ (current policy’s ratio relative to sampling policy)</li>
  <li>⟦INLMATH137MATHEND⟧ (new policy’s incremental ratio relative to current policy)</li>
</ul>

<p>Note: under <strong>trajectory-level mixture</strong> (index fixed), ⟦INLMATH138MATHEND⟧, so per-trajectory advantages from the corresponding old policy are consistent; under <strong>step/segment-level mixture</strong>, replacing ⟦INLMATH139MATHEND⟧ with ⟦INLMATH140MATHEND⟧ introduces advantage-substitution bias (discussed in Part VI), so the advantage/value estimator must reflect future index switching.</p>

<h4 id="standard-ppo">Standard PPO</h4>

<p>Clip ⟦INLMATH141MATHEND⟧ with center at 1</p>

<p>⟦DISPMATH24MATHEND⟧</p>

<h4 id="method-1">Method 1</h4>

<p>Clip ⟦INLMATH142MATHEND⟧ with center at ⟦INLMATH143MATHEND⟧</p>

<p>⟦DISPMATH25MATHEND⟧</p>

<h4 id="method-2">Method 2</h4>

<p>Clip incremental ratio ⟦INLMATH144MATHEND⟧ with center at 1</p>

<p>⟦DISPMATH26MATHEND⟧</p>

<p>where ⟦INLMATH145MATHEND⟧ is the importance-weighted advantage estimate.</p>

<h3 id="53-comparison-and-practical-controls">5.3 Comparison and Practical Controls</h3>

<h4 id="table-51-comparison-of-three-clipping-mechanisms">Table 5.1 Comparison of Three Clipping Mechanisms</h4>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Clipped Variable</th>
      <th>Clipping Center</th>
      <th>Clipping Interval</th>
      <th>Constrained TV Distance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Standard PPO</td>
      <td>⟦INLMATH146MATHEND⟧</td>
      <td>⟦INLMATH147MATHEND⟧</td>
      <td>⟦INLMATH148MATHEND⟧</td>
      <td>⟦INLMATH149MATHEND⟧</td>
    </tr>
    <tr>
      <td>Method 1</td>
      <td>⟦INLMATH150MATHEND⟧</td>
      <td>⟦INLMATH151MATHEND⟧</td>
      <td>⟦INLMATH152MATHEND⟧</td>
      <td>⟦INLMATH153MATHEND⟧</td>
    </tr>
    <tr>
      <td>Method 2</td>
      <td>⟦INLMATH154MATHEND⟧</td>
      <td>⟦INLMATH155MATHEND⟧</td>
      <td>⟦INLMATH156MATHEND⟧</td>
      <td>⟦INLMATH157MATHEND⟧</td>
    </tr>
  </tbody>
</table>

<h4 id="the-fundamental-problem-with-standard-ppo-under-multi-policy-mixture">The Fundamental Problem with Standard PPO Under Multi-Policy Mixture</h4>

<p>Standard PPO constrains ⟦INLMATH158MATHEND⟧, requiring the new policy to be simultaneously close to all sampling source policies. By Observation 3.1, when the old policies ⟦INLMATH159MATHEND⟧ differ significantly from each other, <strong>no ⟦INLMATH160MATHEND⟧ can simultaneously satisfy all constraints</strong>. This causes the trust region intersection to shrink or even become empty, with updates being limited by the most stale policy.</p>

<h4 id="common-advantages-of-methods-1-and-2">Common Advantages of Methods 1 and 2</h4>

<p>Both methods constrain ⟦INLMATH161MATHEND⟧—the deviation of the new policy from the <strong>current policy</strong> (rather than the sampling policy). Since ⟦INLMATH162MATHEND⟧ is uniquely determined, this constraint is consistent across all sample sources, completely avoiding the infeasibility problem.</p>

<h4 id="method-1-vs-method-2">Method 1 vs Method 2</h4>

<table>
  <thead>
    <tr>
      <th>Comparison Dimension</th>
      <th>Method 1 (Adaptive Clipping)</th>
      <th>Method 2 (Incremental Clipping)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Stale samples (⟦INLMATH163MATHEND⟧)</td>
      <td>Automatically tightens constraints, more conservative</td>
      <td>May produce large gradient variance</td>
    </tr>
    <tr>
      <td>LLM large vocabulary low-probability tokens</td>
      <td>Allows larger absolute changes (additive)</td>
      <td>Absolute changes are limited (multiplicative)</td>
    </tr>
    <tr>
      <td>Implementation complexity</td>
      <td>Requires storing ⟦INLMATH164MATHEND⟧ and ⟦INLMATH165MATHEND⟧</td>
      <td>Needs ⟦INLMATH166MATHEND⟧ and ⟦INLMATH167MATHEND⟧ (or stored logprobs) to compute ⟦INLMATH168MATHEND⟧; clipping itself uses only ⟦INLMATH169MATHEND⟧</td>
    </tr>
    <tr>
      <td>Advantage function</td>
      <td>Uses ⟦INLMATH170MATHEND⟧</td>
      <td>Uses weighted advantage ⟦INLMATH171MATHEND⟧</td>
    </tr>
  </tbody>
</table>

<h4 id="detailed-explanations">Detailed Explanations</h4>

<h4 id="1-handling-stale-samples">(1) Handling Stale Samples</h4>

<p>When samples come from very old policies, ⟦INLMATH172MATHEND⟧ can be large.</p>

<ul>
  <li>Method 2’s integrand is ⟦INLMATH173MATHEND⟧; even if ⟦INLMATH174MATHEND⟧, the integrand can reach ⟦INLMATH175MATHEND⟧, producing spikes.</li>
  <li>Method 1 directly constrains ⟦INLMATH176MATHEND⟧; the integrand’s upper bound is always ⟦INLMATH177MATHEND⟧, unaffected by ⟦INLMATH178MATHEND⟧ amplification.</li>
</ul>

<h4 id="2-llm-large-vocabulary-issue">(2) LLM Large Vocabulary Issue</h4>

<p>Large language models have many tokens having very small probabilities.</p>

<ul>
  <li>Method 2 constrains ⟦INLMATH179MATHEND⟧, which is a <strong>multiplicative constraint</strong>: if ⟦INLMATH180MATHEND⟧, the allowed absolute change is only ⟦INLMATH181MATHEND⟧.</li>
  <li>Method 1 constrains ⟦INLMATH182MATHEND⟧, which is an <strong>additive constraint</strong>: if that token has higher probability under the old policy (e.g., ⟦INLMATH183MATHEND⟧), even if the current probability is very low, faster improvement is allowed.</li>
</ul>

<h4 id="controlling-sampling-staleness">Controlling Sampling Staleness</h4>

<p>Corollary 3.2 shows that ⟦INLMATH184MATHEND⟧ also affects the monotonic improvement lower bound, but it <strong>cannot be controlled through optimization-side clipping</strong> and must be implemented by the sampling system:</p>

<h4 id="1-discarding-stale-data">(1) Discarding Stale Data</h4>

<p>Set a threshold ⟦INLMATH185MATHEND⟧. For each sample, compute ⟦INLMATH186MATHEND⟧, and discard samples exceeding the threshold.</p>

<h4 id="2-controlling-policy-version-window">(2) Controlling Policy Version Window</h4>

<p>Limit the number of old policy versions in the mixture sampling, e.g., using only data from the most recent ⟦INLMATH187MATHEND⟧ versions.</p>

<h4 id="operational-meaning-of-clipping">Operational Meaning of Clipping</h4>

<p>Finally, we clarify the relationship between clipping and the theoretical lower bound.</p>

<p>In Corollary 3.2, the coefficient of ⟦INLMATH188MATHEND⟧, namely ⟦INLMATH189MATHEND⟧, depends on the new policy ⟦INLMATH190MATHEND⟧, so the penalty term <strong>cannot be simply replaced by a constant</strong>. The correct operational meaning is:</p>

<blockquote>
  <p><strong>Maximize the surrogate objective ⟦INLMATH191MATHEND⟧ subject to the constraint ⟦INLMATH192MATHEND⟧</strong></p>
</blockquote>

<p>The clipping objective function is precisely an implementation of this constrained optimization—clipping <strong>hard limits</strong> the update magnitude to ensure ⟦INLMATH193MATHEND⟧ is controllable; under this premise, gradient ascent improves the surrogate objective, thereby providing guarantees for monotonic policy improvement.</p>

<h4 id="section-summary">Section Summary</h4>

<p>This section established the theoretical foundations of clipping mechanisms:</p>

<ol>
  <li><strong>Lemma 3.3</strong> converts TV distance to sample-level ratio differences, serving as the bridge between theory and implementation</li>
  <li><strong>Two constraint methods</strong>: Method 1 (adaptive clipping center) and Method 2 (fixed incremental clipping), both guaranteeing ⟦INLMATH194MATHEND⟧</li>
  <li><strong>Comparison with standard PPO</strong>: Standard PPO constrains ⟦INLMATH195MATHEND⟧, which is infeasible under multi-policy mixture; Methods 1/2 constrain ⟦INLMATH196MATHEND⟧, avoiding this issue</li>
  <li><strong>Method selection</strong>: Method 1 (adaptive) is recommended for high staleness or LLM large vocabulary scenarios; Method 2 (incremental) is attractive when you want the trust-region/clipping constraint to avoid depending on the old policy family (but data still needs to provide behavior logprobs to compute ⟦INLMATH197MATHEND⟧)</li>
  <li><strong>⟦INLMATH198MATHEND⟧ control</strong> is the sampling side’s responsibility, implemented through data filtering and version windows</li>
  <li><strong>Clipping is constrained optimization</strong>: Maximize the surrogate objective subject to ⟦INLMATH199MATHEND⟧ constraints</li>
</ol>

<h2 id="part-vi-comparison-of-trajectory-level-and-stepsegment-level-mixture">Part VI: Comparison of Trajectory-Level and Step/Segment-Level Mixture</h2>

<h3 id="61-mechanism-differences-and-estimation-implications">6.1 Mechanism Differences and Estimation Implications</h3>

<p>The essential difference between the two mixture mechanisms lies in the structure of the index transition kernel:</p>

<ul>
  <li><strong>Trajectory-level mixture</strong>: ⟦INLMATH200MATHEND⟧, index never changes</li>
  <li><strong>Step/segment-level mixture</strong>: ⟦INLMATH201MATHEND⟧, allows within-trajectory switching</li>
</ul>

<p>The correspondence with common engineering terminology is:</p>

<ul>
  <li><strong>Trajectory-level mixture</strong> here can be roughly understood as an idealized abstraction of “<strong>conventional asynchronous training</strong>”: data is organized by entire trajectories/episodes belonging to a certain policy version;</li>
  <li><strong>Step/segment-level mixture</strong> here can be roughly understood as an abstraction of “<strong>partial rollout</strong>”: due to asynchrony between actors and learners, and possible refresh to new policy versions at segment boundaries, using an index transition kernel that allows “within-trajectory version switching” can better approximate this phenomenon.</li>
</ul>

<p>The key watershed is <strong>whether Lemma 2.1’s structural simplification holds</strong>: trajectory-level mixture satisfies advantage function reduction; step/segment-level mixture generally does not, because future returns are affected by the index transition kernel.</p>

<h4 id="differences-in-sampling-staleness-inlmath202mathend">Differences in Sampling Staleness ⟦INLMATH202MATHEND⟧</h4>

<p><strong>Trajectory-level mixture</strong>’s staleness arises from: mixture weights ⟦INLMATH203MATHEND⟧ retaining probability mass on old policies after new policy release.</p>

<p><strong>Step/segment-level mixture</strong> has an <strong>exponential compression effect</strong>: Consider a simplified model with switching probability ⟦INLMATH204MATHEND⟧ from old to new. The marginal probability mass on old indices under the discounted visitation distribution is ⟦INLMATH205MATHEND⟧. As long as ⟦INLMATH206MATHEND⟧, the old policy weight can be significantly compressed.</p>

<h4 id="differences-in-surrogate-objective-estimation">Differences in Surrogate Objective Estimation</h4>

<p><strong>Trajectory-level mixture</strong>: The advantage function reduces to ⟦INLMATH207MATHEND⟧, with a clear estimation path.</p>

<p><strong>Advantage substitution bias in step/segment-level mixture</strong>: If single-policy advantage estimates are used, systematic bias will arise. The reason is that ⟦INLMATH208MATHEND⟧ requires taking expectations over future index switching, while ⟦INLMATH209MATHEND⟧ implicitly assumes “the future always follows ⟦INLMATH210MATHEND⟧.”</p>

<h4 id="unification-under-bandit-setting">Unification Under Bandit Setting</h4>

<p>In single-step episode LLM training, with no subsequent state transitions, the estimation problems of both mechanisms unify, with no such bias.</p>

<h3 id="62-risks-and-applicable-scenarios">6.2 Risks and Applicable Scenarios</h3>

<p>Step/segment-level mixture has another hidden concern: even if single-step importance ratios are clipped, multi-step noise accumulation over long trajectories can still amplify gradient estimation variance. When policy changes per update are large, “behavioral discontinuities” within trajectories may induce heavier-tailed ratio distributions. This is why trajectory-level mixture is recommended for “large policy change per update” scenarios in the table below.</p>

<h4 id="applicable-scenarios">Applicable Scenarios</h4>

<h4 id="table-61-applicable-scenarios-for-two-mixture-mechanisms">Table 6.1 Applicable Scenarios for Two Mixture Mechanisms</h4>

<table>
  <thead>
    <tr>
      <th>Scenario Characteristics</th>
      <th>Recommended Mechanism</th>
      <th>Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Long trajectories, high-frequency updates, strong asynchrony</td>
      <td>Step/segment-level</td>
      <td>Can significantly compress ⟦INLMATH211MATHEND⟧</td>
    </tr>
    <tr>
      <td>Short trajectories (non-bandit)</td>
      <td>Trajectory-level</td>
      <td>⟦INLMATH212MATHEND⟧ is naturally low</td>
    </tr>
    <tr>
      <td>Large policy change per update</td>
      <td>Trajectory-level</td>
      <td>Avoids variance amplification</td>
    </tr>
    <tr>
      <td>Single-step episode (bandit)</td>
      <td>Either</td>
      <td>Choose based on implementation convenience</td>
    </tr>
    <tr>
      <td>Need for compromise</td>
      <td>Segment-level</td>
      <td>Switch at natural boundaries</td>
    </tr>
  </tbody>
</table>

<p><strong>Core trade-off</strong>: Step/segment-level mixture is stronger on the sampling side (fast staleness removal), while trajectory-level mixture is more stable on the estimation side (easier surrogate objective estimation).</p>

<h2 id="part-vii-handling-training-inference-inconsistency">Part VII: Handling Training-Inference Inconsistency</h2>

<h3 id="71-background-and-effective-staleness">7.1 Background and Effective Staleness</h3>

<p>In large-scale distributed training, policies on the inference side and training side may be inconsistent:</p>

<ul>
  <li><strong>Numerical implementation differences</strong>: softmax normalization, quantization, kernel fusion</li>
  <li><strong>Decoding rule differences</strong>: temperature scaling, top-p/top-k sampling</li>
</ul>

<p>Let the behavior policy modeled on the training side be ⟦INLMATH213MATHEND⟧, while the policy actually sampling on the inference side is ⟦INLMATH214MATHEND⟧.</p>

<h4 id="effective-staleness">Effective Staleness</h4>

<p>Define <strong>effective staleness</strong>:</p>

<p>⟦DISPMATH27MATHEND⟧</p>

<p>This definition simultaneously covers version staleness and training-inference implementation differences.</p>

<h3 id="72-actionable-control">7.2 Actionable Control</h3>

<p>By Lemma 3.3, ⟦INLMATH215MATHEND⟧ can be expressed in sample-level computable form. Given threshold ⟦INLMATH216MATHEND⟧, if training only uses samples satisfying ⟦INLMATH217MATHEND⟧, then ⟦INLMATH218MATHEND⟧.</p>

<h4 id="key-implementation-points">Key Implementation Points</h4>

<ol>
  <li><strong>Behavior denominator alignment</strong>: The behavior probability in the loss should use the inference-side recorded ⟦INLMATH219MATHEND⟧</li>
  <li><strong>Probability smoothing</strong>: If the inference side has truncation (e.g., top-k), ensure ratios are valid</li>
</ol>

<h2 id="summary-practical-guidelines">Summary: Practical Guidelines</h2>

<h4 id="core-theoretical-framework">Core Theoretical Framework</h4>

<p>The structure of the monotonic improvement lower bound is:</p>

<p>⟦DISPMATH28MATHEND⟧</p>

<h4 id="separation-of-concerns-principle">Separation of Concerns Principle</h4>

<table>
  <thead>
    <tr>
      <th>Control Term</th>
      <th>Responsible Party</th>
      <th>Control Mechanism</th>
      <th>Specific Operation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>⟦INLMATH220MATHEND⟧</td>
      <td>Optimization algorithm</td>
      <td>Policy clipping</td>
      <td>Clip update increments (e.g., clip ⟦INLMATH221MATHEND⟧)</td>
    </tr>
    <tr>
      <td>⟦INLMATH222MATHEND⟧</td>
      <td>Sampling system</td>
      <td>Data filtering</td>
      <td>Discard stale samples</td>
    </tr>
    <tr>
      <td>⟦INLMATH223MATHEND⟧</td>
      <td>Sampling system</td>
      <td>Version window</td>
      <td>Use only most recent ⟦INLMATH224MATHEND⟧ versions</td>
    </tr>
  </tbody>
</table>

<h4 id="clipping-method-selection">Clipping Method Selection</h4>

<table>
  <thead>
    <tr>
      <th>Scenario</th>
      <th>Recommended Method</th>
      <th>Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>High staleness</td>
      <td>Method 1 (adaptive)</td>
      <td>Automatically tightens constraints for stale samples</td>
    </tr>
    <tr>
      <td>Implementation simplicity prioritized</td>
      <td>Method 2 (incremental)</td>
      <td>No need to store old policy information</td>
    </tr>
    <tr>
      <td>LLM large vocabulary</td>
      <td>Method 1</td>
      <td>Avoids slow updates for low-probability tokens</td>
    </tr>
  </tbody>
</table>

<h4 id="handling-training-inference-inconsistency">Handling Training-Inference Inconsistency</h4>

<ul>
  <li>Use inference-side recorded ⟦INLMATH225MATHEND⟧ as the behavior denominator</li>
  <li>Compress effective staleness through sample filtering</li>
</ul>

<h2 id="appendix-quick-reference-for-key-symbols">Appendix: Quick Reference for Key Symbols</h2>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>⟦INLMATH226MATHEND⟧, ⟦INLMATH227MATHEND⟧</td>
      <td>Latest policy at round ⟦INLMATH228MATHEND⟧, ⟦INLMATH229MATHEND⟧-th old policy</td>
    </tr>
    <tr>
      <td>⟦INLMATH230MATHEND⟧, ⟦INLMATH231MATHEND⟧</td>
      <td>Discounted state visitation distribution, advantage function</td>
    </tr>
    <tr>
      <td>⟦INLMATH232MATHEND⟧</td>
      <td>TV distance between two policies at state ⟦INLMATH233MATHEND⟧</td>
    </tr>
    <tr>
      <td>⟦INLMATH234MATHEND⟧</td>
      <td>Mixture behavior policy at round ⟦INLMATH235MATHEND⟧</td>
    </tr>
    <tr>
      <td>⟦INLMATH236MATHEND⟧, ⟦INLMATH237MATHEND⟧</td>
      <td>Index transition kernel, initial index distribution</td>
    </tr>
    <tr>
      <td>⟦INLMATH238MATHEND⟧, ⟦INLMATH239MATHEND⟧</td>
      <td>Update increment shift, sampling staleness</td>
    </tr>
    <tr>
      <td>⟦INLMATH240MATHEND⟧, ⟦INLMATH241MATHEND⟧, ⟦INLMATH242MATHEND⟧</td>
      <td>Clipping radius, staleness threshold, version window</td>
    </tr>
    <tr>
      <td>⟦INLMATH243MATHEND⟧</td>
      <td>Expected advantage upper bound constant</td>
    </tr>
  </tbody>
</table>

<h2 id="references">References</h2>

<ol>
  <li>
    <p>John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. “Trust Region Policy Optimization” (TRPO). arXiv:1502.05477. <a href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a></p>
  </li>
  <li>
    <p>Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel. “Constrained Policy Optimization” (CPO). arXiv:1705.10528. <a href="https://arxiv.org/abs/1705.10528">https://arxiv.org/abs/1705.10528</a></p>
  </li>
  <li>
    <p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. “Proximal Policy Optimization Algorithms” (PPO). arXiv:1707.06347. <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
  </li>
  <li>
    <p>James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras. “Generalized Proximal Policy Optimization with Sample Reuse” (GePPO). arXiv:2111.00072. <a href="https://arxiv.org/abs/2111.00072">https://arxiv.org/abs/2111.00072</a></p>
  </li>
  <li>
    <p>Yuzhen Zhou, Jiajun Li, Yusheng Su, et al. “APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation” (APRIL; partial rollout). arXiv:2509.18521. <a href="https://arxiv.org/abs/2509.18521">https://arxiv.org/abs/2509.18521</a></p>
  </li>
  <li>
    <p>Jacob Hilton, Karl Cobbe, John Schulman. “Batch size-invariance for policy optimization” (Decoupled PPO). arXiv:2110.00641. <a href="https://arxiv.org/abs/2110.00641">https://arxiv.org/abs/2110.00641</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025OffPolicyLLMRL</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Off-Policy Training in LLM Reinforcement Learning: From Theory to Practice}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html}</span><span class="p">,</span>
	<span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-12-17}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[A systematic derivation of off-policy training theory for LLM reinforcement learning: starting from single-policy sampling performance improvement bounds, extending to multi-policy static/dynamic mixture sampling, establishing sufficient conditions for monotonic improvement, decomposing constraints via the triangle inequality into update increment shift (controllable by optimization) and sampling staleness (controllable by sampling), and ultimately translating these into actionable clipping mechanisms and data filtering strategies.]]></summary></entry><entry xml:lang="zh"><title type="html">驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh.html" rel="alternate" type="text/html" title="驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证" /><published>2025-12-17T00:00:00+00:00</published><updated>2025-12-17T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh.html"><![CDATA[<h2 id="引言为什么我们需要关心异策略">引言：为什么我们需要关心”异策略”？</h2>

<p>设想这样一个场景：你正在使用强化学习训练一个大语言模型，希望它能够更好地回答问题。理想情况下，每当模型生成一批回答后，你都会立即用这些数据来更新模型，接着用更新后的模型生成新的数据，如此循环往复。这种”谁的数据就用来更新谁”的方式称为<strong>同策略</strong>（on-policy）训练。</p>

<p>然而，现实情况往往更为复杂。在大规模分布式训练中，数百个GPU并行生成数据，而模型更新需要时间。当新模型发布时，大量由旧版本模型生成的数据尚未被使用——直接丢弃过于浪费，但继续使用又担心数据”过时”会影响训练效果。</p>

<p>这便是<strong>异策略</strong>（off-policy）训练所面临的核心问题：<strong>用旧策略采集的数据来更新新策略，能否保证性能持续提升？</strong></p>

<p>本文将系统地回答这个问题。我们从基础理论出发，逐步推导出可操作的条件，阐明在何种情况下，混合使用多个版本策略的数据仍然能够保证训练的单调改进。</p>

<h2 id="第一部分理论基础">第一部分：理论基础</h2>

<h3 id="11-基本设定">1.1 基本设定</h3>

<p>我们考虑一个标准的马尔可夫决策过程（MDP），包含状态空间 ⟦INLMATH29MATHEND⟧、动作空间 ⟦INLMATH30MATHEND⟧、转移概率 ⟦INLMATH31MATHEND⟧、奖励函数 ⟦INLMATH32MATHEND⟧、初始状态分布 ⟦INLMATH33MATHEND⟧ 和折扣因子 ⟦INLMATH34MATHEND⟧。</p>

<p>策略 ⟦INLMATH35MATHEND⟧ 的<strong>期望累计折扣回报</strong>为：</p>

<p>⟦DISPMATH9MATHEND⟧</p>

<h4 id="折扣状态访问分布">折扣状态访问分布</h4>

<p>定义为策略长期运行中访问各状态的加权频率：</p>

<p>⟦DISPMATH10MATHEND⟧</p>

<h4 id="优势函数">优势函数</h4>

<p>用于衡量在状态 ⟦INLMATH36MATHEND⟧ 下选择动作 ⟦INLMATH37MATHEND⟧ 相对于策略平均水平的优劣：</p>

<p>⟦DISPMATH11MATHEND⟧</p>

<h4 id="全变差距离tv-距离">全变差距离（TV 距离）</h4>

<p>用于衡量两个策略在给定状态 ⟦INLMATH38MATHEND⟧ 下动作分布的差异：</p>

<p>⟦DISPMATH12MATHEND⟧</p>

<p>本文统一用 ⟦INLMATH39MATHEND⟧ 表示条件概率（例如 ⟦INLMATH40MATHEND⟧），并保留 ⟦INLMATH41MATHEND⟧ 表示范数。</p>

<h3 id="12-核心工具策略性能差异引理">1.2 核心工具：策略性能差异引理</h3>

<p>整个理论的基石是如下简洁的结论：</p>

<blockquote>
  <p><strong>引理1.1（策略性能差异引理）</strong></p>

  <p>对于任意旧策略 ⟦INLMATH42MATHEND⟧ 和新策略 ⟦INLMATH43MATHEND⟧，性能差异可表示为：</p>

  <p>⟦DISPMATH1MATHEND⟧</p>
</blockquote>

<p><strong>直观理解</strong>：新策略相较于旧策略的改进程度，等于在新策略访问的状态分布下，采用新策略选择动作所能获得的“平均优势”。</p>

<h2 id="第二部分单策略采样的性能改进下界">第二部分：单策略采样的性能改进下界</h2>

<h3 id="21-分布不匹配与分布差异控制">2.1 分布不匹配与分布差异控制</h3>

<p>策略性能差异引理存在一个实际问题：右侧的期望是在新策略的状态分布 ⟦INLMATH44MATHEND⟧ 下计算的，而我们只能从旧策略的分布 ⟦INLMATH45MATHEND⟧ 中采样。</p>

<p>解决思路是：将期望分解为“旧分布下的期望”与“偏差项”两部分，然后对偏差项加以控制。关键问题在于：<strong>状态分布的差异与策略的差异之间存在怎样的定量关系？</strong></p>

<h4 id="状态分布差异的控制">状态分布差异的控制</h4>

<blockquote>
  <p><strong>引理1.2（状态分布差异与策略TV距离的关系）</strong></p>

  <p>⟦DISPMATH2MATHEND⟧</p>
</blockquote>

<h4 id="物理意义">物理意义</h4>

<p>策略在动作空间上的微小差异，会通过环境动力学被“放大”为状态访问分布的差异。系数 ⟦INLMATH46MATHEND⟧ 反映了<strong>时间累积效应</strong>——在长时域任务中（⟦INLMATH47MATHEND⟧ 接近1），放大效应更为显著。</p>

<h4 id="证明思路">证明思路</h4>

<p>通过推导折扣访问分布的不动点方程，并利用随机矩阵的 ⟦INLMATH48MATHEND⟧ 非扩张性，可以证明状态分布差异被策略差异通过转移动力学放大，且放大系数正是 ⟦INLMATH49MATHEND⟧。</p>

<h3 id="22-策略性能改进下界">2.2 策略性能改进下界</h3>

<blockquote>
  <p><strong>定理1.1（策略性能改进下界）</strong></p>

  <p>定义期望优势上界常数 ⟦INLMATH50MATHEND⟧，则：</p>

  <p>⟦DISPMATH3MATHEND⟧</p>

  <p>其中<strong>代理目标</strong>为：</p>

  <p>⟦DISPMATH4MATHEND⟧</p>
</blockquote>

<p>该下界由两部分组成：</p>

<ol>
  <li>
    <p><strong>代理目标</strong> ⟦INLMATH51MATHEND⟧：可通过旧策略数据利用重要性采样直接估计，它是TRPO/PPO等算法的优化目标。</p>
  </li>
  <li>
    <p><strong>策略偏移惩罚</strong>：随着新旧策略的TV距离增大而增加，这解释了为何PPO等算法需要限制更新幅度。</p>
  </li>
</ol>

<p><strong>核心结论</strong>：在最大化代理目标的同时控制策略偏移，即可保证性能的改进。</p>

<h2 id="第三部分多策略静态混合采样">第三部分：多策略静态混合采样</h2>

<h3 id="31-问题设定与统一建模静态混合">3.1 问题设定与统一建模（静态混合）</h3>

<p>在实际训练中，一个批次的数据可能来自多个策略版本 ⟦INLMATH52MATHEND⟧，各版本占比为 ⟦INLMATH53MATHEND⟧。如何将定理1.1推广到这种情形？</p>

<p><strong>核心思想：扩展状态空间</strong></p>

<p>解决方案采用了一个优雅的建模技巧：<strong>将策略版本索引作为状态的一部分</strong>。</p>

<p>定义扩展状态空间 ⟦INLMATH54MATHEND⟧，其中 ⟦INLMATH55MATHEND⟧ 是策略索引集合。在扩展状态 ⟦INLMATH56MATHEND⟧ 下，<strong>混合行为策略</strong>定义为 ⟦INLMATH57MATHEND⟧。</p>

<p>索引的演化由<strong>索引转移核</strong> ⟦INLMATH58MATHEND⟧ 刻画。扩展MDP继承原始MDP的奖励和环境转移，索引按 ⟦INLMATH59MATHEND⟧ 独立演化。</p>

<p>这个技巧之所以有效，是因为新策略 ⟦INLMATH60MATHEND⟧ 在扩展MDP上的回报与在原始MDP中的回报相同，从而可以直接应用定理1.1。</p>

<h3 id="32-轨迹级混合结构简化与改进下界">3.2 轨迹级混合：结构简化与改进下界</h3>

<p>最常见的情形是<strong>每条轨迹仅使用一个旧策略</strong>：在轨迹开始时采样索引 ⟦INLMATH61MATHEND⟧，随后整条轨迹都使用策略 ⟦INLMATH62MATHEND⟧。此时索引转移核为恒等转移：⟦INLMATH63MATHEND⟧。</p>

<p>从工程实现的角度看，在许多 <strong>actor-learner 的异步训练</strong>架构中（如果采样端与训练端将数据按“整条轨迹/完整 episode 归属于某个策略版本”的方式组织），这可以近似对应这里的<strong>轨迹级混合</strong>：actor 在一个采样单元内固定使用某个策略快照生成数据，learner 再混合使用来自不同版本的整条轨迹数据进行更新。这里使用“近似”一词，是因为不同系统对“轨迹/采样单元”的切分边界可能不完全一致。</p>

<blockquote>
  <p><strong>引理2.1（轨迹级混合的结构简化）</strong></p>

  <p>(a) 扩展状态访问分布分解为：⟦INLMATH64MATHEND⟧</p>

  <p>(b) 优势函数还原为：⟦INLMATH65MATHEND⟧</p>
</blockquote>

<p><strong>(b) 的直观理解</strong>：由于索引永不改变，从扩展状态 ⟦INLMATH66MATHEND⟧ 出发的<strong>所有未来轨迹</strong>都由同一个策略 ⟦INLMATH67MATHEND⟧ 生成。因此，未来的累计回报完全由 ⟦INLMATH68MATHEND⟧ 决定，价值函数和优势函数自然还原为 ⟦INLMATH69MATHEND⟧ 的对应量。</p>

<p>因此，混合策略的回报等于各旧策略回报的加权平均：⟦INLMATH70MATHEND⟧。</p>

<p><strong>改进下界</strong></p>

<blockquote>
  <p><strong>推论2.1（轨迹级混合的性能改进下界）</strong></p>

  <p>⟦DISPMATH5MATHEND⟧</p>
</blockquote>

<p>该结论表明：当混合使用多个旧策略版本的轨迹进行训练时，若对每条轨迹使用对应旧策略的重要性比率来构造损失，并同时控制新策略与各旧策略之间的偏移，则新策略的性能将有明确的改进下界。</p>

<h2 id="第四部分动态混合采样与单调提升条件">第四部分：动态混合采样与单调提升条件</h2>

<h3 id="41-问题与统一建模动态混合">4.1 问题与统一建模（动态混合）</h3>

<p>第三部分讨论的是<strong>静态混合</strong>——混合权重 ⟦INLMATH71MATHEND⟧ 固定不变。本节考虑更一般的<strong>动态混合</strong>——即新策略发布后，采样逐步由新策略接管的过程。</p>

<p>前面的结论刻画了“新策略相对于混合行为策略”的改进。但在实际训练中，我们真正关心的是：<strong>每轮更新后的最新策略 ⟦INLMATH72MATHEND⟧ 相对于上一轮最新策略 ⟦INLMATH73MATHEND⟧ 是否具有单调提升性？</strong></p>

<p>⟦DISPMATH13MATHEND⟧</p>

<h4 id="统一建模框架">统一建模框架</h4>

<p>动态混合采样的两种典型形式都可以用索引转移核 ⟦INLMATH74MATHEND⟧ 统一刻画：</p>

<p><strong>轨迹级混合</strong>（可类比为常规异步训练的一个抽象；索引恒等转移）：⟦INLMATH75MATHEND⟧</p>

<p><strong>步/段级混合</strong>（partial rollout / 段式采样的一个抽象；允许切换）：⟦INLMATH76MATHEND⟧</p>

<p>其中 ⟦INLMATH77MATHEND⟧ 为切换概率，⟦INLMATH78MATHEND⟧ 为目标索引分布。</p>

<h3 id="42-分解与单调提升下界">4.2 分解与单调提升下界</h3>

<p>通过引入混合回报 ⟦INLMATH79MATHEND⟧ 作为中间桥梁，性能差异可分解为：</p>

<p>⟦DISPMATH14MATHEND⟧</p>

<p>第一项可用定理1.1处理。第二项是<strong>混合偏差项</strong>，可以证明它满足以下不等式：</p>

<p>⟦DISPMATH15MATHEND⟧</p>

<h4 id="单调提升下界">单调提升下界</h4>

<p>合并上述结果，我们得到核心定理：</p>

<blockquote>
  <p><strong>定理3.1（动态混合采样下的单调提升下界）</strong></p>

  <p>⟦DISPMATH6MATHEND⟧</p>
</blockquote>

<p>其中 ⟦INLMATH80MATHEND⟧ 表示“相对行为策略 ⟦INLMATH81MATHEND⟧ 的代理目标”（与第二部分的 ⟦INLMATH82MATHEND⟧ 同形，只是把行为策略从单一 ⟦INLMATH83MATHEND⟧ 推广到混合 ⟦INLMATH84MATHEND⟧）。</p>

<p>更具体地，可写为
⟦DISPMATH16MATHEND⟧</p>

<p>类似地，记
⟦DISPMATH17MATHEND⟧</p>

<p>该下界揭示了<strong>双重控制</strong>的必要性：</p>
<ul>
  <li><strong>更新偏移惩罚</strong>：新策略 ⟦INLMATH85MATHEND⟧ 相对于采样来源策略 ⟦INLMATH86MATHEND⟧ 的偏移</li>
  <li><strong>采样陈旧性惩罚</strong>：采样来源策略 ⟦INLMATH87MATHEND⟧ 相对于当前策略 ⟦INLMATH88MATHEND⟧ 的陈旧性</li>
</ul>

<h3 id="43-直接约束为何不可行三角不等式分解">4.3 直接约束为何不可行：三角不等式分解</h3>

<p>定理3.1中的更新偏移惩罚项看似可以通过约束 ⟦INLMATH89MATHEND⟧ 来控制，但这实际上<strong>不可行</strong>：</p>

<blockquote>
  <p><strong>观察3.1（更新偏移约束的不可行性）</strong></p>

  <p>假设混合采样包含两个旧策略 ⟦INLMATH90MATHEND⟧ 和 ⟦INLMATH91MATHEND⟧，若存在某个状态 ⟦INLMATH92MATHEND⟧ 使得 ⟦INLMATH93MATHEND⟧，则不存在任何策略 ⟦INLMATH94MATHEND⟧ 能够同时满足 ⟦INLMATH95MATHEND⟧ 与 ⟦INLMATH96MATHEND⟧。</p>
</blockquote>

<h4 id="证明">证明</h4>

<p>由三角不等式，若同时满足两个约束，则 ⟦INLMATH97MATHEND⟧，矛盾。</p>

<h4 id="问题根源">问题根源</h4>

<p>更新偏移惩罚项将 ⟦INLMATH98MATHEND⟧ 与历史策略族 ⟦INLMATH99MATHEND⟧ 直接耦合，而后者的内部结构是历史训练的产物，不受当前更新控制。</p>

<h4 id="三角不等式分解">三角不等式分解</h4>

<p>解决方案是利用TV距离的三角不等式：</p>

<p>⟦DISPMATH18MATHEND⟧</p>

<p>这将耦合约束拆分为两个独立部分：</p>

<ul>
  <li><strong>更新增量偏移</strong> ⟦INLMATH100MATHEND⟧：新策略相对于当前策略的偏离，<strong>可由优化侧控制</strong></li>
  <li><strong>采样陈旧性</strong> ⟦INLMATH101MATHEND⟧：当前策略相对于各旧策略的偏离，<strong>需由采样侧控制</strong></li>
</ul>

<p>定义：</p>

<p>⟦DISPMATH19MATHEND⟧</p>

<blockquote>
  <p><strong>推论3.2（分解后的单调提升下界）</strong></p>

  <p>⟦DISPMATH7MATHEND⟧</p>
</blockquote>

<h4 id="为何分解能解决问题">为何分解能解决问题？</h4>

<p>关键在于：分解后的 ⟦INLMATH102MATHEND⟧ 只涉及新策略 ⟦INLMATH103MATHEND⟧ 和当前策略 ⟦INLMATH104MATHEND⟧，<strong>与旧策略族 ⟦INLMATH105MATHEND⟧ 的结构完全无关</strong>。因此，无论旧策略之间差异多大，约束 ⟦INLMATH106MATHEND⟧ 都是可行的——这正是观察3.1所揭示的不可行性问题的解决之道。</p>

<p>这揭示了一个重要的工程原则——<strong>职责分离</strong>：</p>

<table>
  <thead>
    <tr>
      <th>控制项</th>
      <th>负责方</th>
      <th>控制手段</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>⟦INLMATH107MATHEND⟧（更新增量偏移）</td>
      <td>优化算法</td>
      <td>策略裁剪</td>
    </tr>
    <tr>
      <td>⟦INLMATH108MATHEND⟧（采样陈旧性）</td>
      <td>采样系统</td>
      <td>数据过滤、版本窗口</td>
    </tr>
  </tbody>
</table>

<h2 id="第五部分裁剪机制的理论基础">第五部分：裁剪机制的理论基础</h2>

<h3 id="51-从-tv-距离到样本可控量">5.1 从 TV 距离到样本可控量</h3>

<p>推论3.2告诉我们，要保证单调提升，需要控制更新增量偏移 ⟦INLMATH109MATHEND⟧。但TV距离是分布层面的量，如何用样本来控制它？</p>

<p>关键桥梁是下面这个恒等式：</p>

<blockquote>
  <p><strong>引理3.3（TV距离的比值差表示）</strong></p>

  <p>设策略 ⟦INLMATH110MATHEND⟧ 的支撑覆盖 ⟦INLMATH111MATHEND⟧ 和 ⟦INLMATH112MATHEND⟧ 的支撑，则对任意状态分布 ⟦INLMATH113MATHEND⟧：</p>

  <p>⟦DISPMATH8MATHEND⟧</p>
</blockquote>

<h4 id="直观理解">直观理解</h4>

<p>左侧是两个分布之间的TV距离（需要遍历所有动作），右侧是在 ⟦INLMATH114MATHEND⟧ 下采样时两个重要性比值的差的绝对值。这使我们能够通过样本来估计和控制TV距离。</p>

<h4 id="inlmath115mathend-的样本表示">⟦INLMATH115MATHEND⟧ 的样本表示</h4>

<p>利用引理3.3，取 ⟦INLMATH116MATHEND⟧，⟦INLMATH117MATHEND⟧，⟦INLMATH118MATHEND⟧（采样来源策略），可得：</p>

<p>⟦DISPMATH20MATHEND⟧</p>

<p>记 ⟦INLMATH119MATHEND⟧ 和 ⟦INLMATH120MATHEND⟧，则：</p>

<p>⟦DISPMATH21MATHEND⟧</p>

<p>这意味着：<strong>如果我们能使每个样本满足 ⟦INLMATH121MATHEND⟧，就能保证 ⟦INLMATH122MATHEND⟧</strong>。</p>

<h3 id="52-约束-inlmath123mathend两种裁剪方式">5.2 约束 ⟦INLMATH123MATHEND⟧：两种裁剪方式</h3>

<h4 id="方法一直接约束比值差">方法一：直接约束比值差</h4>

<p>对每个样本 ⟦INLMATH124MATHEND⟧，要求满足：</p>

<p>⟦DISPMATH22MATHEND⟧</p>

<p>即裁剪区间为 ⟦INLMATH125MATHEND⟧，<strong>裁剪中心是 ⟦INLMATH126MATHEND⟧ 而非 1</strong>。</p>

<h4 id="方法二约束增量比值">方法二：约束增量比值</h4>

<p>注意到 ⟦INLMATH127MATHEND⟧，因此有：</p>

<p>⟦DISPMATH23MATHEND⟧</p>

<p>如果约束 ⟦INLMATH128MATHEND⟧，由于 ⟦INLMATH129MATHEND⟧，可以证明 ⟦INLMATH130MATHEND⟧。</p>

<p>这种方法直接对 ⟦INLMATH131MATHEND⟧ 以 1 为中心进行裁剪，<strong>裁剪约束本身不依赖旧策略 ⟦INLMATH132MATHEND⟧</strong>。但如果采用后文的 ⟦INLMATH133MATHEND⟧，仍需要每条样本的行为概率 ⟦INLMATH134MATHEND⟧（或记录的 logprob）来计算 ⟦INLMATH135MATHEND⟧。下面我们给出三种裁剪机制的完整目标函数。设当前样本来自旧策略 ⟦INLMATH136MATHEND⟧，记：</p>
<ul>
  <li>⟦INLMATH137MATHEND⟧（新策略相对采样策略的比值）</li>
  <li>⟦INLMATH138MATHEND⟧（当前策略相对采样策略的比值）</li>
  <li>⟦INLMATH139MATHEND⟧（新策略相对当前策略的增量比值）</li>
</ul>

<p>说明：若采用<strong>轨迹级混合</strong>（索引不变），则 ⟦INLMATH140MATHEND⟧，可直接用每条轨迹对应旧策略的优势估计；若为<strong>步/段级混合</strong>，直接用 ⟦INLMATH141MATHEND⟧ 代替 ⟦INLMATH142MATHEND⟧ 会引入优势替代偏差（第六部分详述），需要使用能反映未来索引切换的优势/价值估计。</p>

<h4 id="标准-ppo">标准 PPO</h4>

<p>以 1 为中心裁剪 ⟦INLMATH143MATHEND⟧</p>

<p>⟦DISPMATH24MATHEND⟧</p>

<h4 id="方法一">方法一</h4>

<p>以 ⟦INLMATH144MATHEND⟧ 为中心裁剪 ⟦INLMATH145MATHEND⟧</p>

<p>⟦DISPMATH25MATHEND⟧</p>

<h4 id="方法二">方法二</h4>

<p>以 1 为中心裁剪增量比值 ⟦INLMATH146MATHEND⟧</p>

<p>⟦DISPMATH26MATHEND⟧</p>

<p>其中 ⟦INLMATH147MATHEND⟧ 是经过重要性加权的优势估计。</p>

<h3 id="53-对比与落地选型与采样侧控制">5.3 对比与落地：选型与采样侧控制</h3>

<h4 id="表-51三种裁剪机制的对比">表 5.1　三种裁剪机制的对比</h4>

<table>
  <thead>
    <tr>
      <th>方法</th>
      <th>裁剪变量</th>
      <th>裁剪中心</th>
      <th>裁剪区间</th>
      <th>约束的TV距离</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>标准PPO</td>
      <td>⟦INLMATH148MATHEND⟧</td>
      <td>⟦INLMATH149MATHEND⟧</td>
      <td>⟦INLMATH150MATHEND⟧</td>
      <td>⟦INLMATH151MATHEND⟧</td>
    </tr>
    <tr>
      <td>方法一</td>
      <td>⟦INLMATH152MATHEND⟧</td>
      <td>⟦INLMATH153MATHEND⟧</td>
      <td>⟦INLMATH154MATHEND⟧</td>
      <td>⟦INLMATH155MATHEND⟧</td>
    </tr>
    <tr>
      <td>方法二</td>
      <td>⟦INLMATH156MATHEND⟧</td>
      <td>⟦INLMATH157MATHEND⟧</td>
      <td>⟦INLMATH158MATHEND⟧</td>
      <td>⟦INLMATH159MATHEND⟧</td>
    </tr>
  </tbody>
</table>

<h4 id="标准-ppo-的根本问题多策略混合">标准 PPO 的根本问题（多策略混合）</h4>

<p>标准PPO约束 ⟦INLMATH160MATHEND⟧，要求新策略同时接近所有采样来源策略。根据观察3.1，当各旧策略 ⟦INLMATH161MATHEND⟧ 之间差异显著时，<strong>不存在能够同时满足所有约束的 ⟦INLMATH162MATHEND⟧</strong>。这导致信赖域交集收缩甚至为空，更新被最陈旧的策略所限制。</p>

<h4 id="方法一与方法二的共同优势">方法一与方法二的共同优势</h4>

<p>两者都约束 ⟦INLMATH163MATHEND⟧——新策略相对于<strong>当前策略</strong>（而非采样策略）的偏离。由于 ⟦INLMATH164MATHEND⟧ 是唯一确定的，这个约束对所有来源的样本一致，完全规避了不可行性问题。</p>

<h4 id="方法一-vs-方法二">方法一 vs 方法二</h4>

<table>
  <thead>
    <tr>
      <th>比较维度</th>
      <th>方法一（自适应裁剪）</th>
      <th>方法二（增量裁剪）</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>陈旧样本（⟦INLMATH165MATHEND⟧）</td>
      <td>自动收紧约束，更保守</td>
      <td>可能产生大梯度方差</td>
    </tr>
    <tr>
      <td>LLM大词表低概率token</td>
      <td>允许较大绝对变化（加法型）</td>
      <td>绝对变化受限（乘法型）</td>
    </tr>
    <tr>
      <td>实现复杂度</td>
      <td>需存储 ⟦INLMATH166MATHEND⟧ 和 ⟦INLMATH167MATHEND⟧</td>
      <td>需 ⟦INLMATH168MATHEND⟧ 与 ⟦INLMATH169MATHEND⟧（或存储的 logprob）以计算 ⟦INLMATH170MATHEND⟧；裁剪本身仅用 ⟦INLMATH171MATHEND⟧</td>
    </tr>
    <tr>
      <td>优势函数</td>
      <td>使用 ⟦INLMATH172MATHEND⟧</td>
      <td>使用加权优势 ⟦INLMATH173MATHEND⟧</td>
    </tr>
  </tbody>
</table>

<h4 id="详细解释">详细解释</h4>

<h4 id="一-陈旧样本处理">(一) 陈旧样本处理</h4>

<p>当样本来自很旧的策略时，⟦INLMATH174MATHEND⟧ 可能很大。</p>

<ul>
  <li>方法二的被积函数为 ⟦INLMATH175MATHEND⟧，即便 ⟦INLMATH176MATHEND⟧，被积函数仍可达 ⟦INLMATH177MATHEND⟧，产生尖峰。</li>
  <li>方法一直接约束 ⟦INLMATH178MATHEND⟧，被积函数上界恒为 ⟦INLMATH179MATHEND⟧，不受 ⟦INLMATH180MATHEND⟧ 放大。</li>
</ul>

<h4 id="二-llm-大词表问题">(二) LLM 大词表问题</h4>

<p>大语言模型词表规模巨大，大量token的概率极小。</p>

<ul>
  <li>方法二约束 ⟦INLMATH181MATHEND⟧，这是<strong>乘法型约束</strong>：若 ⟦INLMATH182MATHEND⟧，允许的绝对变化仅为 ⟦INLMATH183MATHEND⟧。</li>
  <li>方法一约束 ⟦INLMATH184MATHEND⟧，这是<strong>加法型约束</strong>：若该 token 在旧策略下概率较高（例如 ⟦INLMATH185MATHEND⟧），即便当前概率很低，也允许较快提升。</li>
</ul>

<h4 id="采样陈旧性的控制">采样陈旧性的控制</h4>

<p>推论3.2表明，⟦INLMATH186MATHEND⟧ 同样影响单调提升下界，但它<strong>无法通过优化侧的裁剪来控制</strong>，需要由采样系统实现：</p>

<h4 id="一-丢弃陈旧数据">(一) 丢弃陈旧数据</h4>

<p>设定阈值 ⟦INLMATH187MATHEND⟧，对每个样本计算 ⟦INLMATH188MATHEND⟧，丢弃超过该阈值的样本。</p>

<h4 id="二-控制策略版本窗口">(二) 控制策略版本窗口</h4>

<p>限制混合采样的旧策略版本数量，例如仅使用最近 ⟦INLMATH189MATHEND⟧ 个版本的数据。</p>

<h4 id="裁剪的操作含义">裁剪的操作含义</h4>

<p>最后，需要澄清裁剪与理论下界的关系。</p>

<p>推论3.2中，⟦INLMATH190MATHEND⟧ 的系数 ⟦INLMATH191MATHEND⟧ 依赖于新策略 ⟦INLMATH192MATHEND⟧，因此惩罚项<strong>不能简单地替换为常数</strong>。正确的操作含义是：</p>

<blockquote>
  <p><strong>在 ⟦INLMATH193MATHEND⟧ 的约束下，最大化代理目标 ⟦INLMATH194MATHEND⟧</strong></p>
</blockquote>

<p>裁剪目标函数正是这一约束优化的实现——通过裁剪<strong>硬性限制</strong>更新幅度，确保 ⟦INLMATH195MATHEND⟧ 可控；在此前提下，通过梯度上升提升代理目标，从而为策略的单调改进提供保障。</p>

<h4 id="本节小结">本节小结</h4>

<p>本节建立了裁剪机制的理论基础：</p>

<ol>
  <li><strong>引理3.3</strong>将TV距离转化为样本层面的比值差，是连接理论与实现的桥梁</li>
  <li><strong>两种约束方法</strong>：方法一（自适应裁剪中心）和方法二（固定增量裁剪），均保证 ⟦INLMATH196MATHEND⟧</li>
  <li><strong>与标准PPO对比</strong>：标准PPO约束 ⟦INLMATH197MATHEND⟧，在多策略混合下不可行；方法一/二约束 ⟦INLMATH198MATHEND⟧，规避了该问题</li>
  <li><strong>方法选择</strong>：陈旧性高或LLM大词表场景推荐方法一；若更关注“裁剪/信赖域不再依赖旧策略族”，可选方法二（但仍需数据侧提供行为 logprob 以计算 ⟦INLMATH199MATHEND⟧）</li>
  <li><strong>⟦INLMATH200MATHEND⟧ 的控制</strong>由采样侧负责，通过数据过滤和版本窗口实现</li>
  <li><strong>裁剪是约束优化</strong>：在 ⟦INLMATH201MATHEND⟧ 约束下最大化代理目标</li>
</ol>

<h2 id="第六部分轨迹级与步段级混合的比较">第六部分：轨迹级与步/段级混合的比较</h2>

<h3 id="61-机制差异与估计影响">6.1 机制差异与估计影响</h3>

<p>两类混合机制的本质区别在于索引转移核的结构：</p>

<ul>
  <li><strong>轨迹级混合</strong>：⟦INLMATH202MATHEND⟧，索引永不改变</li>
  <li><strong>步/段级混合</strong>：⟦INLMATH203MATHEND⟧，允许轨迹内切换</li>
</ul>

<p>与常见工程术语的对应关系如下：</p>

<ul>
  <li>这里的<strong>轨迹级混合</strong>可以大致理解为<strong>常规异步训练</strong>的一个理想化抽象：数据按整条轨迹/episode 归属于某个策略版本；</li>
  <li>这里的<strong>步/段级混合</strong>可以大致理解为<strong>partial rollout</strong>的一个抽象：由于 actor 与 learner 异步，且 segment 边界处可能刷新到新策略版本，使用索引转移核允许“轨迹内部版本切换”，可以更好地近似刻画这种现象。</li>
</ul>

<p>关键分水岭在于<strong>引理2.1的结构简化是否成立</strong>：轨迹级混合满足优势函数还原；步/段级混合一般不满足，因为未来回报受索引转移核影响。</p>

<h4 id="采样陈旧性-inlmath204mathend-的差异">采样陈旧性 ⟦INLMATH204MATHEND⟧ 的差异</h4>

<p><strong>轨迹级混合</strong>的陈旧性来源于：混合权重 ⟦INLMATH205MATHEND⟧ 在新策略发布后仍对旧策略保留一定的比例。</p>

<p><strong>步/段级混合</strong>具有<strong>指数压缩效应</strong>：考虑从旧到新以概率 ⟦INLMATH206MATHEND⟧ 切换的简化模型，折扣访问分布下旧索引的边缘质量为 ⟦INLMATH207MATHEND⟧。只要 ⟦INLMATH208MATHEND⟧，旧策略的权重即可被显著压缩。</p>

<h4 id="代理目标估计的差异">代理目标估计的差异</h4>

<p><strong>轨迹级混合</strong>：优势函数还原为 ⟦INLMATH209MATHEND⟧，估计路径清晰。</p>

<p><strong>步/段级混合的优势替代偏差</strong>：若沿用单策略优势估计，将产生系统性偏差。原因是 ⟦INLMATH210MATHEND⟧ 需要对未来索引切换取期望，而 ⟦INLMATH211MATHEND⟧ 隐含了“未来始终沿用 ⟦INLMATH212MATHEND⟧”的假设。</p>

<h4 id="bandit-设定下的统一">Bandit 设定下的统一</h4>

<p>在单步 episode 的 LLM 训练中，无后续状态转移，两类机制的估计问题统一，无上述偏差。</p>

<h3 id="62-风险与适用场景">6.2 风险与适用场景</h3>

<p>步/段级混合还有一个隐患：即便单步重要性比值被裁剪，长轨迹下多步噪声叠加仍会放大梯度估计方差。当每次更新的策略变化幅度较大时，轨迹内部的“行为突变”可能引发更重尾的比值分布。这也是表6.1中“策略变化幅度大”场景推荐轨迹级混合的原因。</p>

<h4 id="适用场景">适用场景</h4>

<h4 id="表-61两类混合机制的适用场景">表 6.1　两类混合机制的适用场景</h4>

<table>
  <thead>
    <tr>
      <th>场景特征</th>
      <th>推荐机制</th>
      <th>理由</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>长轨迹、高频更新、强异步</td>
      <td>步/段级</td>
      <td>可显著压缩 ⟦INLMATH213MATHEND⟧</td>
    </tr>
    <tr>
      <td>短轨迹（非Bandit）</td>
      <td>轨迹级</td>
      <td>⟦INLMATH214MATHEND⟧ 自然较低</td>
    </tr>
    <tr>
      <td>每次更新策略变化幅度大</td>
      <td>轨迹级</td>
      <td>避免方差放大</td>
    </tr>
    <tr>
      <td>单步episode（Bandit）</td>
      <td>均可</td>
      <td>按实现便利选择</td>
    </tr>
    <tr>
      <td>需要折中方案</td>
      <td>段级</td>
      <td>在自然边界切换</td>
    </tr>
  </tbody>
</table>

<p><strong>核心权衡</strong>：步/段级混合在采样侧更强（快速去陈旧），轨迹级混合在估计侧更稳（代理目标易于估计）。</p>

<h2 id="第七部分训推不一致的处理">第七部分：训推不一致的处理</h2>

<h3 id="71-背景与有效陈旧性">7.1 背景与有效陈旧性</h3>

<p>在大规模分布式训练中，推理端和训练端的策略可能存在不一致：</p>

<ul>
  <li><strong>数值实现差异</strong>：softmax归一化、量化、核融合等</li>
  <li><strong>解码规则差异</strong>：温度缩放、top-p/top-k采样等</li>
</ul>

<p>设训练侧建模的行为策略为 ⟦INLMATH215MATHEND⟧，而推理端实际采样的策略为 ⟦INLMATH216MATHEND⟧。</p>

<h4 id="有效陈旧性">有效陈旧性</h4>

<p>定义<strong>有效陈旧性</strong>：</p>

<p>⟦DISPMATH27MATHEND⟧</p>

<p>该定义同时覆盖了版本陈旧性与训推实现差异。</p>

<h3 id="72-可操作控制">7.2 可操作控制</h3>

<p>由引理3.3，⟦INLMATH217MATHEND⟧ 可表示为样本级可计算形式。给定阈值 ⟦INLMATH218MATHEND⟧，若训练仅使用满足 ⟦INLMATH219MATHEND⟧ 的样本，则 ⟦INLMATH220MATHEND⟧。</p>

<h4 id="关键实现要点">关键实现要点</h4>

<ol>
  <li><strong>行为分母对齐</strong>：损失中的行为概率应使用推理端记录的 ⟦INLMATH221MATHEND⟧</li>
  <li><strong>概率平滑</strong>：若推理端有截断（如top-k），需确保比值合法</li>
</ol>

<h2 id="总结实践指南">总结：实践指南</h2>

<h4 id="核心理论框架">核心理论框架</h4>

<p>单调提升下界的结构为：</p>

<p>⟦DISPMATH28MATHEND⟧</p>

<h4 id="职责分离原则">职责分离原则</h4>

<table>
  <thead>
    <tr>
      <th>控制项</th>
      <th>负责方</th>
      <th>控制手段</th>
      <th>具体操作</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>⟦INLMATH222MATHEND⟧</td>
      <td>优化算法</td>
      <td>策略裁剪</td>
      <td>对更新增量进行裁剪（例如对 ⟦INLMATH223MATHEND⟧ 裁剪）</td>
    </tr>
    <tr>
      <td>⟦INLMATH224MATHEND⟧</td>
      <td>采样系统</td>
      <td>数据过滤</td>
      <td>丢弃陈旧样本</td>
    </tr>
    <tr>
      <td>⟦INLMATH225MATHEND⟧</td>
      <td>采样系统</td>
      <td>版本窗口</td>
      <td>仅用最近 ⟦INLMATH226MATHEND⟧ 个版本</td>
    </tr>
  </tbody>
</table>

<h4 id="裁剪方法选择">裁剪方法选择</h4>

<table>
  <thead>
    <tr>
      <th>场景</th>
      <th>推荐方法</th>
      <th>理由</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>陈旧性较高</td>
      <td>方法一（自适应）</td>
      <td>自动对陈旧样本收紧约束</td>
    </tr>
    <tr>
      <td>实现简洁优先</td>
      <td>方法二（增量）</td>
      <td>无需存储旧策略信息</td>
    </tr>
    <tr>
      <td>LLM大词表</td>
      <td>方法一</td>
      <td>避免低概率token更新过慢</td>
    </tr>
  </tbody>
</table>

<h4 id="训推不一致的处理">训推不一致的处理</h4>

<ul>
  <li>使用推理端记录的 ⟦INLMATH227MATHEND⟧ 作为行为分母</li>
  <li>通过样本过滤压缩有效陈旧性</li>
</ul>

<h2 id="附录关键符号速查表">附录：关键符号速查表</h2>

<table>
  <thead>
    <tr>
      <th>符号</th>
      <th>含义</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>⟦INLMATH228MATHEND⟧, ⟦INLMATH229MATHEND⟧</td>
      <td>第 ⟦INLMATH230MATHEND⟧ 轮最新策略，第 ⟦INLMATH231MATHEND⟧ 个旧策略</td>
    </tr>
    <tr>
      <td>⟦INLMATH232MATHEND⟧, ⟦INLMATH233MATHEND⟧</td>
      <td>折扣状态访问分布，优势函数</td>
    </tr>
    <tr>
      <td>⟦INLMATH234MATHEND⟧</td>
      <td>两策略在状态 ⟦INLMATH235MATHEND⟧ 上的TV距离</td>
    </tr>
    <tr>
      <td>⟦INLMATH236MATHEND⟧</td>
      <td>第 ⟦INLMATH237MATHEND⟧ 轮混合行为策略</td>
    </tr>
    <tr>
      <td>⟦INLMATH238MATHEND⟧, ⟦INLMATH239MATHEND⟧</td>
      <td>索引转移核，索引初始分布</td>
    </tr>
    <tr>
      <td>⟦INLMATH240MATHEND⟧, ⟦INLMATH241MATHEND⟧</td>
      <td>更新增量偏移，采样陈旧性</td>
    </tr>
    <tr>
      <td>⟦INLMATH242MATHEND⟧, ⟦INLMATH243MATHEND⟧, ⟦INLMATH244MATHEND⟧</td>
      <td>裁剪半径，陈旧性阈值，版本窗口</td>
    </tr>
    <tr>
      <td>⟦INLMATH245MATHEND⟧</td>
      <td>期望优势上界常数</td>
    </tr>
  </tbody>
</table>

<h2 id="参考文献">参考文献</h2>

<ol>
  <li>
    <p>John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. “Trust Region Policy Optimization” (TRPO). arXiv:1502.05477. <a href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a></p>
  </li>
  <li>
    <p>Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel. “Constrained Policy Optimization” (CPO). arXiv:1705.10528. <a href="https://arxiv.org/abs/1705.10528">https://arxiv.org/abs/1705.10528</a></p>
  </li>
  <li>
    <p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. “Proximal Policy Optimization Algorithms” (PPO). arXiv:1707.06347. <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
  </li>
  <li>
    <p>James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras. “Generalized Proximal Policy Optimization with Sample Reuse” (GePPO). arXiv:2111.00072. <a href="https://arxiv.org/abs/2111.00072">https://arxiv.org/abs/2111.00072</a></p>
  </li>
  <li>
    <p>Yuzhen Zhou, Jiajun Li, Yusheng Su, et al. “APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation” (APRIL; partial rollout). arXiv:2509.18521. <a href="https://arxiv.org/abs/2509.18521">https://arxiv.org/abs/2509.18521</a></p>
  </li>
  <li>
    <p>Jacob Hilton, Karl Cobbe, John Schulman. “Batch size-invariance for policy optimization” (Decoupled PPO). arXiv:2110.00641. <a href="https://arxiv.org/abs/2110.00641">https://arxiv.org/abs/2110.00641</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025OffPolicyLLMRL</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Off-Policy Training in LLM Reinforcement Learning: From Theory to Practice}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html}</span><span class="p">,</span>
	<span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-12-17}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。]]></summary></entry><entry xml:lang="en"><title type="html">Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html" rel="alternate" type="text/html" title="Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" /><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html"><![CDATA[<p><img src="/assets/img/kl-estimators/kl-estimator.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>How we approximate KL divergence directly affects training stability. This post systematically analyzes three estimators ⟦INLMATH55MATHEND⟧ in both on-policy and off-policy scenarios, and gives practical guidelines for choosing them when KL is used as a reward penalty versus when it is used as a loss for backpropagation.</p>
</blockquote>

<h2 id="introduction-the-role-of-kl-divergence-in-reinforcement-learning">Introduction: The Role of KL Divergence in Reinforcement Learning</h2>

<p>In policy optimization (PPO, GRPO, etc.) and alignment training (RLHF/RLAIF), <strong>KL penalty</strong> is the core mechanism to constrain the new policy from deviating too far from the reference policy, preventing training instability or policy collapse. However, implementing KL penalty involves multiple layers of choices: <strong>which estimator</strong> (⟦INLMATH56MATHEND⟧, ⟦INLMATH57MATHEND⟧, ⟦INLMATH58MATHEND⟧), <strong>who to sample from</strong> (on-policy vs off-policy), and <strong>how to use it</strong> (as a loss for gradient backpropagation or as a reward penalty). This post systematically dissects these choices and their interrelationships, helping readers clarify the relevant concepts.</p>

<h3 id="the-distinction-between-forward-kl-and-reverse-kl">The Distinction Between Forward KL and Reverse KL</h3>

<p>Let ⟦INLMATH59MATHEND⟧ be the current actor, ⟦INLMATH60MATHEND⟧ the reference policy. The two directions are:</p>

<p><strong>Reverse KL:</strong>
⟦DISPMATH1MATHEND⟧</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:80%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Forward KL:</strong>
⟦DISPMATH2MATHEND⟧</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:80%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Intuition:</strong></p>
<ul>
  <li><strong>Reverse KL</strong> is mode-seeking: policy concentrates on high-probability regions of ⟦INLMATH61MATHEND⟧, possibly sacrificing diversity.</li>
  <li><strong>Forward KL</strong> is mass-covering: policy tries to cover the support of ⟦INLMATH62MATHEND⟧.</li>
</ul>

<p>RLHF typically uses <strong>reverse KL</strong> because we want the actor not to move too far from the reference, not necessarily to cover every mode.</p>

<h3 id="the-three-core-questions-who-to-sample-from-what-to-estimate-how-to-use">The Three Core Questions: Who to Sample From, What to Estimate, How to Use</h3>

<p>When implementing KL penalty in practice, we need to answer three interrelated questions:</p>

<ol>
  <li><strong>Who to sample from?</strong> Do samples come from the current policy ⟦INLMATH63MATHEND⟧ (on-policy), or from a behavior policy ⟦INLMATH64MATHEND⟧ (off-policy)?</li>
  <li><strong>What to estimate?</strong> Are we trying to estimate reverse KL ⟦INLMATH65MATHEND⟧ or forward KL ⟦INLMATH66MATHEND⟧?</li>
  <li><strong>How to use it?</strong> Is the KL term used as a loss for gradient backpropagation, or as a reward penalty (stop-gradient)?</li>
</ol>

<p>These three questions’ different combinations determine which estimator should be used. The goal of this post is to systematically clarify these choices and their interrelationships.</p>

<h2 id="preliminaries-notation-and-basic-concepts">Preliminaries: Notation and Basic Concepts</h2>

<p>Before diving into the analysis, let’s unify our notation and derive two fundamental results that will be used repeatedly.</p>

<h3 id="notation">Notation</h3>

<ul>
  <li>⟦INLMATH67MATHEND⟧: Current actor policy (parameterized by ⟦INLMATH68MATHEND⟧)</li>
  <li>⟦INLMATH69MATHEND⟧: Reference policy (independent of ⟦INLMATH70MATHEND⟧)</li>
  <li>⟦INLMATH71MATHEND⟧: Behavior policy for off-policy sampling (independent of ⟦INLMATH72MATHEND⟧)</li>
  <li>⟦INLMATH73MATHEND⟧: Score function</li>
  <li>⟦INLMATH74MATHEND⟧: Importance weight</li>
  <li>⟦INLMATH75MATHEND⟧: Stop-gradient operation (<code class="language-plaintext highlighter-rouge">.detach()</code> in code)</li>
</ul>

<h3 id="score-function-and-true-kl-gradients">Score Function and True KL Gradients</h3>

<p>The score function has an important property: ⟦INLMATH76MATHEND⟧ (since ⟦INLMATH77MATHEND⟧).</p>

<p>Using this property, we can derive the <strong>true gradients</strong> of forward and reverse KL divergences.</p>

<p><strong>Reverse KL Gradient:</strong></p>

<p>⟦DISPMATH3MATHEND⟧</p>

<p>Differentiating with respect to ⟦INLMATH78MATHEND⟧ (using product rule):</p>

<p>⟦DISPMATH4MATHEND⟧</p>

<p>Using ⟦INLMATH79MATHEND⟧, ⟦INLMATH80MATHEND⟧, and ⟦INLMATH81MATHEND⟧:</p>

<p>⟦DISPMATH5MATHEND⟧</p>

<p>Thus:</p>

<p>⟦DISPMATH6MATHEND⟧</p>

<p><strong>Forward KL Gradient:</strong></p>

<p>⟦DISPMATH7MATHEND⟧</p>

<p>Since ⟦INLMATH82MATHEND⟧ is independent of ⟦INLMATH83MATHEND⟧:</p>

<p>⟦DISPMATH8MATHEND⟧</p>

<p>To estimate this using samples from ⟦INLMATH84MATHEND⟧, apply importance sampling:</p>

<p>⟦DISPMATH9MATHEND⟧</p>

<p>Using ⟦INLMATH85MATHEND⟧, this can be rewritten as:</p>

<p>⟦DISPMATH10MATHEND⟧</p>

<p>With these two results, we can later determine which KL’s true gradient each estimator’s gradient expectation corresponds to.</p>

<h2 id="three-estimators-definitions-and-design-principles">Three Estimators: Definitions and Design Principles</h2>

<p>Let ⟦INLMATH86MATHEND⟧. John Schulman defined three single-sample estimators:</p>

<h3 id="inlmath87mathend-the-naive-estimator">⟦INLMATH87MATHEND⟧: the naive estimator</h3>

<p>⟦DISPMATH11MATHEND⟧</p>

<p>Direct log-ratio. It is unbiased for reverse KL, but <strong>can be negative</strong> while KL is always nonnegative, giving huge variance because positive and negative samples cancel.</p>

<h3 id="inlmath88mathend-an-f-divergence-lower-variance">⟦INLMATH88MATHEND⟧: an f-divergence, lower variance</h3>

<p>⟦DISPMATH12MATHEND⟧</p>

<p><strong>Motivation:</strong> ⟦INLMATH89MATHEND⟧ can be positive or negative; ⟦INLMATH90MATHEND⟧ squares it so <strong>every sample is positive</strong>, each telling you how far ⟦INLMATH91MATHEND⟧ and ⟦INLMATH92MATHEND⟧ differ.</p>

<p><strong>Why tiny bias?</strong> ⟦INLMATH93MATHEND⟧ is an <strong>f-divergence</strong> with ⟦INLMATH94MATHEND⟧. All smooth f-divergences have the same second-order expansion near ⟦INLMATH95MATHEND⟧:</p>

<p>⟦DISPMATH13MATHEND⟧</p>

<p>KL corresponds to ⟦INLMATH96MATHEND⟧, so ⟦INLMATH97MATHEND⟧. For ⟦INLMATH98MATHEND⟧, ⟦INLMATH99MATHEND⟧ as well. <strong>When policies are close, ⟦INLMATH100MATHEND⟧ tracks true KL almost identically</strong>, bias only appears in higher-order terms.</p>

<h3 id="inlmath101mathend-control-variate-optimal-shape">⟦INLMATH101MATHEND⟧: control variate, “optimal” shape</h3>

<p>⟦DISPMATH14MATHEND⟧</p>

<p><strong>Motivation:</strong> we want <strong>unbiased and low variance</strong>. Add a <strong>control variate</strong> to ⟦INLMATH102MATHEND⟧: something zero-mean and negatively correlated.</p>

<p>Because ⟦INLMATH103MATHEND⟧, for any ⟦INLMATH104MATHEND⟧:</p>

<p>⟦DISPMATH15MATHEND⟧</p>

<p>is still unbiased.</p>

<p><strong>Why ⟦INLMATH105MATHEND⟧?</strong> By concavity of ⟦INLMATH106MATHEND⟧, ⟦INLMATH107MATHEND⟧, so</p>

<p>⟦DISPMATH16MATHEND⟧</p>

<p>It is <strong>always nonnegative</strong>, avoiding the cancelation problem.</p>

<p><strong>Geometric view:</strong> ⟦INLMATH108MATHEND⟧ is a <strong>Bregman divergence</strong> for ⟦INLMATH109MATHEND⟧. Its tangent at ⟦INLMATH110MATHEND⟧ is ⟦INLMATH111MATHEND⟧, so</p>

<p>⟦DISPMATH17MATHEND⟧</p>

<p>Convexity keeps ⟦INLMATH112MATHEND⟧ above its tangent, so this gap is <strong>nonnegative</strong>. As ⟦INLMATH113MATHEND⟧, the gap shrinks quadratically ⟦INLMATH114MATHEND⟧, explaining the low variance when policies are close.</p>

<h3 id="quick-comparison">Quick Comparison</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Definition</th>
      <th style="text-align: center">Design idea</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH115MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH116MATHEND⟧</td>
      <td style="text-align: center">Naive log-ratio</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH117MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH118MATHEND⟧</td>
      <td style="text-align: center">f-divergence, KL-matching 2nd order</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH119MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH120MATHEND⟧</td>
      <td style="text-align: center">Control variate + Bregman</td>
    </tr>
  </tbody>
</table>

<p>These three estimators arise from different design philosophies. Next, we analyze their properties when <strong>estimating KL values</strong> — specifically, their bias and variance characteristics.</p>

<p>After understanding the definitions and design principles of the three estimators, we first analyze their properties in <strong>estimating KL values</strong> — that is, bias and variance.</p>

<h2 id="value-estimation-bias-and-variance">Value Estimation: Bias and Variance</h2>

<p>This section analyzes the properties of the three estimators when <strong>estimating KL values</strong>. These properties are fundamental in any usage scenario.</p>

<p>Assume samples from ⟦INLMATH121MATHEND⟧ to estimate reverse KL ⟦INLMATH122MATHEND⟧.</p>

<h3 id="unbiasedness-analysis">Unbiasedness Analysis</h3>

<p>⟦DISPMATH18MATHEND⟧</p>

<p><strong>Conclusion</strong>: For estimating reverse KL <strong>values</strong>, ⟦INLMATH123MATHEND⟧ and ⟦INLMATH124MATHEND⟧ are unbiased estimators, while ⟦INLMATH125MATHEND⟧ is biased.</p>

<h3 id="variance-characteristics">Variance Characteristics</h3>

<p>John Schulman’s toy experiments (⟦INLMATH126MATHEND⟧, ⟦INLMATH127MATHEND⟧, true KL = 0.005):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH128MATHEND⟧</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">20</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH129MATHEND⟧</td>
      <td style="text-align: center">0.002</td>
      <td style="text-align: center">1.42</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH130MATHEND⟧</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.42</td>
    </tr>
  </tbody>
</table>

<p>When KL is large (⟦INLMATH131MATHEND⟧, true KL = 0.5):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH132MATHEND⟧</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH133MATHEND⟧</td>
      <td style="text-align: center">0.25</td>
      <td style="text-align: center">1.73</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH134MATHEND⟧</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.7</td>
    </tr>
  </tbody>
</table>

<p><strong>Core intuitive understanding</strong>:</p>
<ul>
  <li>⟦INLMATH135MATHEND⟧ starts with a first-order term and when ⟦INLMATH136MATHEND⟧ is close to 1, it fluctuates greatly and can be negative</li>
  <li>⟦INLMATH137MATHEND⟧ is a second-order quantity near ⟦INLMATH138MATHEND⟧ and always non-negative, thus having lower variance when policies are close</li>
  <li>But when coverage is severely insufficient (⟦INLMATH139MATHEND⟧ can explode), ⟦INLMATH140MATHEND⟧’s variance can increase due to weight explosion; in this case, ⟦INLMATH141MATHEND⟧ is actually more stable</li>
</ul>

<h3 id="summary-of-value-estimation">Summary of Value Estimation</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Bias for value</th>
      <th style="text-align: center">Variance characteristics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH142MATHEND⟧</td>
      <td style="text-align: center">Unbiased</td>
      <td style="text-align: center">High (can be +/-)</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH143MATHEND⟧</td>
      <td style="text-align: center">Biased (minimal)</td>
      <td style="text-align: center">Low (always positive)</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH144MATHEND⟧</td>
      <td style="text-align: center">Unbiased</td>
      <td style="text-align: center">Low (always positive)</td>
    </tr>
  </tbody>
</table>

<p>From the perspective of value estimation, ⟦INLMATH145MATHEND⟧ is the optimal choice as “unbiased + low variance”.</p>

<blockquote>
  <p><strong>Note</strong>: To estimate the <strong>forward KL value</strong> ⟦INLMATH146MATHEND⟧, but only sample from ⟦INLMATH147MATHEND⟧, use importance sampling ⟦INLMATH148MATHEND⟧.</p>
</blockquote>

<p>However, before choosing an estimator, there’s a more fundamental question to answer: <strong>should KL be added to rewards, or be part of the loss?</strong> This choice fundamentally affects optimization behavior and credit assignment.</p>

<h2 id="two-ways-to-use-kl-penalty">Two Ways to Use KL Penalty</h2>

<p>Having understood the value properties of these estimators, we must address a more fundamental question: <strong>How should the KL penalty be integrated into the RL algorithm?</strong> This implementation choice determines whether we need only consider the estimator’s value properties, or must also account for its gradient behavior.</p>

<p>Recall the objective function for KL-regularized reinforcement learning:</p>

<p>⟦DISPMATH19MATHEND⟧</p>

<p>This mathematical form looks unified, but when implementing it in Actor-Critic based algorithms (like PPO), it gives rise to two fundamentally different implementation paradigms — they may differ by only a few lines of code, but correspond to completely different optimization semantics.</p>

<blockquote>
  <p><strong>Notation</strong>: In this section, we use ⟦INLMATH149MATHEND⟧ or ⟦INLMATH150MATHEND⟧ to generically refer to a token/state-level KL estimator (such as ⟦INLMATH151MATHEND⟧), with specific definitions from the earlier section “Three Estimators: Definitions and Design Principles”.</p>
</blockquote>

<h3 id="as-loss-kl-participates-in-gradient-backpropagation">As Loss: KL Participates in Gradient Backpropagation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantage</span> <span class="o">*</span> <span class="n">log_prob</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>  <span class="c1"># kl participates in gradient
</span></code></pre></div></div>

<p>The critic only learns environment value; KL as a regularization term for the actor directly participates in loss gradient backpropagation.</p>

<h3 id="as-reward-kl-added-to-reward-shaping">As Reward: KL Added to Reward Shaping</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kl</span> <span class="o">=</span> <span class="nf">compute_kl</span><span class="p">(</span><span class="n">log_prob_q</span><span class="p">,</span> <span class="n">log_prob_p</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">shaped_reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>
</code></pre></div></div>

<p>KL is treated as part of the environment reward, using shaped reward for standard actor-critic updates. The KL term does not participate in loss gradient backpropagation.</p>

<p>These two approaches may seem like just a <code class="language-plaintext highlighter-rouge">.detach()</code> difference in code, but in reality they correspond to fundamentally different optimization semantics.</p>

<h3 id="core-differences-between-the-two-approaches">Core Differences Between the Two Approaches</h3>

<h4 id="different-optimization-targets">Different Optimization Targets</h4>

<p><strong>KL as Loss</strong>: Optimizes <strong>original task + supervised regularization</strong>. KL doesn’t change the MDP definition, it’s just an external constraint term.</p>

<p><strong>KL as Reward</strong>: Optimizes a <strong>regularized new MDP</strong> where the reward function becomes ⟦INLMATH152MATHEND⟧.</p>

<p><strong>Intuition</strong>: The former is “adding constraints under the original rules”; the latter is “changing the game rules”.</p>

<h4 id="different-gradient-paths">Different Gradient Paths</h4>

<p><strong>KL as Loss</strong>: The gradient splits into two independent paths:</p>

<p>⟦DISPMATH20MATHEND⟧</p>

<p><strong>KL as Reward</strong>: Single policy gradient, KL influence is <strong>reflected indirectly through advantage</strong>:</p>

<p>⟦DISPMATH21MATHEND⟧</p>

<p><strong>Key distinction</strong>: Is KL’s force “a separate force” or “multiplied on advantage”? The former’s KL gradient is deterministic, unaffected by critic quality.</p>

<h4 id="different-value-functions-and-credit-assignment">Different Value Functions and Credit Assignment</h4>

<p><strong>Value Function</strong>:</p>

<p><strong>KL as Loss</strong>: Critic only learns environment value</p>

<p>⟦DISPMATH22MATHEND⟧</p>

<p>Cleaner separation of concerns, making it easier to monitor task return and KL divergence separately.</p>

<p><strong>KL as Reward</strong>: Critic learns mixed value</p>

<p>⟦DISPMATH23MATHEND⟧</p>

<p><strong>Credit Assignment</strong>:</p>

<p>Consider a scenario: first few steps are routing behavior, final step has high reward but also high KL.</p>

<p><strong>KL as Loss</strong>: The terminal state’s KL only appears in that state’s gradient term; the policy is still willing to <strong>visit high-reward regions but locally correct</strong> behavior.</p>

<p><strong>KL as Reward</strong>: The terminal state’s large KL is <strong>propagated back to all previous steps</strong> through TD, so the policy tends to <strong>fundamentally avoid</strong> high-KL regions — this is “planning-based KL budget allocation”.</p>

<h3 id="why-this-distinction-matters">Why This Distinction Matters</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Dimension</th>
      <th style="text-align: center">KL as Loss (gradient backprop)</th>
      <th style="text-align: center">KL as Reward (stop-grad)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Optimization target</td>
      <td style="text-align: center">Original task + supervised regularization</td>
      <td style="text-align: center">Regularized new MDP</td>
    </tr>
    <tr>
      <td style="text-align: center">Actor gradient</td>
      <td style="text-align: center">RL gradient + explicit KL gradient</td>
      <td style="text-align: center">Single PG, based on shaped advantage</td>
    </tr>
    <tr>
      <td style="text-align: center">Critic</td>
      <td style="text-align: center">Learns ⟦INLMATH153MATHEND⟧: only environment reward</td>
      <td style="text-align: center">Learns ⟦INLMATH154MATHEND⟧: reward + KL mixed</td>
    </tr>
    <tr>
      <td style="text-align: center">Credit Assignment</td>
      <td style="text-align: center">Local per-state, no planning</td>
      <td style="text-align: center">Multi-step backprop, planning-capable</td>
    </tr>
    <tr>
      <td style="text-align: center">Focus on</td>
      <td style="text-align: center">Estimator’s <strong>explicit gradient</strong> (corresponds to which optimization objective)</td>
      <td style="text-align: center">KL <strong>value</strong> + whether induced <strong>policy gradient</strong> is correct</td>
    </tr>
  </tbody>
</table>

<p><strong>One-liner</strong>: KL as loss makes the agent “visit but locally correct” — constraints are more local and flexible; KL as reward makes the agent “plan to avoid high-KL paths” — constraints are more global and thorough.</p>

<p><strong>Selection Guidelines</strong>:</p>
<ul>
  <li>If you want constraints to be “<strong>corrective</strong>”, allowing the agent to explore but locally correct behavior, choose <strong>KL as Loss</strong></li>
  <li>If you want constraints to be “<strong>preventive</strong>”, making the agent fundamentally avoid high-KL regions, choose <strong>KL as Reward</strong></li>
</ul>

<p>After understanding the difference between these two paradigms, we can clarify:</p>
<ul>
  <li><strong>KL as Loss</strong>: Needs correct explicit gradients of the KL estimator, caring about which optimization objective the gradient corresponds to</li>
  <li><strong>KL as Reward</strong>: Needs accurate value estimation of KL, while also caring about whether the induced policy gradient is correct</li>
</ul>

<p>Below we deeply analyze the gradient properties of estimators according to the two usage modes of “as Loss” and “as Reward”.</p>

<h2 id="gradient-analysis-when-used-as-loss">Gradient Analysis When Used as Loss</h2>

<p>When KL serves as a loss term participating in gradient backpropagation, we must understand the optimization objective each estimator corresponds to. This is the most subtle yet critical aspect in practical implementations.</p>

<h3 id="on-policy-scenario">On-policy Scenario</h3>

<p>We start the analysis from the on-policy scenario, i.e., samples come from the current policy ⟦INLMATH155MATHEND⟧.</p>

<h4 id="two-differentiation-orders-grad-then-expectation-vs-expectation-then-grad">Two Differentiation Orders: Grad-then-Expectation vs. Expectation-then-Grad</h4>

<p>In code implementation, there are two paths:</p>

<ol>
  <li><strong>Grad-then-expectation</strong>: Compute gradient for each sample’s ⟦INLMATH156MATHEND⟧, then take expectation of gradients (Monte Carlo estimation)</li>
  <li><strong>Expectation-then-grad</strong>: Treat ⟦INLMATH157MATHEND⟧ as a loss function, then differentiate the analytical expression</li>
</ol>

<p><strong>In typical deep learning code, we actually execute “grad-then-expectation”</strong> — autograd computes gradients for each sample, then averages over the batch.</p>

<h4 id="gradient-derivations-for-the-three-estimators">Gradient Derivations for the Three Estimators</h4>

<p>Now we compute the gradients of the three estimators to see which KL’s true gradient their expectations correspond to (refer to the “Preliminaries” section).</p>

<p><strong>Deriving ⟦INLMATH158MATHEND⟧</strong>:</p>

<p>⟦DISPMATH24MATHEND⟧</p>

<p>⟦DISPMATH25MATHEND⟧</p>

<p><strong>Deriving ⟦INLMATH159MATHEND⟧</strong>:</p>

<p>⟦DISPMATH26MATHEND⟧</p>

<p>By the chain rule:</p>

<p>⟦DISPMATH27MATHEND⟧</p>

<p><strong>Deriving ⟦INLMATH160MATHEND⟧</strong>:</p>

<p>⟦DISPMATH28MATHEND⟧</p>

<p>First, compute ⟦INLMATH161MATHEND⟧. Since ⟦INLMATH162MATHEND⟧:</p>

<p>⟦DISPMATH29MATHEND⟧</p>

<p>Then compute ⟦INLMATH163MATHEND⟧:</p>

<p>⟦DISPMATH30MATHEND⟧</p>

<p>Therefore:</p>

<p>⟦DISPMATH31MATHEND⟧</p>

<p>Taking expectations under ⟦INLMATH164MATHEND⟧:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">⟦INLMATH165MATHEND⟧</th>
      <th style="text-align: center">Equivalent to</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH166MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH167MATHEND⟧</td>
      <td style="text-align: center">Zero (useless as loss)</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH168MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH169MATHEND⟧</td>
      <td style="text-align: center">Reverse KL gradient</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH170MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH171MATHEND⟧</td>
      <td style="text-align: center">Forward KL gradient</td>
    </tr>
  </tbody>
</table>

<p><strong>Key insights</strong>:</p>
<ul>
  <li><strong>⟦INLMATH172MATHEND⟧’s gradient</strong> equals the true reverse KL gradient — the correct choice for optimizing “constraining policy not to deviate from reference”</li>
  <li><strong>⟦INLMATH173MATHEND⟧’s gradient</strong> equals the true forward KL gradient — corresponding to a “coverage” objective</li>
  <li><strong>⟦INLMATH174MATHEND⟧’s gradient expectation is always zero</strong> — backpropagating as loss is meaningless!</li>
</ul>

<h4 id="key-finding-inlmath175mathend-ineffective-inlmath176mathend-for-reverse-kl-inlmath177mathend-for-forward-kl">Key Finding: ⟦INLMATH175MATHEND⟧ Ineffective, ⟦INLMATH176MATHEND⟧ for Reverse KL, ⟦INLMATH177MATHEND⟧ for Forward KL</h4>

<p><strong>“Expectation-then-grad” vs. “Grad-then-expectation”</strong>:</p>

<p>If we analytically treat ⟦INLMATH178MATHEND⟧ as a function of ⟦INLMATH179MATHEND⟧ and then differentiate (i.e., “expectation-then-grad”), then:</p>

<p>⟦DISPMATH32MATHEND⟧</p>

<p>⟦DISPMATH33MATHEND⟧</p>

<p>Both give reverse KL’s gradient. However, when directly calling backpropagation on sample means of ⟦INLMATH180MATHEND⟧ in code, autograd executes “grad-then-expectation”, yielding ⟦INLMATH181MATHEND⟧, i.e., <strong>the forward KL gradient</strong>.</p>

<p><strong>Key conclusion for on-policy scenario</strong>: For the same estimator, the two differentiation orders can give completely different results. Specifically:</p>
<ul>
  <li>To optimize <strong>reverse KL</strong>: must use ⟦INLMATH182MATHEND⟧</li>
  <li>To optimize <strong>forward KL</strong>: use ⟦INLMATH183MATHEND⟧</li>
  <li>⟦INLMATH184MATHEND⟧’s gradient expectation is always zero, useless as loss</li>
</ul>

<h3 id="off-policy-scenario">Off-policy Scenario</h3>

<p>Now consider the off-policy scenario where samples come from a behavior policy ⟦INLMATH185MATHEND⟧. This situation is very common in practical RL training when using old/mixed policies to generate data or in offline RL with fixed sample distributions.</p>

<p>For an in-depth analysis of off-policy scenarios in large language models, refer to: <a href="/reinforcement-learning/2025/11/15/three-policy-en.html">From Two-Policy to Three-Policy: TRPO Extension Under Behavior-Reference Mismatch in LLM RL</a>.</p>

<h4 id="importance-weighting-and-gradient-equivalence">Importance Weighting and Gradient Equivalence</h4>

<p>When sampling from ⟦INLMATH186MATHEND⟧ independent of ⟦INLMATH187MATHEND⟧, we use importance weight ⟦INLMATH188MATHEND⟧ in the loss. A key difference emerges:</p>

<blockquote>
  <p><strong>Before</strong> (on-policy): expectation ⟦INLMATH189MATHEND⟧ depends on ⟦INLMATH190MATHEND⟧<br />
<strong>Now</strong> (off-policy): expectation ⟦INLMATH191MATHEND⟧ is independent of ⟦INLMATH192MATHEND⟧</p>
</blockquote>

<p>This makes “expectation-then-grad” and “grad-then-expectation” <strong>equivalent</strong>:</p>

<p>⟦DISPMATH34MATHEND⟧</p>

<h4 id="gradient-derivations-with-importance-weights">Gradient Derivations with Importance Weights</h4>

<p>Since ⟦INLMATH193MATHEND⟧ and using product rule with previously derived ⟦INLMATH194MATHEND⟧:</p>

<p>⟦DISPMATH35MATHEND⟧</p>

<p>Note that ⟦INLMATH195MATHEND⟧  — these two are <strong>gradient-identical</strong>.</p>

<p>Taking expectations and using ⟦INLMATH196MATHEND⟧ and ⟦INLMATH197MATHEND⟧:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Weighted estimator</th>
      <th style="text-align: center">Value target</th>
      <th style="text-align: center">Gradient target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH198MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH199MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH200MATHEND⟧ ✓ (higher var)</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH201MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH202MATHEND⟧ (f-div)</td>
      <td style="text-align: center">⟦INLMATH203MATHEND⟧ ✗ (not reverse KL)</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH204MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH205MATHEND⟧ (f-div)</td>
      <td style="text-align: center">⟦INLMATH206MATHEND⟧ ✓ (low var)</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH207MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH208MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH209MATHEND⟧ ✓ (low var)</td>
    </tr>
  </tbody>
</table>

<p><strong>Key finding</strong>: ⟦INLMATH210MATHEND⟧ and ⟦INLMATH211MATHEND⟧ have <strong>identical gradients</strong> ⟦INLMATH212MATHEND⟧ and are statistically equivalent.</p>

<h4 id="variance-analysis">Variance Analysis</h4>

<p>In the typical regime where ⟦INLMATH213MATHEND⟧, setting ⟦INLMATH214MATHEND⟧ with ⟦INLMATH215MATHEND⟧:</p>

<ul>
  <li>⟦INLMATH216MATHEND⟧ — contains ⟦INLMATH217MATHEND⟧ noise term ⟦INLMATH218MATHEND⟧</li>
  <li>⟦INLMATH219MATHEND⟧ — only ⟦INLMATH220MATHEND⟧ terms</li>
</ul>

<p>The variance difference is:</p>

<p>⟦DISPMATH36MATHEND⟧</p>

<p>Therefore, ⟦INLMATH221MATHEND⟧ has roughly one order of magnitude higher variance than ⟦INLMATH222MATHEND⟧ or ⟦INLMATH223MATHEND⟧.</p>

<p>This is why DeepSeek v3.2 uses ⟦INLMATH224MATHEND⟧ for off-policy KL penalty:</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/dpsk-3d2-k3.png" style="width:95%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Source: <a href="https://arxiv.org/pdf/2512.02556v1">DeepSeek v3.2 Technical Report Section 3.1</a></figcaption>
</figure>

<h4 id="practical-recommendations">Practical Recommendations</h4>

<p><strong>For on-policy reverse KL optimization:</strong></p>
<ul>
  <li><strong>Use ⟦INLMATH225MATHEND⟧ as loss</strong>: ⟦INLMATH226MATHEND⟧</li>
  <li>Gradient: ⟦INLMATH227MATHEND⟧</li>
</ul>

<p><strong>For on-policy forward KL optimization</strong> (coverage objectives):</p>
<ul>
  <li><strong>Use ⟦INLMATH228MATHEND⟧ as loss</strong>: Autograd gives ⟦INLMATH229MATHEND⟧</li>
</ul>

<p><strong>For off-policy reverse KL optimization</strong> (samples from ⟦INLMATH230MATHEND⟧):</p>
<ul>
  <li><strong>Recommended</strong>: ⟦INLMATH231MATHEND⟧ or ⟦INLMATH232MATHEND⟧ (identical, low variance)</li>
  <li><strong>Fallback</strong>: ⟦INLMATH233MATHEND⟧ (unbiased but ~10× higher variance)</li>
  <li><strong>Avoid</strong>: ⟦INLMATH234MATHEND⟧ with weight in gradient (biased for reverse KL)</li>
</ul>

<h3 id="comprehensive-gradient-analysis-summary">Comprehensive Gradient Analysis Summary</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Sampling</th>
      <th style="text-align: center">Loss</th>
      <th style="text-align: center">Gradient expectation</th>
      <th style="text-align: center">Optimizes</th>
      <th style="text-align: center">Usable for reverse KL?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH235MATHEND⟧ (on)</td>
      <td style="text-align: center">⟦INLMATH236MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH237MATHEND⟧</td>
      <td style="text-align: center">None (zero)</td>
      <td style="text-align: center">✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH238MATHEND⟧ (on)</td>
      <td style="text-align: center">⟦INLMATH239MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH240MATHEND⟧</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">✓</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH241MATHEND⟧ (on)</td>
      <td style="text-align: center">⟦INLMATH242MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH243MATHEND⟧</td>
      <td style="text-align: center">Forward KL</td>
      <td style="text-align: center">✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH244MATHEND⟧ (off)</td>
      <td style="text-align: center">⟦INLMATH245MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH246MATHEND⟧</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">✓ (high variance)</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH247MATHEND⟧ (off)</td>
      <td style="text-align: center">⟦INLMATH248MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH249MATHEND⟧</td>
      <td style="text-align: center">f-div (wrong)</td>
      <td style="text-align: center">✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH250MATHEND⟧ (off)</td>
      <td style="text-align: center">⟦INLMATH251MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH252MATHEND⟧</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">✓ (low variance)</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH253MATHEND⟧ (off)</td>
      <td style="text-align: center">⟦INLMATH254MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH255MATHEND⟧</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">✓ (low variance, recommended)</td>
    </tr>
  </tbody>
</table>

<h2 id="gradient-analysis-when-used-as-reward">Gradient Analysis When Used as Reward</h2>

<p>Having analyzed the value estimation properties of the three estimators in the previous section, one might naturally assume: since ⟦INLMATH256MATHEND⟧ and ⟦INLMATH257MATHEND⟧ are both unbiased for reverse KL value, incorporating them (with stop-gradient) as reward penalties should be equally valid.</p>

<p><strong>This intuition, however, is incorrect.</strong></p>

<p>The issue is: when KL is used as a reward penalty, although the KL term itself doesn’t backpropagate gradients, it will indirectly affect the policy gradient through advantage. Therefore, to evaluate whether an estimator “can be used for reward penalty”, we shouldn’t just look at value bias, but whether <strong>the policy gradient it induces is correct</strong>.</p>

<h3 id="the-true-kl-regularized-policy-gradient">The True KL-Regularized Policy Gradient</h3>

<p>Consider the KL-regularized reinforcement learning objective:</p>

<p>⟦DISPMATH37MATHEND⟧</p>

<p>Its true gradient is:</p>

<p>⟦DISPMATH38MATHEND⟧</p>

<p>Using the result from the “Preliminaries” section, the reverse KL gradient is:</p>

<p>⟦DISPMATH39MATHEND⟧</p>

<p>Therefore, the true KL-regularized policy gradient is:</p>

<p>⟦DISPMATH40MATHEND⟧</p>

<h3 id="gradient-form-when-using-estimator-inlmath258mathend">Gradient Form When Using Estimator ⟦INLMATH258MATHEND⟧</h3>

<p>When we use some estimator ⟦INLMATH259MATHEND⟧ (with stop-gradient) as a reward penalty, the shaped reward is ⟦INLMATH260MATHEND⟧, and the policy gradient becomes:</p>

<p>⟦DISPMATH41MATHEND⟧</p>

<p><strong>Unbiasedness condition</strong>: ⟦INLMATH261MATHEND⟧ if and only if</p>

<p>⟦DISPMATH42MATHEND⟧</p>

<h3 id="using-inlmath262mathend-as-penalty-gradient-unbiased">Using ⟦INLMATH262MATHEND⟧ as Penalty: Gradient Unbiased</h3>

<p>When ⟦INLMATH263MATHEND⟧, the condition is automatically satisfied:</p>

<p>⟦DISPMATH43MATHEND⟧</p>

<p>Therefore, <strong>when ⟦INLMATH264MATHEND⟧ is used as a reward penalty, the induced policy gradient is unbiased</strong>.</p>

<h3 id="using-inlmath265mathend-as-penalty-gradient-biased">Using ⟦INLMATH265MATHEND⟧ as Penalty: Gradient Biased</h3>

<p>When ⟦INLMATH266MATHEND⟧:</p>

<p>⟦DISPMATH44MATHEND⟧</p>

<p>The second term is exactly ⟦INLMATH267MATHEND⟧. The problem lies in the first term:</p>

<p>⟦DISPMATH45MATHEND⟧</p>

<p>This can be rewritten as:</p>

<p>⟦DISPMATH46MATHEND⟧</p>

<p>Using the forward KL gradient formula ⟦INLMATH268MATHEND⟧, we have:</p>

<p>⟦DISPMATH47MATHEND⟧</p>

<p>Therefore:</p>

<p>⟦DISPMATH48MATHEND⟧</p>

<p><strong>When ⟦INLMATH269MATHEND⟧ is used as a reward penalty, the gradient is biased</strong>, with the bias term equal to the negative of the forward KL gradient.</p>

<p><strong>Geometric meaning of the bias</strong>: Using ⟦INLMATH270MATHEND⟧ as a reward penalty is equivalent to optimizing a “wrong mixed objective”:</p>
<ul>
  <li>Penalizing reverse KL (hoping policy doesn’t deviate from reference)</li>
  <li>But also <strong>wrongly encouraging forward KL to increase</strong> (hoping reference doesn’t cover policy)</li>
</ul>

<p>These two directions conflict, potentially causing optimization instability.</p>

<p><strong>Experimental verification</strong>: Shah et al. (2025)’s experiments show that in on-policy RL fine-tuning of LLMs:</p>
<ul>
  <li><strong>⟦INLMATH271MATHEND⟧ in reward</strong>: Training is stable</li>
  <li><strong>⟦INLMATH272MATHEND⟧ in reward</strong>: <strong>Training collapses</strong></li>
</ul>

<p>This is completely consistent with our theoretical analysis.</p>

<h3 id="off-policy-scenario-1">Off-policy Scenario</h3>

<p>The above analysis assumes on-policy sampling. Does the conclusion change in off-policy scenarios?</p>

<p>Let samples come from behavior policy ⟦INLMATH273MATHEND⟧, using importance-weighted policy gradient:</p>

<p>⟦DISPMATH49MATHEND⟧</p>

<p>Using ⟦INLMATH274MATHEND⟧, this equals:</p>

<p>⟦DISPMATH50MATHEND⟧</p>

<p><strong>The unbiasedness condition</strong> remains ⟦INLMATH275MATHEND⟧, exactly the same as on-policy.</p>

<p><strong>Key insight</strong>: In the off-policy policy gradient framework, the importance weight ⟦INLMATH276MATHEND⟧ acts on the entire policy gradient estimator, <strong>no need to separately weight the KL estimator in shaped reward</strong>. Therefore:</p>

<ul>
  <li>Shaped reward keeps its original form: ⟦INLMATH277MATHEND⟧ (not ⟦INLMATH278MATHEND⟧)</li>
  <li>Conclusion same as on-policy: <strong>only use ⟦INLMATH279MATHEND⟧, not ⟦INLMATH280MATHEND⟧</strong></li>
</ul>

<h3 id="key-finding-only-inlmath281mathend-can-be-used-for-reward-penalty">Key Finding: Only ⟦INLMATH281MATHEND⟧ Can Be Used for Reward Penalty</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Value unbiased?</th>
      <th style="text-align: center">Gradient unbiased when used as reward penalty?</th>
      <th style="text-align: center">Actual performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH282MATHEND⟧</td>
      <td style="text-align: center">✓</td>
      <td style="text-align: center">✓</td>
      <td style="text-align: center">Stable</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH283MATHEND⟧</td>
      <td style="text-align: center">✓</td>
      <td style="text-align: center">✗</td>
      <td style="text-align: center">Collapses</td>
    </tr>
  </tbody>
</table>

<p><strong>Core lesson</strong>: When evaluating KL estimators, “value unbiasedness” and “gradient correctness” are two independent dimensions. For reward penalty scenarios (whether on-policy or off-policy), <strong>only ⟦INLMATH284MATHEND⟧ is the correct choice</strong>. Although ⟦INLMATH285MATHEND⟧ is value-unbiased and has lower variance, using it as a reward penalty causes biased gradients and may lead to training collapse.</p>

<h2 id="practical-guide-and-common-pitfalls">Practical Guide and Common Pitfalls</h2>

<p>With the preceding theoretical analysis, this section provides selection recommendations for specific scenarios, convenient for direct reference.</p>

<h3 id="quick-reference-table">Quick Reference Table</h3>

<p>The table below provides recommended estimator choices along three dimensions: “target KL direction” × “sampling source” × “usage mode”. “For <strong>Loss</strong>” corresponds to KL as loss (gradient backpropagation needed); “For <strong>Reward</strong>” corresponds to KL as reward penalty (stop-gradient).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Target</th>
      <th style="text-align: center">Sampling</th>
      <th style="text-align: center">For Loss (gradient backprop)</th>
      <th style="text-align: center">For Reward (stop-grad)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Reverse KL ⟦INLMATH286MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH287MATHEND⟧ (on-policy)</td>
      <td style="text-align: center">⟦INLMATH288MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH289MATHEND⟧</td>
    </tr>
    <tr>
      <td style="text-align: center">Reverse KL ⟦INLMATH290MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH291MATHEND⟧ (off-policy)</td>
      <td style="text-align: center">⟦INLMATH292MATHEND⟧ or ⟦INLMATH293MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH294MATHEND⟧</td>
    </tr>
    <tr>
      <td style="text-align: center">Forward KL ⟦INLMATH295MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH296MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH297MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH298MATHEND⟧</td>
    </tr>
  </tbody>
</table>

<h3 id="kl-as-loss-needs-gradient-backpropagation">KL as Loss (Needs Gradient Backpropagation)</h3>

<p>When KL is used as part of a loss for backpropagation, we must consider gradient correctness.</p>

<h4 id="on-policy-optimize-reverse-kl-most-common">On-policy: Optimize Reverse KL (Most Common)</h4>

<p>Goal: Control actor to not deviate from reference policy.</p>

<p><strong>Correct approach</strong>: Use <strong>⟦INLMATH299MATHEND⟧</strong> as loss.</p>

<p>⟦DISPMATH51MATHEND⟧</p>

<p>Its gradient expectation ⟦INLMATH300MATHEND⟧ is exactly the true gradient of reverse KL.</p>

<h4 id="on-policy-optimize-forward-kl-coverage-scenario">On-policy: Optimize Forward KL (Coverage Scenario)</h4>

<p>Goal: Make policy cover the support of the reference distribution (offline RL, imitation learning, etc.).</p>

<p><strong>Correct approach</strong>: Use <strong>⟦INLMATH301MATHEND⟧</strong> as loss.</p>

<p>⟦DISPMATH52MATHEND⟧</p>

<p>Directly calling loss gradient backprop on the sample mean of ⟦INLMATH302MATHEND⟧, autograd computes ⟦INLMATH303MATHEND⟧, which is the forward KL gradient, no additional processing needed.</p>

<h4 id="off-policy-optimize-reverse-kl">Off-policy: Optimize Reverse KL</h4>

<p>Goal: Data comes from behavior policy ⟦INLMATH304MATHEND⟧, still want to optimize reverse KL.</p>

<p><strong>Recommended approach</strong>: Use <strong>⟦INLMATH305MATHEND⟧</strong> or <strong>⟦INLMATH306MATHEND⟧</strong> as loss (both have identical gradients).</p>

<p>⟦DISPMATH53MATHEND⟧</p>

<p>or</p>

<p>⟦DISPMATH54MATHEND⟧</p>

<ul>
  <li>Gradients are unbiased</li>
  <li>When ⟦INLMATH307MATHEND⟧, both have much lower variance</li>
</ul>

<p><strong>Fallback</strong>: Use ⟦INLMATH308MATHEND⟧ (gradient also unbiased but higher variance)</p>

<p><strong>Avoid</strong>: Using ⟦INLMATH309MATHEND⟧ (with weight in gradient) — gradient is biased, not the correct direction for reverse KL</p>

<h3 id="kl-as-reward-penalty-stop-gradient">KL as Reward Penalty (Stop-gradient)</h3>

<p>When KL is used as a scalar penalty added to reward, although the KL term itself doesn’t backpropagate gradients, it will indirectly affect the policy gradient through advantage. Based on the earlier section “Gradient Analysis When Used as Reward”:</p>

<p><strong>Recommend</strong>:</p>
<ul>
  <li>Use <strong>⟦INLMATH310MATHEND⟧</strong> (value-unbiased and induced policy gradient is also unbiased)</li>
  <li>Conclusion is the same whether on-policy or off-policy</li>
</ul>

<p><strong>Avoid</strong>:</p>
<ul>
  <li>Using ⟦INLMATH311MATHEND⟧ (although value-unbiased and lower variance, the induced policy gradient is biased and may cause training collapse)</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: In off-policy policy gradient, the importance weight ⟦INLMATH312MATHEND⟧ acts on the entire ⟦INLMATH313MATHEND⟧; the shaped reward itself can keep the form ⟦INLMATH314MATHEND⟧.</p>
</blockquote>

<h3 id="common-pitfalls">Common Pitfalls</h3>

<h4 id="pitfall-1-using-inlmath315mathend-directly-as-loss-on-policy">Pitfall 1: Using ⟦INLMATH315MATHEND⟧ Directly as Loss (On-policy)</h4>

<p>The gradient expectation of ⟦INLMATH316MATHEND⟧ is always zero (⟦INLMATH317MATHEND⟧); using it as a loss is completely ineffective.</p>

<blockquote>
  <p><strong>Solution</strong>: Use ⟦INLMATH318MATHEND⟧ for on-policy reverse KL optimization; use ⟦INLMATH319MATHEND⟧ for forward KL optimization.</p>
</blockquote>

<h4 id="pitfall-2-confusing-inlmath320mathends-value-unbiasedness-with-gradient-behavior">Pitfall 2: Confusing ⟦INLMATH320MATHEND⟧’s Value Unbiasedness with Gradient Behavior</h4>

<p>⟦INLMATH321MATHEND⟧ is value-unbiased for <strong>reverse KL value</strong>, but its <strong>gradient</strong> corresponds to <strong>forward KL</strong> — these are completely different.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Scenario</th>
      <th style="text-align: center">Problem</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Using ⟦INLMATH322MATHEND⟧ as Loss (targeting reverse KL)</td>
      <td style="text-align: center">⟦INLMATH323MATHEND⟧ corresponds to forward KL, optimizing wrong direction</td>
    </tr>
    <tr>
      <td style="text-align: center">Using ⟦INLMATH324MATHEND⟧ as Reward penalty (targeting reverse KL)</td>
      <td style="text-align: center">Induces biased policy gradient (bias term ⟦INLMATH325MATHEND⟧), may cause training collapse</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Solution</strong>:</p>
  <ul>
    <li>Use as <strong>Loss</strong> to optimize reverse KL → use ⟦INLMATH326MATHEND⟧; use ⟦INLMATH327MATHEND⟧ only for forward KL</li>
    <li>Use as <strong>Reward</strong> penalty → only use ⟦INLMATH328MATHEND⟧ (whether on-policy or off-policy)</li>
  </ul>
</blockquote>

<h4 id="pitfall-3-off-policy-detach-handling-of-importance-weights">Pitfall 3: Off-policy Detach Handling of Importance Weights</h4>

<p>In off-policy scenarios, whether to detach the importance weight ⟦INLMATH329MATHEND⟧ leads to completely different results. The following table summarizes the correct detach strategies:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Detach ⟦INLMATH330MATHEND⟧?</th>
      <th style="text-align: center">Gradient corresponds to</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH331MATHEND⟧</td>
      <td style="text-align: center">No detach</td>
      <td style="text-align: center">Reverse KL ✓</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH332MATHEND⟧</td>
      <td style="text-align: center">No detach</td>
      <td style="text-align: center">Reverse KL ✓</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH333MATHEND⟧</td>
      <td style="text-align: center">No detach</td>
      <td style="text-align: center">f-divergence ✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH334MATHEND⟧</td>
      <td style="text-align: center">Detach</td>
      <td style="text-align: center">Reverse KL ✓</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Solution</strong>: For off-policy reverse KL optimization, recommend using ⟦INLMATH335MATHEND⟧ or ⟦INLMATH336MATHEND⟧ (both have identical gradients). If using ⟦INLMATH337MATHEND⟧, gradient is unbiased but variance is higher.</p>
</blockquote>

<h4 id="pitfall-4-using-inlmath338mathend-in-reward-penalty">Pitfall 4: Using ⟦INLMATH338MATHEND⟧ in Reward Penalty</h4>

<p>Although ⟦INLMATH339MATHEND⟧ is value-unbiased for reverse KL and has lower variance, using it as a reward penalty causes biased policy gradient (bias term ⟦INLMATH340MATHEND⟧), potentially leading to training collapse.</p>

<blockquote>
  <p><strong>Solution</strong>: Whether on-policy or off-policy, reward penalty should only use ⟦INLMATH341MATHEND⟧.</p>
</blockquote>

<h2 id="summary">Summary</h2>

<p>This post systematically analyzes the three KL estimators ⟦INLMATH342MATHEND⟧ around three core questions: <strong>who to sample from</strong>, <strong>whose value to estimate</strong>, <strong>whose gradient is needed</strong>.</p>

<p><strong>Core takeaways</strong>:</p>

<ol>
  <li><strong>First clarify usage mode</strong>: KL as Loss (gradient backprop) or as Reward (stop-grad)?</li>
  <li><strong>KL as Loss (on-policy)</strong>: Use ⟦INLMATH343MATHEND⟧ for reverse KL; use ⟦INLMATH344MATHEND⟧ for forward KL</li>
  <li><strong>KL as Loss (off-policy)</strong>: Use ⟦INLMATH345MATHEND⟧ or ⟦INLMATH346MATHEND⟧ (note detach strategy!)</li>
  <li><strong>KL as Reward</strong>: Only use ⟦INLMATH347MATHEND⟧ (⟦INLMATH348MATHEND⟧ although value-unbiased causes biased policy gradient)</li>
</ol>

<p>Clarify these points, and the three estimators will no longer be confusing.</p>

<p><strong>One-liners:</strong></p>

<ul>
  <li><strong>Only value (reward penalty):</strong> use ⟦INLMATH349MATHEND⟧ or ⟦INLMATH350MATHEND⟧ (both unbiased for reverse KL value); off-policy multiply by ⟦INLMATH351MATHEND⟧.</li>
  <li><strong>Need gradients (loss):</strong>
    <ul>
      <li><strong>On-policy:</strong> reverse KL -&gt; ⟦INLMATH352MATHEND⟧; forward KL -&gt; ⟦INLMATH353MATHEND⟧.</li>
      <li><strong>Off-policy:</strong> reverse KL -&gt; ⟦INLMATH354MATHEND⟧ or ⟦INLMATH355MATHEND⟧ (same gradient, low variance); fallback ⟦INLMATH356MATHEND⟧ (unbiased but noisier).</li>
    </ul>
  </li>
</ul>

<p>Keep three questions clear: <strong>who do we sample from, whose value do we estimate, whose gradient do we need?</strong> Especially note: <strong>on-policy vs. off-policy choose different estimators for reverse KL</strong> — on-policy use ⟦INLMATH357MATHEND⟧, off-policy use ⟦INLMATH358MATHEND⟧ or ⟦INLMATH359MATHEND⟧.</p>

<p>Additionally, don’t forget to determine <strong>the KL usage mode</strong> before choosing an estimator:</p>
<ul>
  <li><strong>KL as reward:</strong> Constraints act on the policy indirectly through shaped advantage, with cross-timestep credit assignment capability; agent will “plan to avoid high-KL paths”</li>
  <li><strong>KL as loss:</strong> Constraints act on the policy directly as an independent gradient term; agent will “visit but locally correct”</li>
</ul>

<p>This choice is more fundamental than the estimator itself, depending on whether you want constraints to be “preventive” or “corrective”.</p>

<h2 id="references">References</h2>

<ol>
  <li>
    <p>Dibya Ghosh. “KL Divergence for Machine Learning”. <a href="https://dibyaghosh.com/blog/probability/kldivergence">https://dibyaghosh.com/blog/probability/kldivergence</a></p>
  </li>
  <li>
    <p>John Schulman. “Approximating KL Divergence”. <a href="https://joschu.net/blog/kl-approx.html">https://joschu.net/blog/kl-approx.html</a></p>
  </li>
  <li>
    <p>Verl Documentation. “Proximal Policy Optimization (PPO)”. <a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">https://verl.readthedocs.io/en/latest/algo/ppo.html</a></p>
  </li>
  <li>
    <p>初七123334. RLHF/RLVR 训练中的 KL 近似方法浅析（k1 / k2 / k3）. <a href="https://zhuanlan.zhihu.com/p/1966872846212010437">https://zhuanlan.zhihu.com/p/1966872846212010437</a></p>
  </li>
  <li>
    <p>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. “Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization”. <a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></p>
  </li>
  <li>
    <p>Yifan Zhang, Yiping Ji, Gavin Brown, et al. “On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning”. <a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></p>
  </li>
  <li>
    <p>Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro. “A Comedy of Estimators: On KL Regularization in RL Training of LLMs”. <a href="https://arxiv.org/abs/2512.21852">https://arxiv.org/abs/2512.21852</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate.]]></summary></entry><entry xml:lang="zh"><title type="html">简单理解 RL 中的 KL 散度估计器：从数值估计到梯度估计</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-zh.html" rel="alternate" type="text/html" title="简单理解 RL 中的 KL 散度估计器：从数值估计到梯度估计" /><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-zh</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-zh.html"><![CDATA[<p><img src="/assets/img/kl-estimators/kl-estimator.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>在强化学习中，KL 散度的估计方式直接影响训练稳定性。本文系统剖析三种经典估计器 ⟦INLMATH69MATHEND⟧ 在 on-policy 和 off-policy 场景的性质差异，并给出「用于 loss 梯度回传」与「用于 reward 惩罚」时的选型指南。</p>
</blockquote>

<h2 id="引言kl-散度在强化学习中的角色">引言：KL 散度在强化学习中的角色</h2>

<p>在策略优化（如 PPO、GRPO）或对齐训练（RLHF/RLAIF）中，<strong>KL 惩罚</strong>是约束新策略不偏离参考策略的核心手段，旨在防止训练不稳定或策略崩溃。然而，KL 惩罚的实现涉及多个层次的选择：<strong>使用哪个估计器</strong>（⟦INLMATH70MATHEND⟧, ⟦INLMATH71MATHEND⟧, ⟦INLMATH72MATHEND⟧）、<strong>从哪个策略采样</strong>（on-policy 与 off-policy）、以及<strong>如何使用</strong>（作为 loss 梯度回传还是作为 reward 惩罚）。本文将系统地梳理这些选择及其相互关系，帮助读者厘清其中的关键概念。</p>

<h3 id="正向-kl-与反向-kl-的区别">正向 KL 与反向 KL 的区别</h3>

<p>设 ⟦INLMATH73MATHEND⟧ 为当前 actor 策略，⟦INLMATH74MATHEND⟧ 为参考策略，两种方向的 KL 散度分别为：</p>

<p><strong>反向 KL（Reverse KL）</strong>：
⟦DISPMATH1MATHEND⟧</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:80%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>正向 KL（Forward KL）</strong>：
⟦DISPMATH2MATHEND⟧</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:80%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>直观理解</strong>：</p>
<ul>
  <li><strong>反向 KL</strong> 倾向于「模式寻找」（mode-seeking）——策略会集中在参考分布的高概率区域，可能牺牲多样性。</li>
  <li><strong>正向 KL</strong> 倾向于「全覆盖」（mass-covering）——策略会尽量覆盖参考分布的支撑集。</li>
</ul>

<p>在 RLHF 的主流实现中，<strong>反向 KL</strong> 更为常见，因为我们希望 actor 策略不要偏离参考策略太远，而非要求完全覆盖其所有模式。</p>

<h3 id="本文的核心问题从谁采样估计什么怎么用">本文的核心问题：从谁采样、估计什么、怎么用</h3>

<p>在实际实现 KL 惩罚时，我们需要明确三个相互关联的问题：</p>

<ol>
  <li><strong>从谁采样？</strong> 样本来自当前策略 ⟦INLMATH75MATHEND⟧（on-policy），还是来自行为策略 ⟦INLMATH76MATHEND⟧（off-policy）？</li>
  <li><strong>估计什么？</strong> 我们想要估计的是反向 KL ⟦INLMATH77MATHEND⟧ 还是正向 KL ⟦INLMATH78MATHEND⟧？</li>
  <li><strong>怎么用？</strong> KL 项是作为 loss 参与梯度回传，还是作为 reward 惩罚（stop-gradient）？</li>
</ol>

<p>这三个问题的不同组合，决定了应该选用哪个估计器。本文的目标是系统地梳理这些选择及其相互关系。</p>

<h2 id="准备工作符号与基本概念">准备工作：符号与基本概念</h2>

<p>在深入分析之前，我们先统一符号约定，并推导两个在后文反复用到的基础结论。</p>

<h3 id="符号采样分布与真梯度">符号、采样分布与真梯度</h3>

<p><strong>符号约定</strong></p>

<ul>
  <li>⟦INLMATH79MATHEND⟧：当前 actor 策略（参数为 ⟦INLMATH80MATHEND⟧）</li>
  <li>⟦INLMATH81MATHEND⟧：若无歧义，后文简写 ⟦INLMATH82MATHEND⟧</li>
  <li>⟦INLMATH83MATHEND⟧：参考策略（reference policy），不依赖于 ⟦INLMATH84MATHEND⟧</li>
  <li>⟦INLMATH85MATHEND⟧：行为策略（behavior policy），用于 off-policy 采样，不依赖于 ⟦INLMATH86MATHEND⟧</li>
  <li>⟦INLMATH87MATHEND⟧：score function</li>
  <li>⟦INLMATH88MATHEND⟧：stop-gradient 操作（在代码中对应 <code class="language-plaintext highlighter-rouge">.detach()</code>）</li>
</ul>

<h4 id="统一的采样策略视角引入-inlmath89mathend-记号">统一的采样策略视角：引入 ⟦INLMATH89MATHEND⟧ 记号</h4>

<p>在分析 KL 估计器的梯度性质时，on-policy 和 off-policy 场景看似需要分开处理，但我们实际上可以用一个统一的框架来描述。</p>

<p>引入<strong>采样策略</strong> ⟦INLMATH90MATHEND⟧：数据来自 ⟦INLMATH91MATHEND⟧。定义<strong>统一的比率</strong>：</p>

<p>⟦DISPMATH3MATHEND⟧</p>

<p>这里的关键是：<strong>无论 on-policy 还是 off-policy，我们都把采样策略 ⟦INLMATH92MATHEND⟧ 视为梯度常量</strong>（即对 ⟦INLMATH93MATHEND⟧ 做 stop-gradient）。</p>

<ul>
  <li><strong>Off-policy</strong>（⟦INLMATH94MATHEND⟧）：⟦INLMATH95MATHEND⟧ 本来就不依赖 ⟦INLMATH96MATHEND⟧，所以 ⟦INLMATH97MATHEND⟧，有 ⟦INLMATH98MATHEND⟧</li>
  <li><strong>On-policy</strong>（⟦INLMATH99MATHEND⟧）：令 ⟦INLMATH100MATHEND⟧ 但 stop-gradient，于是 ⟦INLMATH101MATHEND⟧（数值恒为 1），但 ⟦INLMATH102MATHEND⟧</li>
</ul>

<p><strong>实现提示</strong>：on-policy 时虽然数值上 ⟦INLMATH103MATHEND⟧，但必须在计算图中显式构造 ⟦INLMATH104MATHEND⟧（或写成 ⟦INLMATH105MATHEND⟧）。如果直接把 ⟦INLMATH106MATHEND⟧ 写成常数 1，会丢失这条 score-function 梯度路径，导致推导退化为后文所说的“朴素 on-policy 写法”。</p>

<p><strong>直观理解</strong>：⟦INLMATH107MATHEND⟧ 的作用是把「采样分布对 ⟦INLMATH108MATHEND⟧ 的依赖」那条梯度路径补回来。在 on-policy 时，这正是「先期望后梯度」与「先梯度后期望」分裂的根源与修复方式。</p>

<p>有了这个统一记号，我们可以把 on-policy 和 off-policy 的分析合并成一套框架，大大简化后文的推导。</p>

<h4 id="score-function-与-kl-真梯度">Score Function 与 KL 真梯度</h4>

<p>Score function 有一个重要性质：⟦INLMATH109MATHEND⟧（因为 ⟦INLMATH110MATHEND⟧）。</p>

<p>利用这一性质，我们可以推导正向和反向 KL 散度对 ⟦INLMATH111MATHEND⟧ 的<strong>真梯度</strong>。</p>

<p><strong>反向 KL 的梯度</strong>：</p>

<p>⟦DISPMATH4MATHEND⟧</p>

<p>对 ⟦INLMATH112MATHEND⟧ 求梯度（使用乘积法则）：</p>

<p>⟦DISPMATH5MATHEND⟧</p>

<p>利用 ⟦INLMATH113MATHEND⟧ 以及 ⟦INLMATH114MATHEND⟧、⟦INLMATH115MATHEND⟧：</p>

<p>⟦DISPMATH6MATHEND⟧</p>

<p>即：</p>

<p>⟦DISPMATH7MATHEND⟧</p>

<blockquote>
  <p><strong>预告</strong>：后文将定义 ⟦INLMATH116MATHEND⟧，因此上式可简写为 ⟦INLMATH117MATHEND⟧——这个形式在梯度分析中反复出现。</p>
</blockquote>

<p><strong>正向 KL 的梯度</strong>：</p>

<p>⟦DISPMATH8MATHEND⟧</p>

<p>由于 ⟦INLMATH118MATHEND⟧ 不依赖于 ⟦INLMATH119MATHEND⟧：</p>

<p>⟦DISPMATH9MATHEND⟧</p>

<p>为了用 ⟦INLMATH120MATHEND⟧ 的样本估计这个量，进行重要性采样：</p>

<p>⟦DISPMATH10MATHEND⟧</p>

<p>利用 ⟦INLMATH121MATHEND⟧，可改写为：</p>

<p>⟦DISPMATH11MATHEND⟧</p>

<blockquote>
  <p><strong>预告</strong>：后文将推导 ⟦INLMATH122MATHEND⟧，因此 ⟦INLMATH123MATHEND⟧（正向 KL）——这解释了为什么直接对 ⟦INLMATH124MATHEND⟧ 反传会给出「错误」的梯度方向。</p>
</blockquote>

<p>有了这两个结果，我们就能在后文判断各估计器的梯度期望究竟对应哪个 KL 的真梯度。</p>

<h2 id="三种估计器的定义与设计原理">三种估计器的定义与设计原理</h2>

<p>记比值 ⟦INLMATH125MATHEND⟧，John Schulman 提出的三种单样本估计器定义如下：</p>

<h3 id="三种估计器定义与直觉">三种估计器：定义与直觉</h3>

<p><strong>⟦INLMATH126MATHEND⟧：最朴素的 log-ratio 估计器</strong></p>

<p>⟦DISPMATH12MATHEND⟧</p>

<p>这是最直接的定义——直接取 log-ratio 的负值。它对反向 KL 无偏，但有一个致命缺陷：<strong>可能取负值</strong>，而 KL 散度始终非负。这导致其方差极高，因为正负估计值会相互抵消。</p>

<p><strong>⟦INLMATH127MATHEND⟧：基于 f-散度的平方估计器</strong></p>

<p>⟦DISPMATH13MATHEND⟧</p>

<p><strong>设计动机</strong>：⟦INLMATH128MATHEND⟧ 的问题在于可正可负，而 ⟦INLMATH129MATHEND⟧ 通过取平方保证<strong>每个样本都是正的</strong>，直观上每个样本都在衡量 ⟦INLMATH130MATHEND⟧ 和 ⟦INLMATH131MATHEND⟧ 之间的差异程度。</p>

<p><strong>为什么偏差很小？</strong> ⟦INLMATH132MATHEND⟧ 本质上是一个 <strong>f-散度</strong>（f-divergence），其中 ⟦INLMATH133MATHEND⟧。f-散度有一个重要性质：<strong>所有可微的 f-散度在 ⟦INLMATH134MATHEND⟧ 时，二阶展开都形如</strong></p>

<p>⟦DISPMATH14MATHEND⟧</p>

<p>其中 ⟦INLMATH135MATHEND⟧ 是在 ⟦INLMATH136MATHEND⟧ 处的 Fisher 信息矩阵。KL 散度对应 ⟦INLMATH137MATHEND⟧，有 ⟦INLMATH138MATHEND⟧；而 ⟦INLMATH139MATHEND⟧ 对应的 ⟦INLMATH140MATHEND⟧，同样有 ⟦INLMATH141MATHEND⟧。这意味着<strong>当策略接近时，⟦INLMATH142MATHEND⟧ 与真实 KL 在二阶近似上具有相同的局部曲率</strong>，偏差主要体现在更高阶项。</p>

<p><strong>⟦INLMATH143MATHEND⟧：控制变量法构造的 Bregman 散度估计器</strong></p>

<p>⟦DISPMATH15MATHEND⟧</p>

<p><strong>设计动机</strong>：我们想要一个<strong>既无偏又低方差</strong>的估计器。标准做法是给 ⟦INLMATH144MATHEND⟧ 加一个<strong>控制变量</strong>（control variate）——一个期望为零但与 ⟦INLMATH145MATHEND⟧ 负相关的量。</p>

<p>注意到 ⟦INLMATH146MATHEND⟧，所以对于任意 ⟦INLMATH147MATHEND⟧，</p>

<p>⟦DISPMATH16MATHEND⟧</p>

<p>仍然是无偏估计。</p>

<p><strong>为什么选 ⟦INLMATH148MATHEND⟧？</strong> 由于 ⟦INLMATH149MATHEND⟧ 是凹函数，有 ⟦INLMATH150MATHEND⟧，因此</p>

<p>⟦DISPMATH17MATHEND⟧</p>

<p><strong>始终非负</strong>！这保证了每个样本都在「正向」贡献信息，消除了 ⟦INLMATH151MATHEND⟧ 正负抵消的问题。</p>

<p><strong>几何直观</strong>：⟦INLMATH152MATHEND⟧ 实际上是一个 <strong>Bregman 散度</strong>。考虑凸函数 ⟦INLMATH153MATHEND⟧，它在 ⟦INLMATH154MATHEND⟧ 处的切线为 ⟦INLMATH155MATHEND⟧。Bregman 散度定义为「函数值与切线值之差」：</p>

<p>⟦DISPMATH18MATHEND⟧</p>

<p>由于凸函数始终位于其切线上方，这个差值<strong>自然非负</strong>。更重要的是，当 ⟦INLMATH156MATHEND⟧ 时，函数与切线「贴合」得越来越紧密，差值以 ⟦INLMATH157MATHEND⟧ 的二阶速度趋近于零——这正是 ⟦INLMATH158MATHEND⟧ 在策略接近时方差小的根本原因。</p>

<p><strong>小结：三者的设计逻辑对比</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">估计器</th>
      <th style="text-align: center">定义</th>
      <th style="text-align: center">设计原理</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH159MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH160MATHEND⟧</td>
      <td style="text-align: center">最朴素定义</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH161MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH162MATHEND⟧</td>
      <td style="text-align: center">f-散度，二阶行为与 KL 一致</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH163MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH164MATHEND⟧</td>
      <td style="text-align: center">控制变量 + Bregman 散度</td>
    </tr>
  </tbody>
</table>

<p>了解了三种估计器的定义与设计原理后，我们首先分析它们在<strong>估计 KL 数值</strong>时的性质——即偏差与方差。</p>

<h2 id="数值估计偏差与方差">数值估计：偏差与方差</h2>

<p>本节分析三种估计器在<strong>估计 KL 数值</strong>时的性质。这些性质在任何使用场景下都是基础。</p>

<p>假设从 ⟦INLMATH165MATHEND⟧ 采样来估计反向 KL ⟦INLMATH166MATHEND⟧：</p>

<h3 id="无偏性分析">无偏性分析</h3>

<p>⟦DISPMATH19MATHEND⟧</p>

<p><strong>结论</strong>：对于估计反向 KL 的<strong>数值</strong>，⟦INLMATH167MATHEND⟧ 和 ⟦INLMATH168MATHEND⟧ 是无偏估计，而 ⟦INLMATH169MATHEND⟧ 是有偏的。</p>

<h3 id="方差特性分析">方差特性分析</h3>

<p>John Schulman 的实验（⟦INLMATH170MATHEND⟧，⟦INLMATH171MATHEND⟧，真实 KL = 0.005）表明：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">估计器</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH172MATHEND⟧</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">20</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH173MATHEND⟧</td>
      <td style="text-align: center">0.002</td>
      <td style="text-align: center">1.42</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH174MATHEND⟧</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.42</td>
    </tr>
  </tbody>
</table>

<p>当 KL 较大时（⟦INLMATH175MATHEND⟧，真实 KL = 0.5）：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">估计器</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH176MATHEND⟧</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH177MATHEND⟧</td>
      <td style="text-align: center">0.25</td>
      <td style="text-align: center">1.73</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH178MATHEND⟧</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.7</td>
    </tr>
  </tbody>
</table>

<p><strong>核心直观理解</strong>：</p>
<ul>
  <li>⟦INLMATH179MATHEND⟧ 以一阶项起步，当 ⟦INLMATH180MATHEND⟧ 接近 1 时波动较大，且可能取负值</li>
  <li>⟦INLMATH181MATHEND⟧ 在 ⟦INLMATH182MATHEND⟧ 处是二阶小量，始终非负，因此在策略接近时方差更小</li>
  <li>但当覆盖严重不足（⟦INLMATH183MATHEND⟧ 可能爆炸）时，⟦INLMATH184MATHEND⟧ 的方差会因权重爆炸而增大；此时 ⟦INLMATH185MATHEND⟧ 反而更稳定</li>
</ul>

<p><strong>数值估计小结</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">估计器</th>
      <th style="text-align: center">对数值的偏差</th>
      <th style="text-align: center">方差特性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH186MATHEND⟧</td>
      <td style="text-align: center">无偏</td>
      <td style="text-align: center">高（可正可负）</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH187MATHEND⟧</td>
      <td style="text-align: center">有偏（但极小）</td>
      <td style="text-align: center">低（恒正）</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH188MATHEND⟧</td>
      <td style="text-align: center">无偏</td>
      <td style="text-align: center">低（恒正）</td>
    </tr>
  </tbody>
</table>

<p>从数值估计的角度看，⟦INLMATH189MATHEND⟧ 是「无偏 + 低方差」的最优选择。</p>

<blockquote>
  <p><strong>注</strong>：若要估计<strong>正向 KL 的数值</strong> ⟦INLMATH190MATHEND⟧，而只能从 ⟦INLMATH191MATHEND⟧ 采样，可用重要性采样 ⟦INLMATH192MATHEND⟧。</p>
</blockquote>

<h2 id="kl-惩罚的两种使用方式">KL 惩罚的两种使用方式</h2>

<p>了解了估计器的数值性质后，我们需要进一步明确：<strong>KL 惩罚在强化学习中到底怎么用？</strong> 这一选择决定了我们是只关心估计器的数值性质，还是必须同时关心其梯度性质。</p>

<p>回顾 KL 正则化强化学习的目标函数（下式中用 ⟦INLMATH193MATHEND⟧ 表示“由策略 ⟦INLMATH194MATHEND⟧ 诱导的轨迹分布”）：</p>

<p>⟦DISPMATH20MATHEND⟧</p>

<p>这个数学形式看起来很统一，但在基于 Actor-Critic 的算法（如 PPO）中实现时，却衍生出了两种截然不同的实现范式——它们在代码层面可能只差几行，却对应着完全不同的优化语义。</p>

<blockquote>
  <p><strong>符号说明</strong>：本节用 ⟦INLMATH195MATHEND⟧ 或 ⟦INLMATH196MATHEND⟧ 泛指某个 token/state 级的 KL 估计器（如 ⟦INLMATH197MATHEND⟧），具体定义见前文「三种估计器的定义与设计原理」一节。</p>
</blockquote>

<h3 id="作为-losskl-参与梯度反传">作为 Loss：KL 参与梯度反传</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantage</span> <span class="o">*</span> <span class="n">log_prob</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>  <span class="c1"># kl 参与梯度计算
</span></code></pre></div></div>

<p>Critic 只学环境价值，KL 作为 actor 的正则项直接参与 loss 梯度回传。</p>

<h3 id="作为-rewardkl-加入奖励塑形">作为 Reward：KL 加入奖励塑形</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kl</span> <span class="o">=</span> <span class="nf">compute_kl</span><span class="p">(</span><span class="n">log_prob_q</span><span class="p">,</span> <span class="n">log_prob_p</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">shaped_reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>
</code></pre></div></div>

<p>KL 被视为环境奖励的一部分，用 shaped reward 做标准 actor-critic 更新。KL 项不参与 loss 梯度回传。</p>

<p>这两种做法看似只是代码里一个 <code class="language-plaintext highlighter-rouge">.detach()</code> 的区别，实际上对应着截然不同的优化语义。两种方式的深入对比将在后文「⟦INLMATH198MATHEND⟧ in Reward 与低方差 KL in Loss 的等价性与差异」一节详细展开。这里先给出核心区分：</p>

<ul>
  <li><strong>KL 作为 Loss</strong>：需要 KL 估计器的正确显式梯度，关心梯度对应哪个优化目标</li>
  <li><strong>KL 作为 Reward</strong>：需要 KL 的准确数值估计，同时还要关心它诱导的策略梯度是否正确</li>
</ul>

<p>下面我们按照「作为 Loss」和「作为 Reward」两种使用方式，深入剖析估计器的梯度性质。</p>

<h2 id="作为-loss-时的梯度分析">作为 Loss 时的梯度分析</h2>

<p>当 KL 作为 loss 参与梯度回传时，我们需要关心估计器对应的优化目标。这是实践中最容易混淆也最关键的部分。</p>

<p>利用前文引入的统一框架，我们可以把 on-policy 和 off-policy 的分析合并成一套推导。回顾统一的比率定义：</p>

<p>⟦DISPMATH21MATHEND⟧</p>

<p>其中 ⟦INLMATH199MATHEND⟧ 是采样策略。在这个框架下：</p>
<ul>
  <li><strong>On-policy</strong>（⟦INLMATH200MATHEND⟧）：⟦INLMATH201MATHEND⟧，但 ⟦INLMATH202MATHEND⟧</li>
  <li><strong>Off-policy</strong>（⟦INLMATH203MATHEND⟧）：⟦INLMATH204MATHEND⟧，且 ⟦INLMATH205MATHEND⟧</li>
</ul>

<h3 id="三种估计器的基本梯度">三种估计器的基本梯度</h3>

<p>首先计算三种估计器本身的梯度（不含 ⟦INLMATH206MATHEND⟧），这些结果在后续分析中会反复用到。</p>

<p><strong>推导 ⟦INLMATH207MATHEND⟧</strong>：</p>

<p>⟦DISPMATH22MATHEND⟧</p>

<p>⟦DISPMATH23MATHEND⟧</p>

<p><strong>推导 ⟦INLMATH208MATHEND⟧</strong>：</p>

<p>⟦DISPMATH24MATHEND⟧</p>

<p>由链式法则：</p>

<p>⟦DISPMATH25MATHEND⟧</p>

<p><strong>推导 ⟦INLMATH209MATHEND⟧</strong>：</p>

<p>⟦DISPMATH26MATHEND⟧</p>

<p>首先计算 ⟦INLMATH210MATHEND⟧。由于 ⟦INLMATH211MATHEND⟧：</p>

<p>⟦DISPMATH27MATHEND⟧</p>

<p>再计算 ⟦INLMATH212MATHEND⟧：</p>

<p>⟦DISPMATH28MATHEND⟧</p>

<p>因此：</p>

<p>⟦DISPMATH29MATHEND⟧</p>

<p><strong>小结</strong>：三种估计器的梯度分别为：</p>
<ul>
  <li>⟦INLMATH213MATHEND⟧</li>
  <li>⟦INLMATH214MATHEND⟧</li>
  <li>⟦INLMATH215MATHEND⟧</li>
</ul>

<p>这些基本梯度将在后续的统一框架分析中反复用到。</p>

<h4 id="先期望后梯度vs先梯度后期望一个重要警示">「先期望后梯度」vs「先梯度后期望」：一个重要警示</h4>

<p>在分析 KL 估计器的梯度时，有一个容易混淆的陷阱：<strong>「先期望后梯度」与「先梯度后期望」可能给出不同的结果</strong>。</p>

<p>如果从解析角度把 ⟦INLMATH216MATHEND⟧ 当作一个关于 ⟦INLMATH217MATHEND⟧ 的函数再求梯度（即「先期望后梯度」），由「数值估计」一节的结论 ⟦INLMATH218MATHEND⟧，我们有：</p>

<p>⟦DISPMATH30MATHEND⟧</p>

<p>⟦DISPMATH31MATHEND⟧</p>

<p>两者都给出反向 KL 的梯度。但在代码中直接对 ⟦INLMATH219MATHEND⟧ 的样本均值调用反传时，自动微分执行的是「先梯度后期望」，得到的是 ⟦INLMATH220MATHEND⟧——这与「先期望后梯度」的结果<strong>可能不同</strong>。</p>

<p>这种分裂的根源在于：当采样分布 ⟦INLMATH221MATHEND⟧ 本身依赖于 ⟦INLMATH222MATHEND⟧ 时，期望与梯度不能随意交换。这正是 on-policy 场景的核心困难，也是我们需要引入统一 ⟦INLMATH223MATHEND⟧ 框架的原因。</p>

<h3 id="统一框架下的梯度分析">统一框架下的梯度分析</h3>

<p>现在我们用 ⟦INLMATH224MATHEND⟧ 框架统一处理 on-policy 和 off-policy 场景。考虑 loss 形式 ⟦INLMATH225MATHEND⟧，其中 ⟦INLMATH226MATHEND⟧。</p>

<p><strong>关键观察</strong>：因为 ⟦INLMATH227MATHEND⟧ 不依赖 ⟦INLMATH228MATHEND⟧，对任何关于 ⟦INLMATH229MATHEND⟧ 可微的函数 ⟦INLMATH230MATHEND⟧，有</p>

<p>⟦DISPMATH32MATHEND⟧</p>

<p>这意味着在 ⟦INLMATH231MATHEND⟧ 框架下，「先期望后梯度」与「先梯度后期望」<strong>总是等价的</strong>——无论 on-policy 还是 off-policy。</p>

<blockquote>
  <p><strong>注意</strong>：这里的“期望”指的是对<strong>固定的采样分布</strong> ⟦INLMATH232MATHEND⟧ 的 ⟦INLMATH233MATHEND⟧。我们把“分布对 ⟦INLMATH234MATHEND⟧ 的依赖”统一塞进了 ⟦INLMATH235MATHEND⟧ 这条路径里；因此不要把这句话误读成对 ⟦INLMATH236MATHEND⟧ 也能不加条件地交换微分与期望。</p>
</blockquote>

<h4 id="统一框架下三种估计器的梯度推导">统一框架下三种估计器的梯度推导</h4>

<p>利用 ⟦INLMATH237MATHEND⟧（因为 ⟦INLMATH238MATHEND⟧），结合前文已推导的 ⟦INLMATH239MATHEND⟧，用乘积法则：</p>

<p><strong>⟦INLMATH240MATHEND⟧</strong>：</p>

<p>⟦DISPMATH33MATHEND⟧</p>

<p><strong>⟦INLMATH241MATHEND⟧</strong>：</p>

<p>⟦DISPMATH34MATHEND⟧</p>

<p><strong>⟦INLMATH242MATHEND⟧</strong>（对 ⟦INLMATH243MATHEND⟧ 做 stop-gradient）：</p>

<p>⟦DISPMATH35MATHEND⟧</p>

<p><strong>⟦INLMATH244MATHEND⟧</strong>：</p>

<p>⟦DISPMATH36MATHEND⟧</p>

<p>代入 ⟦INLMATH245MATHEND⟧：</p>

<p>⟦DISPMATH37MATHEND⟧</p>

<p>因此有一个关键简化：</p>

<p>⟦DISPMATH38MATHEND⟧</p>

<h4 id="梯度期望与优化目标">梯度期望与优化目标</h4>

<p>利用 ⟦INLMATH246MATHEND⟧ 和 ⟦INLMATH247MATHEND⟧：</p>

<p><strong>⟦INLMATH248MATHEND⟧</strong>：</p>

<p>⟦DISPMATH39MATHEND⟧</p>

<p><strong>⟦INLMATH249MATHEND⟧</strong>：</p>

<p>⟦DISPMATH40MATHEND⟧</p>

<p>也就是说，⟦INLMATH250MATHEND⟧ 的梯度期望对应的是“最小化 ⟦INLMATH251MATHEND⟧”（一个与 KL 二阶近似一致的 f-散度），而<strong>不是</strong>反向 KL ⟦INLMATH252MATHEND⟧ 的真梯度；因此当目标是反向 KL 时，⟦INLMATH253MATHEND⟧ 应当避免。</p>

<p><strong>⟦INLMATH254MATHEND⟧</strong>：</p>

<p>⟦DISPMATH41MATHEND⟧</p>

<p><strong>⟦INLMATH255MATHEND⟧</strong>：</p>

<p>⟦DISPMATH42MATHEND⟧</p>

<h4 id="梯度等价性哪些方法产生相同的梯度随机变量">梯度等价性：哪些方法产生相同的梯度随机变量</h4>

<p>从上述推导中，我们发现一个非常关键的事实：</p>

<blockquote>
  <p><strong>⟦INLMATH256MATHEND⟧ 与 ⟦INLMATH257MATHEND⟧ 的梯度完全相同</strong>：⟦INLMATH258MATHEND⟧</p>
</blockquote>

<p>这意味着它们不仅期望相同，而且<strong>作为随机变量完全等价</strong>——同均值、同方差、同高阶矩。</p>

<p><strong>总结表格</strong>：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Loss 形式</th>
      <th style="text-align: center">梯度随机变量</th>
      <th style="text-align: center">梯度期望</th>
      <th style="text-align: center">对应的优化目标</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH259MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH260MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH261MATHEND⟧</td>
      <td style="text-align: center">反向 KL ✓</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH262MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH263MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH264MATHEND⟧</td>
      <td style="text-align: center">f-散度（非反向 KL）✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH265MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH266MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH267MATHEND⟧</td>
      <td style="text-align: center">反向 KL ✓</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH268MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH269MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH270MATHEND⟧</td>
      <td style="text-align: center">反向 KL ✓</td>
    </tr>
  </tbody>
</table>

<h3 id="on-policy-与-off-policy-的统一视角">On-policy 与 Off-policy 的统一视角</h3>

<p>现在我们可以用统一框架重新审视 on-policy 和 off-policy 的关系。</p>

<p><strong>On-policy</strong>（⟦INLMATH271MATHEND⟧）：</p>
<ul>
  <li>⟦INLMATH272MATHEND⟧（数值恒为 1）</li>
  <li>⟦INLMATH273MATHEND⟧，⟦INLMATH274MATHEND⟧，⟦INLMATH275MATHEND⟧</li>
  <li>但梯度不同！因为 ⟦INLMATH276MATHEND⟧</li>
</ul>

<p>这解释了为什么 on-policy 时<strong>朴素直接反传</strong>（不显式构造 ⟦INLMATH277MATHEND⟧）用 ⟦INLMATH278MATHEND⟧ 或 ⟦INLMATH279MATHEND⟧ 作为 loss 会出问题：</p>
<ul>
  <li>直接用 ⟦INLMATH280MATHEND⟧：相当于没有 ⟦INLMATH281MATHEND⟧ 的版本，⟦INLMATH282MATHEND⟧，<strong>完全无效</strong></li>
  <li>直接用 ⟦INLMATH283MATHEND⟧：相当于没有 ⟦INLMATH284MATHEND⟧ 的版本，⟦INLMATH285MATHEND⟧（正向 KL），<strong>方向错误</strong></li>
  <li>直接用 ⟦INLMATH286MATHEND⟧：⟦INLMATH287MATHEND⟧（反向 KL）✓ <strong>朴素实现下唯一正确选择</strong></li>
</ul>

<p>但如果<strong>显式构造</strong> ⟦INLMATH288MATHEND⟧，则：</p>
<ul>
  <li><strong>可用</strong>：⟦INLMATH289MATHEND⟧（方差高）、⟦INLMATH290MATHEND⟧（推荐）、⟦INLMATH291MATHEND⟧（推荐）——三者均给出反向 KL 梯度</li>
  <li><strong>不可用</strong>：⟦INLMATH292MATHEND⟧（⟦INLMATH293MATHEND⟧ 参与梯度）——优化的是 f-散度而非反向 KL</li>
</ul>

<p><strong>Off-policy</strong>（⟦INLMATH294MATHEND⟧）：</p>
<ul>
  <li>⟦INLMATH295MATHEND⟧（标准重要性权重）</li>
  <li><strong>可用</strong>：⟦INLMATH296MATHEND⟧（方差高）、⟦INLMATH297MATHEND⟧（推荐）、⟦INLMATH298MATHEND⟧（推荐）——三者均给出反向 KL 梯度</li>
  <li><strong>不可用</strong>：⟦INLMATH299MATHEND⟧（⟦INLMATH300MATHEND⟧ 参与梯度）——优化的是 f-散度而非反向 KL</li>
</ul>

<p><strong>关键洞察</strong>：on-policy 时 ⟦INLMATH301MATHEND⟧ 能直接工作，本质上是因为 ⟦INLMATH302MATHEND⟧ 的梯度形式 ⟦INLMATH303MATHEND⟧ 恰好等于 ⟦INLMATH304MATHEND⟧（当 ⟦INLMATH305MATHEND⟧ 时）。这是一个「巧合」，而非一般规律。</p>

<p>关于大模型 off-policy 场景的深入分析，可以参考我之前的博客：<a href="/reinforcement-learning/2025/11/15/three-policy-zh.html">从两策略到三策略：LLM RL 中行为策略–参考策略不一致下的 TRPO 扩展</a>。</p>

<h3 id="方差分析">方差分析</h3>

<p>前面我们看到，给出反向 KL 无偏梯度的有三个选择：⟦INLMATH306MATHEND⟧、⟦INLMATH307MATHEND⟧、⟦INLMATH308MATHEND⟧。它们的梯度随机变量分别是（注意这里的 ⟦INLMATH309MATHEND⟧ 是向量，因此梯度也是向量）：</p>

<p>⟦DISPMATH43MATHEND⟧</p>

<p>其中 ⟦INLMATH310MATHEND⟧ 对应 ⟦INLMATH311MATHEND⟧ 和 ⟦INLMATH312MATHEND⟧（两者完全相同）。</p>

<p>为了避免“向量梯度的方差”这一表述的歧义，我们比较任意方向上的投影方差：取任意单位向量 ⟦INLMATH313MATHEND⟧，定义标量随机变量</p>

<p>⟦DISPMATH44MATHEND⟧</p>

<p>令 ⟦INLMATH314MATHEND⟧，⟦INLMATH315MATHEND⟧，则</p>

<p>⟦DISPMATH45MATHEND⟧</p>

<p>两者期望相同，且任意方向上的方差之差为</p>

<p>⟦DISPMATH46MATHEND⟧</p>

<p>（你也可以把这理解为对每个坐标分量分别比较方差；结论与直观量级判断是一致的。）</p>

<p><strong>在典型的 KL 惩罚 regime 下</strong>（⟦INLMATH316MATHEND⟧），取 ⟦INLMATH317MATHEND⟧，⟦INLMATH318MATHEND⟧：</p>
<ul>
  <li>⟦INLMATH319MATHEND⟧</li>
  <li>⟦INLMATH320MATHEND⟧，主导项为正的 ⟦INLMATH321MATHEND⟧ 常数</li>
</ul>

<p>因此 ⟦INLMATH322MATHEND⟧。</p>

<p><strong>核心直观理解</strong>：</p>
<ul>
  <li>⟦INLMATH323MATHEND⟧ 包含一个量级为 ⟦INLMATH324MATHEND⟧ 的零均值噪声项 ⟦INLMATH325MATHEND⟧</li>
  <li>⟦INLMATH326MATHEND⟧ 已把该常数噪声项消去，剩下与 ⟦INLMATH327MATHEND⟧ 成正比的一阶小量</li>
</ul>

<p><strong>方差对比表格</strong>：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">估计器</th>
      <th style="text-align: center">梯度随机变量</th>
      <th style="text-align: center">系数量级（⟦INLMATH328MATHEND⟧）</th>
      <th style="text-align: center">方差</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH329MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH330MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH331MATHEND⟧</td>
      <td style="text-align: center">高</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH332MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH333MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH334MATHEND⟧</td>
      <td style="text-align: center">低</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH335MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH336MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH337MATHEND⟧</td>
      <td style="text-align: center">低</td>
    </tr>
  </tbody>
</table>

<p><strong>结论</strong>：⟦INLMATH338MATHEND⟧ 与 ⟦INLMATH339MATHEND⟧ 在梯度层面完全等价——同均值、同方差、同高阶矩；相比之下，⟦INLMATH340MATHEND⟧ 的梯度多了一个零均值的常数噪声项，在典型的 KL 惩罚 regime 下其方差大约高一个量级。</p>

<blockquote>
  <p><strong>实践建议</strong>：若优化反向 KL，首选 ⟦INLMATH341MATHEND⟧ 或 ⟦INLMATH342MATHEND⟧（两者梯度等价且方差低）；⟦INLMATH343MATHEND⟧ 虽无偏但方差高，可作为备选并需配合 clipping/正则化。</p>
</blockquote>

<p><strong>极度 off-policy 时的警示</strong>：</p>

<p>当 ⟦INLMATH344MATHEND⟧ 与 ⟦INLMATH345MATHEND⟧ 差异很大——比如 ⟦INLMATH346MATHEND⟧ 在 ⟦INLMATH347MATHEND⟧ 的高密度区域几乎没有采样，或 ⟦INLMATH348MATHEND⟧ 在尾部爆炸——任何基于 ⟦INLMATH349MATHEND⟧ 的方法都会遭遇严重的方差问题。此时 ⟦INLMATH350MATHEND⟧（或 ⟦INLMATH351MATHEND⟧）相对 ⟦INLMATH352MATHEND⟧ 的优势不再有理论保证，需要结合 clipping、正则化等策略综合处理。</p>

<p>不过，在 RL 实践中我们通常会控制 KL 约束、限制 off-policy 程度（比如使用近邻策略 ⟦INLMATH353MATHEND⟧），在这个常见的 regime 里，可以相当有信心地说：</p>

<blockquote>
  <p><strong>如果已经决定用重要性采样来优化反向 KL，推荐使用 ⟦INLMATH354MATHEND⟧ 或 ⟦INLMATH355MATHEND⟧（两者梯度等价且方差低）；相较之下，⟦INLMATH356MATHEND⟧ 方差更高。</strong></p>
</blockquote>

<p>这就是为什么 DeepSeek v3.2 技术报告中使用的是 ⟦INLMATH357MATHEND⟧ 作为 off-policy KL 惩罚的估计器。</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/dpsk-3d2-k3.png" style="width:95%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://arxiv.org/pdf/2512.02556v1">DeepSeek v3.2 技术报告 3.1 章节</a></figcaption>
</figure>

<h4 id="梯度分析总览表">梯度分析总览表</h4>

<p>综合以上分析，下表汇总了统一框架下各估计器的梯度期望及其对应的优化目标：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">采样类型</th>
      <th style="text-align: center">Loss</th>
      <th style="text-align: center">⟦INLMATH358MATHEND⟧ Loss 的期望</th>
      <th style="text-align: center">对应的优化目标</th>
      <th style="text-align: center">能否用于优化反向 KL？</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">on/off-policy</td>
      <td style="text-align: center">⟦INLMATH359MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH360MATHEND⟧</td>
      <td style="text-align: center">反向 KL</td>
      <td style="text-align: center">✓（但方差较高）</td>
    </tr>
    <tr>
      <td style="text-align: center">on/off-policy</td>
      <td style="text-align: center">⟦INLMATH361MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH362MATHEND⟧</td>
      <td style="text-align: center">f-散度（非反向 KL）</td>
      <td style="text-align: center">✗</td>
    </tr>
    <tr>
      <td style="text-align: center">on/off-policy</td>
      <td style="text-align: center">⟦INLMATH363MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH364MATHEND⟧</td>
      <td style="text-align: center">反向 KL</td>
      <td style="text-align: center">✓（推荐，低方差）</td>
    </tr>
    <tr>
      <td style="text-align: center">on/off-policy</td>
      <td style="text-align: center">⟦INLMATH365MATHEND⟧</td>
      <td style="text-align: center">⟦INLMATH366MATHEND⟧</td>
      <td style="text-align: center">反向 KL</td>
      <td style="text-align: center">✓（推荐，低方差）</td>
    </tr>
  </tbody>
</table>

<p>其中 ⟦INLMATH367MATHEND⟧。当 on-policy（⟦INLMATH368MATHEND⟧）时，⟦INLMATH369MATHEND⟧。</p>

<p>需要特别强调：<strong>上表的结论针对的是 “loss 写成 ⟦INLMATH370MATHEND⟧ 且 ⟦INLMATH371MATHEND⟧ 在计算图中保留梯度路径” 的统一框架</strong>。在 on-policy 时虽然数值上 ⟦INLMATH372MATHEND⟧，但由于 ⟦INLMATH373MATHEND⟧，仍然有 ⟦INLMATH374MATHEND⟧，因此 ⟦INLMATH375MATHEND⟧ 与“直接对 ⟦INLMATH376MATHEND⟧ 的样本均值反传”在梯度上并不等价。</p>

<p>如果你采用的是<strong>朴素 on-policy 写法</strong>（即从 ⟦INLMATH377MATHEND⟧ 采样后，把 ⟦INLMATH378MATHEND⟧ 当作普通标量，对其样本均值直接反传；不显式构造 ⟦INLMATH379MATHEND⟧ 来补上 score-function 那条路径），那么会退化为：</p>
<ul>
  <li>直接用 ⟦INLMATH380MATHEND⟧：⟦INLMATH381MATHEND⟧（无效）</li>
  <li>直接用 ⟦INLMATH382MATHEND⟧：⟦INLMATH383MATHEND⟧（反向 KL）✓</li>
  <li>直接用 ⟦INLMATH384MATHEND⟧：⟦INLMATH385MATHEND⟧（正向 KL）✗</li>
</ul>

<p><strong>关键结论</strong>：</p>

<ol>
  <li><strong>On-policy 优化反向 KL（朴素直接反传的实现）</strong>：唯一正确选择是 ⟦INLMATH386MATHEND⟧</li>
  <li><strong>Off-policy 优化反向 KL</strong>：有三个正确选项：
    <ul>
      <li>⟦INLMATH387MATHEND⟧：无偏但方差较高</li>
      <li>⟦INLMATH388MATHEND⟧：无偏，与 ⟦INLMATH389MATHEND⟧ <strong>梯度完全等价</strong></li>
      <li>⟦INLMATH390MATHEND⟧：无偏且方差更低（与上一项等价，均为推荐选择）</li>
    </ul>
  </li>
  <li><strong>⟦INLMATH391MATHEND⟧（权重参与梯度）失效</strong>：这是一个容易被忽视的陷阱</li>
</ol>

<h2 id="作为-reward-时的梯度分析">作为 Reward 时的梯度分析</h2>

<p>前文分析了 KL 作为 Loss 时各估计器的梯度性质。一个自然的想法是：既然 ⟦INLMATH392MATHEND⟧ 和 ⟦INLMATH393MATHEND⟧ 对反向 KL 数值都是无偏的（见「数值估计」章节），那么把它们（加 stop-gradient）作为 reward 惩罚应该都没问题。</p>

<p><strong>但这是错误的。</strong></p>

<p>问题在于：当 KL 作为 reward 惩罚时，虽然 KL 项本身不反传梯度，但它会通过 advantage 间接影响策略梯度。因此，评价一个估计器「能否用于 reward 惩罚」，不应只看数值偏差，而应看<strong>它诱导的策略梯度是否正确</strong>。</p>

<h3 id="真正的-kl-正则化策略梯度">真正的 KL 正则化策略梯度</h3>

<p>考虑 KL 正则化的强化学习目标：</p>

<p>⟦DISPMATH47MATHEND⟧</p>

<p>其真梯度为：</p>

<p>⟦DISPMATH48MATHEND⟧</p>

<p>利用前文「准备工作」章节的结论，反向 KL 的梯度为：</p>

<p>⟦DISPMATH49MATHEND⟧</p>

<p>因此，真正的 KL 正则化策略梯度是：</p>

<p>⟦DISPMATH50MATHEND⟧</p>

<h4 id="使用估计器-inlmath394mathend-时的梯度形式">使用估计器 ⟦INLMATH394MATHEND⟧ 时的梯度形式</h4>

<p>当我们用某个估计器 ⟦INLMATH395MATHEND⟧（加 stop-gradient）作为 reward 惩罚时，shaped reward 为 ⟦INLMATH396MATHEND⟧，策略梯度变为：</p>

<p>⟦DISPMATH51MATHEND⟧</p>

<p><strong>无偏条件</strong>：⟦INLMATH397MATHEND⟧ 当且仅当</p>

<p>⟦DISPMATH52MATHEND⟧</p>

<h4 id="使用-inlmath398mathend-作为惩罚梯度无偏">使用 ⟦INLMATH398MATHEND⟧ 作为惩罚：梯度无偏</h4>

<p>当 ⟦INLMATH399MATHEND⟧ 时，条件自动满足：</p>

<p>⟦DISPMATH53MATHEND⟧</p>

<p>因此，<strong>⟦INLMATH400MATHEND⟧ 作为 reward 惩罚时，诱导的策略梯度是无偏的</strong>。</p>

<h4 id="使用-inlmath401mathend-作为惩罚梯度有偏">使用 ⟦INLMATH401MATHEND⟧ 作为惩罚：梯度有偏</h4>

<p>当 ⟦INLMATH402MATHEND⟧ 时：</p>

<p>⟦DISPMATH54MATHEND⟧</p>

<p>第二项正是 ⟦INLMATH403MATHEND⟧。问题出在第一项：</p>

<p>⟦DISPMATH55MATHEND⟧</p>

<p>而这个量可以改写为：</p>

<p>⟦DISPMATH56MATHEND⟧</p>

<p>利用正向 KL 的梯度公式 ⟦INLMATH404MATHEND⟧，有：</p>

<p>⟦DISPMATH57MATHEND⟧</p>

<p>因此：</p>

<p>⟦DISPMATH58MATHEND⟧</p>

<p><strong>⟦INLMATH405MATHEND⟧ 作为 reward 惩罚时，梯度是有偏的</strong>，偏差项等于正向 KL 梯度的负值。</p>

<p><strong>偏差的几何含义</strong>：使用 ⟦INLMATH406MATHEND⟧ 作为 reward 惩罚，相当于在优化一个「错误的混合目标」：</p>
<ul>
  <li>既惩罚反向 KL（希望策略不偏离参考）</li>
  <li>又<strong>错误地鼓励正向 KL 变大</strong>（希望参考不覆盖策略）</li>
</ul>

<p>这两个方向相互冲突，可能导致优化不稳定。</p>

<p><strong>实验验证</strong>：Shah et al. (2025) 的实验表明，在 on-policy RL 微调 LLM 时：</p>
<ul>
  <li><strong>⟦INLMATH407MATHEND⟧ in reward</strong>：训练稳定</li>
  <li><strong>⟦INLMATH408MATHEND⟧ in reward</strong>：<strong>训练崩溃</strong></li>
</ul>

<p>这与我们的理论分析完全一致。</p>

<h4 id="off-policy-场景下的结论">Off-policy 场景下的结论</h4>

<p>上述分析假设 on-policy 采样。在 off-policy 场景下，结论是否改变？</p>

<p>设样本来自行为策略 ⟦INLMATH409MATHEND⟧，使用重要性加权的策略梯度：</p>

<p>⟦DISPMATH59MATHEND⟧</p>

<p>利用 ⟦INLMATH410MATHEND⟧，上式等于：</p>

<p>⟦DISPMATH60MATHEND⟧</p>

<p><strong>无偏条件</strong>仍然是 ⟦INLMATH411MATHEND⟧，与 on-policy 完全相同。</p>

<p><strong>关键洞察</strong>：在 off-policy 策略梯度框架下，重要性权重 ⟦INLMATH412MATHEND⟧ 作用于整个策略梯度估计器，<strong>不需要对 shaped reward 中的 KL 估计器单独加权</strong>。因此：</p>

<ul>
  <li>Shaped reward 保持原形式：⟦INLMATH413MATHEND⟧（不是 ⟦INLMATH414MATHEND⟧）</li>
  <li>在本文讨论的 <strong>stop-grad reward shaping</strong>（⟦INLMATH415MATHEND⟧）且目标为 <strong>反向 KL 正则</strong> 的设定下：结论与 on-policy 相同，<strong>只能用 ⟦INLMATH416MATHEND⟧，不能用 ⟦INLMATH417MATHEND⟧</strong></li>
</ul>

<h3 id="关键发现只有-inlmath418mathend-可用于-reward-惩罚">关键发现：只有 ⟦INLMATH418MATHEND⟧ 可用于 Reward 惩罚</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">估计器</th>
      <th style="text-align: center">数值无偏？</th>
      <th style="text-align: center">作为 Reward 惩罚时梯度无偏？</th>
      <th style="text-align: center">实际表现</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH419MATHEND⟧</td>
      <td style="text-align: center">✓</td>
      <td style="text-align: center">✓</td>
      <td style="text-align: center">稳定</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH420MATHEND⟧</td>
      <td style="text-align: center">✓</td>
      <td style="text-align: center">✗</td>
      <td style="text-align: center">崩溃</td>
    </tr>
  </tbody>
</table>

<p><strong>核心教训</strong>：评价 KL 估计器时，「数值无偏」和「梯度正确」是两个独立的维度。对于本文讨论的 reward 惩罚用法（stop-grad reward shaping，目标为反向 KL 正则；无论 on-policy 还是 off-policy），<strong>只有 ⟦INLMATH421MATHEND⟧ 是正确的选择</strong>。⟦INLMATH422MATHEND⟧ 虽然数值无偏且方差更低，但作为 reward 惩罚会导致梯度有偏，可能引发训练崩溃。</p>

<p>到这里容易产生一个“表面矛盾”：</p>
<ul>
  <li>在 <strong>Reward 惩罚</strong>里我们强调“只能用 ⟦INLMATH423MATHEND⟧”；</li>
  <li>但在前文 <strong>Loss 反传</strong>（尤其 off-policy）里，我们又推荐用 $
ho k_3⟦INLMATH424MATHEND⟧	ext{sg}(\rho)k_2$ 来获得更低方差的反向 KL 梯度。</li>
</ul>

<p>下一节将解释：两者并不冲突——在“KL 正则项对策略更新的那一部分”上，它们甚至可以做到<strong>样本级完全等价</strong>；差异主要来自 KL 是否进入 advantage/baseline、以及信用分配（credit assignment）的路径。</p>

<h2 id="inlmath425mathend-in-reward-与低方差-kl-in-loss-的等价性与差异">⟦INLMATH425MATHEND⟧ in Reward 与低方差 KL in Loss 的等价性与差异</h2>

<p>前面我们分别分析了 KL 作为 Loss 和作为 Reward 两种使用方式。一个自然的问题是：<strong>这两种方式在什么意义上等价，又在什么意义上不同？</strong> 本节将深入探讨这个问题，特别是在大模型 RL 的实践场景下。</p>

<h3 id="kl-梯度项的样本级等价性">KL 梯度项的样本级等价性</h3>

<p>本节只比较“KL 正则化带来的那一项策略梯度”，并统一写成 <strong>policy gradient 的上升方向</strong> ⟦INLMATH426MATHEND⟧（若你在代码里最小化 loss，则整体只差一个全局负号，不影响等价性结论）。同时默认你使用的是本文前文的统一权重记号：样本来自 ⟦INLMATH427MATHEND⟧，重要性权重 ⟦INLMATH428MATHEND⟧ 作用在策略梯度估计器上。</p>

<p>回顾前文的关键结论：</p>

<p><strong>KL 作为 Loss（低方差选择）</strong>：前文已证明，采用 ⟦INLMATH429MATHEND⟧ 或 ⟦INLMATH430MATHEND⟧ 作为正则项时，梯度随机变量都化简为</p>

<p>⟦DISPMATH61MATHEND⟧</p>

<p><strong>KL 作为 Reward（⟦INLMATH431MATHEND⟧ in reward）</strong>：shaped reward 为 ⟦INLMATH432MATHEND⟧（对 ⟦INLMATH433MATHEND⟧ 做 stop-gradient 只是在实现上避免“KL 直接反传”，不改变它作为惩罚的数值）。在“策略梯度项”里，KL 惩罚贡献的是</p>

<p>⟦DISPMATH62MATHEND⟧</p>

<p><strong>关键发现</strong>：两者的 KL 梯度项<strong>样本级完全相同</strong>。</p>

<p>也就是说，在不考虑 baseline/advantage 的具体构造细节时：</p>
<ul>
  <li>“把 KL 写进 loss 并用低方差实现（⟦INLMATH434MATHEND⟧ 或 ⟦INLMATH435MATHEND⟧）”</li>
  <li>与“把 KL 写进 reward 并选 ⟦INLMATH436MATHEND⟧（stop-grad shaped reward）”</li>
</ul>

<p>对策略更新施加的 KL 正则“力”可以是一模一样的。</p>

<p>具体来说，如果我们只看“最大化 ⟦INLMATH437MATHEND⟧”时 KL 惩罚贡献的那一项梯度（惩罚项在 ⟦INLMATH438MATHEND⟧ 里带负号，因此这项的上升方向自然带 ⟦INLMATH439MATHEND⟧）：</p>
<ul>
  <li><strong>KL in Loss（低方差实现）</strong>：⟦INLMATH440MATHEND⟧</li>
  <li><strong>KL in Reward（⟦INLMATH441MATHEND⟧ in reward）</strong>：⟦INLMATH442MATHEND⟧</li>
</ul>

<p>它们是<strong>同一个随机变量</strong>，不仅期望相同，方差也完全相同。</p>

<h4 id="整体更新语义的差异">整体更新语义的差异</h4>

<p>尽管 KL 梯度项在样本级等价，<strong>两种方式的整体更新语义仍然不同</strong>。差异主要体现在以下几个方面：</p>

<h4 id="1-kl-是否进入-advantagebaseline">1. KL 是否进入 Advantage/Baseline</h4>

<p><strong>KL 作为 Loss</strong>（等价于最大化 ⟦INLMATH443MATHEND⟧，但把 KL 项作为一个独立的、可控的“显式力”来实现）：</p>

<p>⟦DISPMATH63MATHEND⟧</p>

<p>KL 是一个<strong>独立的正则项</strong>，与 advantage 完全解耦。KL 梯度的大小只取决于 ⟦INLMATH444MATHEND⟧ 本身，不受 critic 质量或 baseline 选择的影响。</p>

<p><strong>KL 作为 Reward</strong>：</p>

<p>⟦DISPMATH64MATHEND⟧</p>

<p>KL 通过 shaped reward 进入 advantage 计算，会被 baseline 处理。这意味着：</p>
<ul>
  <li>KL 的影响会被 advantage 的构造方式调制</li>
  <li>如果使用 value function baseline，KL 的影响会被部分吸收</li>
</ul>

<p>从实现角度看，这里的差别可以理解为：Loss 方案把“环境回报部分”和“KL 正则部分”分开估计；Reward 方案把 KL 视为回报的一部分，因此它会跟着你对回报做的所有处理（baseline、归一化、截断等）一起走。</p>

<h4 id="2-信度分配独立正则力-vs-混入-shaped-reward">2. 信度分配：独立正则力 vs 混入 Shaped Reward</h4>

<p><strong>KL 作为 Loss</strong>：每个 token/state 的 KL 梯度是「局部」的，只影响该位置的策略更新。</p>

<p><strong>KL 作为 Reward</strong>：KL 惩罚通过 return/advantage 的时间回传，可能影响到更早的决策。</p>

<h4 id="3-reward-中心化-kl对梯度无偏性的影响">3. Reward 中心化 KL：对梯度无偏性的影响</h4>

<p>在大模型 RL（如 GRPO、PPO for LLM）中，常见的 advantage 计算方式是 ⟦INLMATH445MATHEND⟧。当 KL 作为 Reward 时，是否把 KL 也纳入 mean 会影响梯度的无偏性。</p>

<p>设采样 ⟦INLMATH446MATHEND⟧，记 ⟦INLMATH447MATHEND⟧，并用 ⟦INLMATH448MATHEND⟧ 表示第 ⟦INLMATH449MATHEND⟧ 个样本的 KL 惩罚标量，⟦INLMATH450MATHEND⟧。</p>

<p><strong>不中心化（⟦INLMATH451MATHEND⟧）</strong>：KL 梯度项的期望为</p>

<p>⟦DISPMATH65MATHEND⟧</p>

<p>这是对 ⟦INLMATH452MATHEND⟧ 的<strong>无偏梯度</strong>。</p>

<p><strong>同 batch 均值中心化（⟦INLMATH453MATHEND⟧，含自身）</strong>：由于 ⟦INLMATH454MATHEND⟧ 依赖所有样本（包括 ⟦INLMATH455MATHEND⟧ 自身），期望梯度变为</p>

<p>⟦DISPMATH66MATHEND⟧</p>

<p>即 KL 正则梯度被<strong>缩小</strong>了 ⟦INLMATH456MATHEND⟧，等价于有效 ⟦INLMATH457MATHEND⟧ 变小。这不是严格无偏的。</p>

<p><strong>Leave-one-out 中心化（⟦INLMATH458MATHEND⟧）</strong>：若改用 ⟦INLMATH459MATHEND⟧，则 ⟦INLMATH460MATHEND⟧ 与 ⟦INLMATH461MATHEND⟧ 独立，有 ⟦INLMATH462MATHEND⟧，因此</p>

<p>⟦DISPMATH67MATHEND⟧</p>

<p>仍是<strong>无偏梯度</strong>，同时享受中心化带来的方差缩减。</p>

<p><strong>结论</strong>：同 batch 均值中心化引入的偏差为 ⟦INLMATH463MATHEND⟧，在 GRPO 等大 batch 场景下影响很小；若追求严格无偏，可改用 leave-one-out 均值，同时享受方差缩减。</p>

<h3 id="何时选择哪种方式">何时选择哪种方式？</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">维度</th>
      <th style="text-align: center">KL 作为 Loss</th>
      <th style="text-align: center">KL 作为 Reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">KL 梯度形态</td>
      <td style="text-align: center">⟦INLMATH464MATHEND⟧（低方差选择）</td>
      <td style="text-align: center">⟦INLMATH465MATHEND⟧</td>
    </tr>
    <tr>
      <td style="text-align: center">与 Advantage</td>
      <td style="text-align: center">完全解耦</td>
      <td style="text-align: center">通过 shaped reward 耦合</td>
    </tr>
    <tr>
      <td style="text-align: center">KL 中心化</td>
      <td style="text-align: center">无（绝对惩罚）</td>
      <td style="text-align: center">有（⟦INLMATH466MATHEND⟧）</td>
    </tr>
    <tr>
      <td style="text-align: center">信度分配</td>
      <td style="text-align: center">局部、per-token</td>
      <td style="text-align: center">可能有时间回传（取决于实现）</td>
    </tr>
    <tr>
      <td style="text-align: center">适用场景</td>
      <td style="text-align: center">希望 KL 约束更可控、更不依赖 critic</td>
      <td style="text-align: center">希望 KL 约束更全局、有规划性</td>
    </tr>
  </tbody>
</table>

<p><strong>实践建议</strong>：</p>

<ol>
  <li>
    <p><strong>如果你希望 KL 约束是「修正性」的</strong>——允许 agent 探索但在局部修正行为，且希望 KL 压力更可控、更不依赖 critic 质量 → 选择 <strong>KL 作为 Loss</strong>，使用 ⟦INLMATH467MATHEND⟧ 或 ⟦INLMATH468MATHEND⟧（其中 on-policy 时若不想显式构造 ⟦INLMATH469MATHEND⟧，直接用 ⟦INLMATH470MATHEND⟧ 更简单且不易踩坑）</p>
  </li>
  <li>
    <p><strong>如果你希望 KL 约束是「预防性」的</strong>——让 agent 从根源上避开高 KL 区域，且接受 KL 被 baseline 调制 → 选择 <strong>KL 作为 Reward</strong>，使用 ⟦INLMATH471MATHEND⟧</p>
  </li>
</ol>

<p>基于上述“数值无偏 vs 梯度正确”“Loss vs Reward 实现差异”的结论，下面进入可直接照抄到代码里的选型速查与常见踩坑点。</p>

<h2 id="实践指南与常见陷阱">实践指南与常见陷阱</h2>

<h3 id="三种估计器定义速查">三种估计器定义速查</h3>

<p>⟦DISPMATH68MATHEND⟧</p>

<h3 id="数值估计性质">数值估计性质</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">估计器</th>
      <th style="text-align: center">对反向 KL ⟦INLMATH472MATHEND⟧ 数值无偏？</th>
      <th style="text-align: center">方差</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH473MATHEND⟧</td>
      <td style="text-align: center">✓</td>
      <td style="text-align: center">高（可负）</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH474MATHEND⟧</td>
      <td style="text-align: center">✗（但偏差极小）</td>
      <td style="text-align: center">低</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH475MATHEND⟧</td>
      <td style="text-align: center">✓</td>
      <td style="text-align: center">低</td>
    </tr>
  </tbody>
</table>

<h3 id="选型速查表">选型速查表</h3>

<h4 id="on-policy-优化反向-klloss">On-policy 优化反向 KL（Loss）</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Loss 形式</th>
      <th style="text-align: center">优点</th>
      <th style="text-align: center">问题</th>
      <th style="text-align: center">推荐</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH476MATHEND⟧</td>
      <td style="text-align: center">—</td>
      <td style="text-align: center">梯度期望为零，<strong>完全无效</strong>，不能用于优化</td>
      <td style="text-align: center">✗✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH477MATHEND⟧</td>
      <td style="text-align: center">梯度正确（反向 KL），低方差，<strong>实现最简单</strong></td>
      <td style="text-align: center">数值有偏（但偏差极小）</td>
      <td style="text-align: center">✓✓</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH478MATHEND⟧</td>
      <td style="text-align: center">—</td>
      <td style="text-align: center">梯度对应<strong>正向 KL</strong>，方向错误，不能用于优化反向 KL</td>
      <td style="text-align: center">✗✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH479MATHEND⟧</td>
      <td style="text-align: center">梯度正确（反向 KL），低方差，数值无偏</td>
      <td style="text-align: center">需显式构造 ⟦INLMATH480MATHEND⟧，实现稍复杂</td>
      <td style="text-align: center">✓</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>注</strong>：⟦INLMATH481MATHEND⟧ 与 ⟦INLMATH482MATHEND⟧ 的梯度完全相同（样本级等价）。On-policy 时推荐直接用 ⟦INLMATH483MATHEND⟧，实现最简单。</p>
</blockquote>

<h4 id="off-policy-优化反向-klloss">Off-policy 优化反向 KL（Loss）</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Loss 形式</th>
      <th style="text-align: center">优点</th>
      <th style="text-align: center">问题</th>
      <th style="text-align: center">推荐</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH484MATHEND⟧</td>
      <td style="text-align: center">梯度正确（反向 KL），数值无偏</td>
      <td style="text-align: center"><strong>方差较高</strong></td>
      <td style="text-align: center">△</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH485MATHEND⟧</td>
      <td style="text-align: center">—</td>
      <td style="text-align: center">梯度对应 <strong>f-散度</strong>（非反向 KL），不能用于优化反向 KL</td>
      <td style="text-align: center">✗✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH486MATHEND⟧</td>
      <td style="text-align: center">梯度正确（反向 KL），<strong>低方差</strong></td>
      <td style="text-align: center">数值有偏（但偏差极小）</td>
      <td style="text-align: center">✓✓</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH487MATHEND⟧</td>
      <td style="text-align: center">梯度正确（反向 KL），<strong>低方差</strong>，数值无偏</td>
      <td style="text-align: center">—</td>
      <td style="text-align: center">✓✓</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>注</strong>：⟦INLMATH488MATHEND⟧ 与 ⟦INLMATH489MATHEND⟧ 的梯度完全相同（样本级等价）。两者均为推荐选择。</p>
</blockquote>

<h4 id="kl-作为-reward-惩罚stop-grad-shaped-reward">KL 作为 Reward 惩罚（stop-grad shaped reward）</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: center">估计器</th>
      <th style="text-align: center">优点</th>
      <th style="text-align: center">问题</th>
      <th style="text-align: center">推荐</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">⟦INLMATH490MATHEND⟧</td>
      <td style="text-align: center">数值无偏，<strong>诱导的策略梯度无偏</strong></td>
      <td style="text-align: center">方差较高</td>
      <td style="text-align: center">✓✓</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH491MATHEND⟧</td>
      <td style="text-align: center">数值有偏</td>
      <td style="text-align: center">诱导的策略梯度有偏</td>
      <td style="text-align: center">✗✗</td>
    </tr>
    <tr>
      <td style="text-align: center">⟦INLMATH492MATHEND⟧</td>
      <td style="text-align: center">数值无偏，低方差</td>
      <td style="text-align: center"><strong>诱导的策略梯度有偏</strong>，偏差项为 ⟦INLMATH493MATHEND⟧，可能导致<strong>训练崩溃</strong></td>
      <td style="text-align: center">✗✗</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>注</strong>：Reward 惩罚场景下，<strong>只有 ⟦INLMATH494MATHEND⟧ 是正确的选择</strong>。⟦INLMATH495MATHEND⟧ 虽然数值无偏且方差低，但会导致策略梯度有偏，实验中观察到训练崩溃。</p>
</blockquote>

<h4 id="图例说明">图例说明</h4>

<ul>
  <li>✓✓：<strong>强烈推荐</strong>，理论正确且实践表现好</li>
  <li>✓：推荐，理论正确但实现稍复杂或有小缺点</li>
  <li>△：可用但需谨慎，存在方差高等问题</li>
  <li>✗✗：<strong>禁止使用</strong>，理论上错误或会导致训练失败</li>
</ul>

<h3 id="常见陷阱">常见陷阱</h3>

<ol>
  <li><strong>On-policy 下用 ⟦INLMATH496MATHEND⟧ 作为 Loss</strong>：梯度期望为零，完全无效</li>
  <li><strong>On-policy 下用 ⟦INLMATH497MATHEND⟧ 作为 Loss 优化反向 KL</strong>：其梯度对应正向 KL ⟦INLMATH498MATHEND⟧，方向错误</li>
  <li><strong>Off-policy 下用 ⟦INLMATH499MATHEND⟧（重要性权重不 detach）</strong>：梯度对应 f-散度而非反向 KL</li>
  <li><strong>在 Reward 惩罚中使用 ⟦INLMATH500MATHEND⟧</strong>：虽然数值无偏，但诱导的策略梯度有偏，可能导致训练崩溃</li>
  <li><strong>On-policy 时把 ⟦INLMATH501MATHEND⟧ 写成常数 1</strong>：必须显式构造 ⟦INLMATH502MATHEND⟧（或 ⟦INLMATH503MATHEND⟧），否则会丢失 score-function 梯度路径，导致 ⟦INLMATH504MATHEND⟧ 和 ⟦INLMATH505MATHEND⟧ 退化为朴素写法而失效</li>
  <li><strong>混淆「数值无偏」与「梯度正确」</strong>：⟦INLMATH506MATHEND⟧ 对反向 KL 数值无偏，但作为 Reward 惩罚时诱导的策略梯度有偏；选估计器时必须同时考虑两个维度</li>
</ol>

<h2 id="总结">总结</h2>

<p>本文围绕「<strong>从谁采样</strong>」「<strong>怎么用</strong>」「<strong>估计什么</strong>」三个核心问题，系统剖析了 ⟦INLMATH507MATHEND⟧ 三种 KL 估计器。</p>

<blockquote>
  <p><strong>核心结论</strong>：<strong>数值无偏 ≠ 梯度正确</strong>。选估计器时，必须同时考虑「估计谁的数值」和「梯度对应哪个优化目标」。</p>
</blockquote>

<p><strong>核心内容</strong>：</p>

<ol>
  <li><strong>数值估计</strong>：⟦INLMATH508MATHEND⟧ 和 ⟦INLMATH509MATHEND⟧ 对反向 KL 数值无偏，⟦INLMATH510MATHEND⟧ 兼具低方差</li>
  <li><strong>作为 Loss 时的梯度</strong>：On-policy 用 ⟦INLMATH511MATHEND⟧ 或 ⟦INLMATH512MATHEND⟧；Off-policy 用 ⟦INLMATH513MATHEND⟧ 或 ⟦INLMATH514MATHEND⟧</li>
  <li><strong>作为 Reward 惩罚</strong>：只能用 ⟦INLMATH515MATHEND⟧，⟦INLMATH516MATHEND⟧ 会导致策略梯度有偏</li>
  <li><strong>Loss 与 Reward 两种实现的关系</strong>：
    <ul>
      <li><strong>样本级等价性</strong>：当 Loss 使用低方差实现（⟦INLMATH517MATHEND⟧ 或 ⟦INLMATH518MATHEND⟧）、Reward 使用 ⟦INLMATH519MATHEND⟧ 时，两者的 KL 梯度项是<strong>同一个随机变量</strong> ⟦INLMATH520MATHEND⟧，不仅期望相同，方差也完全相同</li>
      <li><strong>整体语义差异</strong>：Loss 方式中 KL 是独立正则项，与 advantage 完全解耦，不受 critic 质量影响；Reward 方式中 KL 通过 shaped reward 进入 advantage 计算，会被 baseline 处理和调制</li>
      <li><strong>信度分配差异</strong>：Loss 方式的 KL 梯度是局部的（per-token）；Reward 方式的 KL 惩罚可能通过 return 回传影响更早的决策</li>
    </ul>
  </li>
  <li><strong>统一 ⟦INLMATH521MATHEND⟧ 框架</strong>：本文引入 ⟦INLMATH522MATHEND⟧ 统一处理 on-policy 和 off-policy 场景。该框架的核心洞察是：把「采样分布对 ⟦INLMATH523MATHEND⟧ 的依赖」显式地塞进 ⟦INLMATH524MATHEND⟧ 这条梯度路径，从而使「先期望后梯度」与「先梯度后期望」在 ⟦INLMATH525MATHEND⟧ 下总是等价。On-policy 时 ⟦INLMATH526MATHEND⟧ 但 ⟦INLMATH527MATHEND⟧，这解释了为什么直接对 ⟦INLMATH528MATHEND⟧ 或 ⟦INLMATH529MATHEND⟧ 反传会失效，而 ⟦INLMATH530MATHEND⟧ 能「巧合」地工作</li>
</ol>

<h2 id="参考文献">参考文献</h2>

<ol>
  <li>
    <p>Dibya Ghosh. “KL Divergence for Machine Learning”. <a href="https://dibyaghosh.com/blog/probability/kldivergence">https://dibyaghosh.com/blog/probability/kldivergence</a></p>
  </li>
  <li>
    <p>John Schulman. “Approximating KL Divergence”. <a href="https://joschu.net/blog/kl-approx.html">https://joschu.net/blog/kl-approx.html</a></p>
  </li>
  <li>
    <p>Verl Documentation. “Proximal Policy Optimization (PPO)”. <a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">https://verl.readthedocs.io/en/latest/algo/ppo.html</a></p>
  </li>
  <li>
    <p>初七123334. RLHF/RLVR 训练中的 KL 近似方法浅析（k1 / k2 / k3）. <a href="https://zhuanlan.zhihu.com/p/1966872846212010437">https://zhuanlan.zhihu.com/p/1966872846212010437</a></p>
  </li>
  <li>
    <p>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. “Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization”. <a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></p>
  </li>
  <li>
    <p>Yifan Zhang, Yiping Ji, Gavin Brown, et al. “On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning”. <a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></p>
  </li>
  <li>
    <p>Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro. “A Comedy of Estimators: On KL Regularization in RL Training of LLMs”. <a href="https://arxiv.org/abs/2512.21852">https://arxiv.org/abs/2512.21852</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[在强化学习中，KL 散度的估计方式直接影响训练稳定性。本文系统剖析三种经典估计器 k1, k2, k3 的性质差异，涵盖 on-policy 与 off-policy 两种场景，并给出「用于 loss 梯度回传」与「用于 reward 惩罚」时的选型指南。]]></summary></entry><entry xml:lang="en"><title type="html">From Two Policies to Three: Extending TRPO under Behavior–Reference Policy Mismatch in LLM RL</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html" rel="alternate" type="text/html" title="From Two Policies to Three: Extending TRPO under Behavior–Reference Policy Mismatch in LLM RL" /><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html"><![CDATA[<p><img src="/assets/img/three-policy/three-policy-mini-class-en.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<h2 id="traininginference-mismatch-and-asynchronous-frameworks">Training–Inference Mismatch and Asynchronous Frameworks</h2>

<p>Recently I’ve seen quite a lot of discussion around <em>training–inference mismatch</em> and <em>asynchronous RL frameworks</em> for large language models. My intuition is that many of these seemingly diverse and complicated issues are, in fact, manifestations of a more fundamental tension: a mismatch between the <strong>behavior policy</strong> and the <strong>reference policy</strong>.</p>

<p>In this post, I’ll first briefly summarize the related work I’ve come across, and then try to connect them through the lens of “behavior policy vs. reference policy,” as a complementary way to look at the problem.</p>

<p>Throughout the post I’ll use:</p>

<ul>
  <li>
    <p><strong>Behavior policy</strong> ⟦INLMATH48MATHEND⟧: the policy that <em>actually</em> generates rollouts, i.e., “under which distribution your data are sampled.” In modern LLM RL systems this typically corresponds to the implementation inside the inference engine (vLLM, SGLang, etc.), and under asynchronous frameworks it is often a <strong>mixture distribution over multiple worker policies</strong>.</p>
  </li>
  <li>
    <p><strong>Reference policy</strong> ⟦INLMATH49MATHEND⟧: the policy used in the training objective for importance sampling, clipping, or KL constraints — typically the “old policy” in PPO / GRPO.</p>
  </li>
  <li>
    <p><strong>Target policy</strong> ⟦INLMATH50MATHEND⟧: the policy we optimize in the training objective, i.e., “what we want the model to become” — typically the “new policy” in PPO / GRPO.</p>
  </li>
</ul>

<p>In the classical idealized setup, we usually <strong>implicitly assume</strong> ⟦INLMATH51MATHEND⟧. In real systems, however, asynchronous updates, different inference / training backends, MoE routing fluctuations, and even hardware-level numerical differences cause these two policies to deviate to varying degrees.</p>

<h2 id="related-work">Related Work</h2>

<p>Below is a rough timeline of the works that left a strong impression on me (this is only a partial and biased subset of the literature I’ve seen):</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2110.00641">Decoupled PPO</a> was among the first to point out that in trust-region policy optimization methods (TRPO and PPO), the “old policy” actually plays two distinct roles:
    <ol>
      <li>
        <p>It is used for importance sampling to perform off-policy correction. In this sense, the “old policy” is meant to represent the <strong>behavior policy</strong> that generated the training data.</p>
      </li>
      <li>
        <p>It is also used to limit the update step size of the new policy. In this sense, the “old policy” acts as a baseline to measure how much the new and old policies differ, i.e., a <strong>proximal policy</strong> (what I call the reference policy here).</p>
      </li>
    </ol>

    <p>The paper points out that these two roles do <em>not</em> have to be played by the same policy, and proposes the Decoupled PPO objective, which explicitly decouples “who generates the data” from “who defines the trust region” at the level of the optimization objective.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2505.24298">AReaL</a> focuses on the mismatch between behavior and reference policies under asynchronous training frameworks: rollouts are often generated by <strong>stale parameter versions</strong> or <strong>different workers</strong>. The paper adopts a Decoupled-PPO-style objective in the asynchronous setting, explicitly separating the behavior distribution from the reference policy, while still maintaining PPO-like optimization properties in this asynchronous regime.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2507.18071">GSPO</a> starts from stability issues of GRPO on long sequences and MoE models. It shows that token-level PPO / GRPO can become highly unstable when MoE expert routing is extremely volatile (especially when routing differs significantly between old and new policies), leading to large variance and training collapse. GSPO proposes a <strong>sequence-level</strong> PPO-style objective and ratio constraint, using the ratio over entire sequences to control updates. This substantially mitigates training collapse in MoE scenarios caused by routing instability and token-level noise.</p>
  </li>
  <li>
    <p><a href="https://fengyao.notion.site/off-policy-rl#28b721e3f6c480c3a756f8fb319e860d">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training</a> observes that in existing LLM RL frameworks (such as VeRL), the inference stack and the training stack often differ across multiple functional modules (e.g., vLLM vs. FSDP / Megatron kernels and operators). This makes the behavior policy ⟦INLMATH52MATHEND⟧ differ from the reference policy ⟦INLMATH53MATHEND⟧, so what is <em>assumed</em> to be on-policy training actually becomes off-policy training with nontrivial bias. The article summarizes two existing ways to handle this: PPO-IS and vanilla-IS, and further proposes <strong>token-level truncated importance sampling (TIS)</strong> to downweight samples with severe training–inference mismatch. The author also wrote two more foundational notes analyzing training–inference mismatch from basic principles: <a href="https://fengyao.notion.site/pg-seq-token-part1-basics">Part I</a> and <a href="https://fengyao.notion.site/pg-seq-token-part2-mismatch">Part II</a>.</p>
  </li>
  <li>
    <p><a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference">Defeating Nondeterminism in LLM Inference</a> points out that the lack of <strong>batch-size invariance</strong> is a core source of randomness in LLM inference: the same input can yield noticeably different probability distributions under different batch compositions and kernel paths. This means that even when you “nominally” have a single set of parameters, the <strong>behavior policy</strong> ⟦INLMATH54MATHEND⟧ realized in practice can fluctuate with system load and scheduling, further exacerbating training–inference mismatch.</p>
  </li>
  <li>
    <p><a href="https://ringtech.notion.site/icepop">Small Leak Can Sink a Great Ship—Boost RL Training on MoE with 𝑰𝒄𝒆𝑷𝒐𝒑!</a> observes that the above mismatch issues are further amplified in MoE models: routing itself is highly sensitive to small perturbations, and stacked with inference / training implementation differences and asynchronous sampling, it is easy to magnify bias and instability. The paper proposes IcePop: at the <strong>token level</strong>, it computes importance sampling ratios and applies <strong>two-sided masking</strong> to discard tokens whose ratios are either too large or too small. This removes “very noisy” data from the gradient, stabilizing RL training on MoE models.</p>
  </li>
  <li>
    <p><a href="https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda">When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch</a> gives a systematic analysis of the causes of training–inference mismatch, including large amounts of out-of-distribution and low-probability content introduced by agent workflows, hardware and kernel-level numerical uncertainty, and how <strong>token-level</strong> importance sampling can introduce severe bias on long sequences. It further proposes <strong>sequence-level</strong> masked importance sampling (sequence-level MIS): compute an IS ratio at the sequence level and discard only those sequences whose overall ratio is too large, thereby controlling bias while strongly suppressing training collapse caused by extreme samples. The paper provides reasonably complete theoretical derivations and extensive experimental evidence.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2510.11370">Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</a> focuses on the MoE-specific problem of <strong>routing inconsistency</strong>. The paper finds that even for identical inputs, inference and training can route tokens to different experts due to small differences in operator implementations or parallelism. This “physical-path” mismatch makes the gap between the behavior policy ⟦INLMATH55MATHEND⟧ and the reference policy ⟦INLMATH56MATHEND⟧ much larger than expected and can easily cause training collapse. To address this, the paper proposes <strong>Rollout Routing Replay (R3)</strong>: during rollout it records, for each token, the actual expert indices selected by the inference router, and during training it <strong>replays</strong> these routing decisions instead of recomputing them. In effect, R3 forces the training and inference stacks to share the same routing paths in the MoE topology, aligning the two sides at the level of the computation graph.</p>
  </li>
  <li>
    <p><a href="https://zhuanlan.zhihu.com/p/1959976628290590602">RL 老训崩？训推差异是基石</a> approaches the problem more from a practical perspective, sharing experience on how to engineer for near training–inference consistency: choosing consistent operators and precision settings, monitoring and constraining the log-prob gap between training and inference, etc. The focus is on framework-level engineering practices that can mitigate training–inference difference at the root.</p>
  </li>
  <li>
    <p><a href="https://verl.readthedocs.io/en/latest/algo/rollout_corr.html">verl Rollout Importance Sampling</a> introduces a <strong>Token Veto</strong> mechanism in its rollout correction module: it computes <strong>token-level</strong> importance ratios ⟦INLMATH57MATHEND⟧, and if any token in a trajectory satisfies ⟦INLMATH58MATHEND⟧, the entire sequence is discarded from training. This “token-level detection, sequence-level veto” design embodies a conservative “one-vote veto” strategy.</p>
  </li>
  <li><a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf">INTELLECT-3 Technical Report</a> adopts a similar rejection sampling strategy in its asynchronous distributed RL training framework. INTELLECT-3 computes <strong>token-level</strong> importance ratios for each rollout; if any token’s ratio falls below a threshold (⟦INLMATH59MATHEND⟧ in the paper), the entire trajectory is masked.</li>
</ul>

<h2 id="a-minimally-unified-view-from-a-three-policy-trpo-perspective">A Minimally Unified View from a Three-Policy TRPO Perspective</h2>

<p>At first glance, the works listed above seem to tackle different aspects:</p>

<ul>
  <li><strong>Algorithmic level</strong>: how to formulate PPO / GRPO objectives, token-level vs. sequence-level, clip vs. mask, etc.</li>
  <li><strong>Systems level</strong>: how to align inference and training stacks.</li>
  <li><strong>Model level</strong>: how MoE routing amplifies instability, and so on.</li>
</ul>

<p>However, if we align everything along a single axis — <strong>behavior policy vs. reference policy</strong> — a large fraction of these issues can be placed in a relatively simple theoretical framework: a <strong>three-policy TRPO</strong>.</p>

<p>In the next section I’ll unpack this three-policy TRPO in as simple math as I can. You can think of it as “TRPO + triangle inequality” — a very small extension conceptually, but surprisingly handy when analyzing training–inference mismatch in LLM RL:</p>

<ul>
  <li>On the one hand, it helps us understand what exactly “training–inference mismatch” and “asynchronous training frameworks” are harming within the TRPO view.</li>
  <li>On the other hand, it offers a unifying way to interpret TIS, IcePop, sequence-level MIS, etc. In the view of this post, they can all be seen as different incarnations of <strong>Constraint 2</strong> introduced below.</li>
</ul>

<h3 id="three-policies">Three Policies</h3>

<p>We stick to the notation from above and consider a discounted MDP with discount factor ⟦INLMATH60MATHEND⟧:</p>

<ul>
  <li>States ⟦INLMATH61MATHEND⟧, actions ⟦INLMATH62MATHEND⟧.</li>
  <li>Policy ⟦INLMATH63MATHEND⟧.</li>
  <li>Discounted state distribution:
⟦DISPMATH10MATHEND⟧</li>
  <li>Return (episodic view):
⟦DISPMATH11MATHEND⟧</li>
  <li>Value / Q / advantage functions:
⟦DISPMATH12MATHEND⟧</li>
</ul>

<p>It’s worth spelling out that in the three-policy setup we have:</p>

<ul>
  <li>
    <p><strong>Behavior policy</strong> ⟦INLMATH64MATHEND⟧: the policy that actually generates rollouts. Data ⟦INLMATH65MATHEND⟧ are sampled from it.</p>
  </li>
  <li>
    <p><strong>Reference policy</strong> ⟦INLMATH66MATHEND⟧: the “old policy” used in the optimization objective for importance sampling ratios, clipping, or KL constraints.</p>
  </li>
  <li>
    <p><strong>Target policy</strong> ⟦INLMATH67MATHEND⟧: the policy we are optimizing in this update.</p>
  </li>
</ul>

<p>In the ideal setup we assume ⟦INLMATH68MATHEND⟧; in real systems they are often unequal. This is the mathematical shadow of “training–inference mismatch.”</p>

<h3 id="two-policy-trpo">Two-Policy TRPO</h3>

<blockquote>
  <p>If you’re already familiar with TRPO, feel free to skip ahead to the “Three-Policy TRPO” subsection.</p>
</blockquote>

<p>All the theoretical guarantees in TRPO are stated <strong>with respect to the advantage function of some baseline policy</strong>. Since the only advantage we can estimate reliably in practice is ⟦INLMATH69MATHEND⟧ (data are sampled under ⟦INLMATH70MATHEND⟧), we may as well treat ⟦INLMATH71MATHEND⟧ as the baseline policy.</p>

<p>A classical result is the <strong>Performance Difference Lemma</strong>:</p>

<blockquote>
  <p>For any two policies ⟦INLMATH72MATHEND⟧ and ⟦INLMATH73MATHEND⟧, we have</p>

  <p>⟦DISPMATH1MATHEND⟧</p>
</blockquote>

<p>The intuition is simple:</p>

<ul>
  <li>⟦INLMATH74MATHEND⟧ says: “if I deviate from what ⟦INLMATH75MATHEND⟧ would do at state ⟦INLMATH76MATHEND⟧ and instead take action ⟦INLMATH77MATHEND⟧, how much will the long-term return change?”</li>
  <li>Summing that “gain” across all time steps, states, and actions gives the total improvement of the new policy over the behavior policy.</li>
</ul>

<p>The challenge in TRPO is that we cannot compute</p>

<p>⟦DISPMATH13MATHEND⟧</p>

<p>exactly, because ⟦INLMATH78MATHEND⟧ is the state distribution of the <em>new</em> policy, under which we do not have samples.</p>

<p>So TRPO introduces a surrogate objective by replacing the state distribution with that of the behavior policy:</p>

<p>⟦DISPMATH14MATHEND⟧</p>

<p>Intuitively, ⟦INLMATH79MATHEND⟧ asks the following question: “Under the states visited by the behavior policy, how good is the new policy if we just let it pick the actions?”</p>

<p>Starting from the Performance Difference Lemma, the difference between the true objective and the surrogate is:</p>

<p>⟦DISPMATH15MATHEND⟧</p>

<p>If we define</p>

<p>⟦DISPMATH16MATHEND⟧</p>

<p>we immediately get the following upper bound:</p>

<blockquote>
  <p><strong>Lemma 1</strong></p>

  <p>⟦DISPMATH2MATHEND⟧</p>
</blockquote>

<p>This reveals the first key quantity:</p>

<blockquote>
  <p><strong>State distribution shift</strong> ⟦INLMATH80MATHEND⟧, i.e., “how differently the new policy sees the world, compared to the behavior policy.”</p>
</blockquote>

<p>We usually do <em>not</em> directly impose constraints on ⟦INLMATH81MATHEND⟧. Instead, we constrain the per-timestep action distribution difference — via trust regions, KL penalties, clipping, etc.</p>

<p>Define the total variation (TV) distance:</p>

<p>⟦DISPMATH17MATHEND⟧</p>

<p>Assume there is a constant ⟦INLMATH82MATHEND⟧ such that</p>

<blockquote>
  <p>For all ⟦INLMATH83MATHEND⟧, the TV distance between the behavior and target policies is bounded:</p>

  <p>⟦DISPMATH3MATHEND⟧</p>
</blockquote>

<p>Intuitively: in any state, the action distribution of the “new policy” cannot deviate too much from that of the policy that generated the data.</p>

<p>A standard result (provable via coupling) is:</p>

<blockquote>
  <p><strong>Lemma 2</strong>
Under the assumption above,</p>

  <p>⟦DISPMATH4MATHEND⟧</p>
</blockquote>

<p>Combining Lemma 1 and Lemma 2, we obtain</p>

<p>⟦DISPMATH18MATHEND⟧</p>

<p>This gives a compact <strong>two-policy TRPO lower bound (baseline = behavior policy)</strong>:</p>

<blockquote>
  <p><strong>Theorem 1 (Two-Policy TRPO)</strong></p>

  <p>⟦DISPMATH5MATHEND⟧</p>
</blockquote>

<p>This suggests:</p>

<ul>
  <li><strong>What really matters for the tightness of ⟦INLMATH84MATHEND⟧ as a surrogate for ⟦INLMATH85MATHEND⟧ is how far the behavior policy ⟦INLMATH86MATHEND⟧ and the target policy ⟦INLMATH87MATHEND⟧ drift apart:</strong>
⟦DISPMATH19MATHEND⟧</li>
</ul>

<p>If you can directly control this ⟦INLMATH88MATHEND⟧, you can essentially port TRPO’s monotonic improvement guarantees to the behavior-policy view.</p>

<h3 id="three-policy-trpo">Three-Policy TRPO</h3>

<p>In practice, especially in large-scale LLM RL, <strong>we often cannot directly control ⟦INLMATH89MATHEND⟧ itself.</strong></p>

<p>In most PPO / GRPO / GSPO / RLHF-style frameworks, the actual situation is:</p>

<ul>
  <li>Rollout data are generated by some <strong>behavior policy</strong> ⟦INLMATH90MATHEND⟧ (some particular parameter version plus system details inside the inference engine).</li>
  <li>During updates, we would like to leverage a <strong>reference policy</strong> ⟦INLMATH91MATHEND⟧ to limit the update of the <strong>target policy</strong> ⟦INLMATH92MATHEND⟧.</li>
</ul>

<p>In other words, what we can actually touch and control are two quantities:</p>

<ol>
  <li>
    <p><strong>Reference vs. target</strong>: via KL penalties, clipping, etc., we constrain</p>

    <p>⟦DISPMATH20MATHEND⟧</p>
  </li>
  <li>
    <p><strong>Behavior vs. reference</strong>: we would <em>like</em> to keep
⟦DISPMATH21MATHEND⟧
small as well — this is where training–inference mismatch and asynchronous execution come in.</p>
  </li>
</ol>

<p>This motivates defining two “proxy gaps”:</p>

<ul>
  <li>
    <p><strong>Constraint 1: reference vs. target</strong></p>

    <p>⟦DISPMATH22MATHEND⟧</p>
  </li>
  <li>
    <p><strong>Constraint 2: behavior vs. reference</strong>
⟦DISPMATH23MATHEND⟧</p>
  </li>
</ul>

<p>Intuitively:</p>

<ul>
  <li>⟦INLMATH93MATHEND⟧: how far the new policy is from the “old policy” you are using in the loss — this is the trust-region part.</li>
  <li>⟦INLMATH94MATHEND⟧: how far the reference policy used in training is from the <em>actual</em> behavior policy that generated the data — this is the footprint of training–inference mismatch and asynchrony.</li>
</ul>

<p>Now we can plug these two quantities back into the TRPO lower bound.</p>

<p>For any state ⟦INLMATH95MATHEND⟧, by the triangle inequality we have</p>

<p>⟦DISPMATH24MATHEND⟧</p>

<p>Taking the supremum over ⟦INLMATH96MATHEND⟧ gives</p>

<p>⟦DISPMATH25MATHEND⟧</p>

<p>Plugging this inequality into the two-policy TRPO bound (Theorem 1), and denoting</p>

<p>⟦DISPMATH26MATHEND⟧</p>

<p>we obtain</p>

<p>⟦DISPMATH27MATHEND⟧</p>

<p>This yields a very direct <strong>three-policy TRPO lower bound</strong>:</p>

<blockquote>
  <p><strong>Theorem 2 (Three-Policy TRPO)</strong>
Let</p>

  <p>⟦DISPMATH6MATHEND⟧</p>

  <p>and</p>

  <p>⟦DISPMATH7MATHEND⟧</p>

  <p>Then for any target policy ⟦INLMATH97MATHEND⟧,</p>

  <p>⟦DISPMATH8MATHEND⟧</p>

  <p>where</p>

  <p>⟦DISPMATH9MATHEND⟧</p>
</blockquote>

<p>The meaning of this bound is quite straightforward:</p>

<ul>
  <li><strong>The gap between the surrogate objective ⟦INLMATH98MATHEND⟧ and the true performance ⟦INLMATH99MATHEND⟧ decomposes into two pieces:</strong>
    <ul>
      <li>The deviation between reference and target policies, ⟦INLMATH100MATHEND⟧.</li>
      <li>The deviation between behavior and reference policies, ⟦INLMATH101MATHEND⟧.</li>
    </ul>
  </li>
</ul>

<p>As long as both terms are small, <strong>optimizing ⟦INLMATH102MATHEND⟧ is likely to improve ⟦INLMATH103MATHEND⟧</strong>.</p>

<h3 id="how-to-control-these-two-deviations-in-practice">How to Control These Two Deviations in Practice?</h3>

<p>We can now revisit various practical methods through the lens of Theorem 2:</p>

<ul>
  <li>Most PPO / GRPO / GSPO-style work focuses on controlling <strong>Constraint 1: ⟦INLMATH104MATHEND⟧</strong>.</li>
  <li>Most TIS / IcePop / MIS-style work, in the view of this post, can be understood as primarily targeting <strong>Constraint 2: ⟦INLMATH105MATHEND⟧</strong>.</li>
</ul>

<p>In the remainder of this post I will focus on <strong>Constraint 2</strong>.</p>

<p>The goal of Constraint 2 is: <strong>ensure that the data used in training come (effectively) from a behavior policy that is close to the reference policy.</strong></p>

<p>In practice, this usually involves both <strong>system-level mechanisms</strong> and <strong>algorithmic mechanisms (importance sampling)</strong>.</p>

<ol>
  <li><strong>System level: keep the behavior policy from drifting too far</strong>
    <ul>
      <li>
        <p>Asynchronous frameworks:
Tag each sample with a policy version, and only use data generated by parameter versions that are close enough to ⟦INLMATH106MATHEND⟧.</p>
      </li>
      <li>
        <p>Training–inference alignment:
Use consistent precision, operators, and similar kernel behavior between the training and inference stacks.</p>
      </li>
    </ul>

    <p>These mechanisms act “outside” the algorithm to make ⟦INLMATH107MATHEND⟧ closer to ⟦INLMATH108MATHEND⟧, thereby shrinking ⟦INLMATH109MATHEND⟧.</p>
  </li>
  <li>
    <p><strong>Algorithmic level: sample-wise correction</strong></p>

    <p>At the algorithmic level, we no longer attempt to “fix” the entire behavior policy. Instead, we use importance sampling ratios to correct at the <strong>sample level</strong>: we filter or reweight samples so that the behavior policy is close to the reference policy <em>on the subset of data that actually participates in training</em>, or at least reduce the influence of samples with large mismatch.</p>

    <p>Concretely, this gives rise to methods like TIS, IcePop, and MIS, which can be seen as different ways of implementing Constraint 2 at the sample level.</p>
  </li>
</ol>

<h2 id="importance-sampling-and-masking-four-implementations-of-constraint-2">Importance Sampling and Masking: Four Implementations of Constraint 2</h2>

<p>In this section I’ll reuse the notation introduced above to write down the objectives of these three methods, focusing only on the design choices related to “behavior vs. reference policy.” Let the token-level PPO / GRPO-style update term be</p>

<p>⟦DISPMATH28MATHEND⟧</p>

<p>where</p>

<p>⟦DISPMATH29MATHEND⟧</p>

<p>Here:</p>

<ul>
  <li>⟦INLMATH110MATHEND⟧ is the <strong>target vs. reference</strong> ratio (corresponding to Constraint 1).</li>
  <li>⟦INLMATH111MATHEND⟧ is the advantage estimated from data sampled under the behavior policy.</li>
</ul>

<p>To connect token-level ⟦INLMATH112MATHEND⟧ with sequence-level ⟦INLMATH113MATHEND⟧ notation, consider the RLHF setting (reinforcement learning from human feedback) for LLMs:</p>

<ul>
  <li>Prompts are denoted by ⟦INLMATH114MATHEND⟧, and responses by ⟦INLMATH115MATHEND⟧.</li>
  <li>Token-level states and actions are defined as ⟦INLMATH116MATHEND⟧, ⟦INLMATH117MATHEND⟧.</li>
  <li>The behavior and reference policies on sequences can then be written as
⟦DISPMATH30MATHEND⟧</li>
</ul>

<p>To quantify the deviation between reference and behavior policies, we can define the token-level importance ratio:</p>

<p>⟦DISPMATH31MATHEND⟧</p>

<p>and its sequence-level counterpart:</p>

<p>⟦DISPMATH32MATHEND⟧</p>

<p>The difference between TIS, IcePop, and MIS lies in <strong>how they use ⟦INLMATH118MATHEND⟧ to implement Constraint 2</strong>.</p>

<h3 id="1-tis-token-level-truncated-importance-sampling">1. TIS: Token-Level Truncated Importance Sampling</h3>

<p>TIS directly truncates the token-level ratio ⟦INLMATH119MATHEND⟧; define</p>

<p>⟦DISPMATH33MATHEND⟧</p>

<p>The update objective becomes</p>

<p>⟦DISPMATH34MATHEND⟧</p>

<ul>
  <li>The blue ⟦INLMATH120MATHEND⟧ is the truncated IS weight: extremely large ratios are capped at a constant ⟦INLMATH121MATHEND⟧.</li>
  <li>From the three-policy TRPO perspective, this is a <em>soft</em> way to downweight tokens where behavior and reference policies differ significantly, effectively reducing their contribution to ⟦INLMATH122MATHEND⟧ in the gradient.</li>
</ul>

<h3 id="2-icepop-token-level-two-sided-masking-in-moe">2. IcePop: Token-Level Two-Sided Masking in MoE</h3>

<p>IcePop also uses ⟦INLMATH123MATHEND⟧ as a discrepancy measure, but opts for <strong>two-sided masking</strong>:</p>

<p>⟦DISPMATH35MATHEND⟧</p>

<p>The update objective becomes</p>

<p>⟦DISPMATH36MATHEND⟧</p>

<ul>
  <li>The blue ⟦INLMATH124MATHEND⟧ decides whether a token participates in the update: tokens with ratios that are too large or too small are dropped entirely.</li>
  <li>This is a <em>hard</em> sample selection scheme: only tokens where behavior and reference policies are reasonably aligned (ratios within ⟦INLMATH125MATHEND⟧) are kept, implementing a stricter version of Constraint 2 at the token level.</li>
</ul>

<h3 id="3-sequence-level-mis-masked-importance-sampling-over-entire-sequences">3. Sequence-Level MIS: Masked Importance Sampling Over Entire Sequences</h3>

<p>The core operation in sequence-level MIS is to <strong>retain only sequences whose sequence-level IS ratio is below a threshold ⟦INLMATH126MATHEND⟧</strong>, zeroing out the loss for all other sequences:</p>

<p>⟦DISPMATH37MATHEND⟧</p>

<p>In a unified loss form, this can be written as</p>

<p>⟦DISPMATH38MATHEND⟧</p>

<p>In words:</p>

<ul>
  <li>For <strong>sequences with small IS ratios</strong>, the full weight ⟦INLMATH127MATHEND⟧ is retained for off-policy correction.</li>
  <li>For <strong>sequences whose ratios exceed the threshold ⟦INLMATH128MATHEND⟧</strong>, the entire policy loss is masked out (weight set to ⟦INLMATH129MATHEND⟧).</li>
</ul>

<p>From the three-policy TRPO viewpoint, sequence-level MIS no longer truncates at the token level. Instead, it performs <strong>trajectory-level</strong> filtering: it drops trajectories where behavior and reference policies diverge too much, and only optimizes on the subset with ⟦INLMATH130MATHEND⟧. This implements Constraint 2 at the sequence level.</p>

<h3 id="4-worst-token-reject-sampling-rejecting-entire-sequences-based-on-the-worst-token">4. Worst Token Reject Sampling: Rejecting Entire Sequences Based on the Worst Token</h3>

<p>The verl Token Veto mechanism and INTELLECT-3 both adopt a rejection sampling strategy that can be collectively called <strong>Worst Token Reject Sampling (WTRS)</strong>:</p>

<ul>
  <li>
    <p><strong>verl Token Veto</strong>: In its rollout correction module, if any token in a trajectory has ⟦INLMATH131MATHEND⟧, the entire sequence is discarded via response*mask. The threshold ⟦INLMATH132MATHEND⟧ is user-configurable.</p>
  </li>
  <li>
    <p><strong>INTELLECT-3 Token Masking</strong>: In its asynchronous distributed RL framework, if any token’s ratio is below ⟦INLMATH133MATHEND⟧, the entire trajectory is masked.</p>
  </li>
</ul>

<p>The core operation is identical: <strong>if any token in a trajectory has an IS ratio below a threshold ⟦INLMATH134MATHEND⟧, the entire sequence is rejected from training.</strong> This can be written as:</p>

<p>⟦DISPMATH39MATHEND⟧</p>

<p>In a unified loss form:</p>

<p>⟦DISPMATH40MATHEND⟧</p>

<p>In words:</p>

<ul>
  <li>For <strong>sequences where all tokens have IS ratios ⟦INLMATH135MATHEND⟧</strong>: participate in training normally.</li>
  <li>For <strong>sequences where any token has an IS ratio ⟦INLMATH136MATHEND⟧</strong>: the entire sequence’s policy loss is masked out.</li>
</ul>

<p>From the three-policy TRPO perspective, WTRS adopts a hybrid “token-level detection, sequence-level veto” strategy: it detects extreme mismatch signals at the <strong>token level</strong>, and once detected, rejects at the <strong>sequence level</strong>. This “one-vote veto” design reflects a conservative philosophy — when a trajectory contains a token that “the behavior policy generated but the reference policy would almost never generate,” <strong>the credibility of the entire trajectory is called into question</strong>, thereby implementing control over Constraint 2 (⟦INLMATH137MATHEND⟧ vs. ⟦INLMATH138MATHEND⟧ deviation) at the trajectory granularity.</p>

<h2 id="moe-routing-replay-what-does-it-actually-do-in-three-policy-trpo">MoE Routing Replay: What Does It Actually Do in Three-Policy TRPO?</h2>

<p>In MoE (Mixture-of-Experts) models, training–inference mismatch often first appears as <strong>routing inconsistency</strong>: even with identical parameters, the inference and training stacks may route tokens to different experts because of small differences in operators, parallelism, or numerics. A natural engineering response is <strong>routing replay</strong>: during rollout (inference), record the actual expert paths, and during training, force the model to reuse these routing decisions.</p>

<p>These methods are often intuitively described as “implementing Constraint 2 and shrinking ⟦INLMATH139MATHEND⟧.” From the three-policy TRPO perspective, a more precise statement is:</p>

<blockquote>
  <p><strong>Routing replay does not tighten the original surrogate objective via a constraint; instead, it rewrites the surrogate objective into one that is conditioned on / replaces the routing.</strong>
It makes routing mismatch invisible in the loss, but it does not actually shrink the true policy distances ⟦INLMATH140MATHEND⟧ or ⟦INLMATH141MATHEND⟧.</p>
</blockquote>

<p>Below I’ll sketch a <strong>minimal</strong> abstraction that is sufficient to make this concrete.</p>

<h3 id="surrogate-objective-in-moe-separating-routing-and-token-generation">Surrogate Objective in MoE: Separating Routing and Token Generation</h3>

<p>Abstract an MoE model as a two-stage stochastic decision: “first choose an expert ⟦INLMATH142MATHEND⟧, then generate token ⟦INLMATH143MATHEND⟧ conditioned on that expert.” The target policy can be factorized as</p>

<p>⟦DISPMATH41MATHEND⟧</p>

<p>where:</p>

<ul>
  <li>⟦INLMATH144MATHEND⟧ is the router distribution.</li>
  <li>⟦INLMATH145MATHEND⟧ is the token distribution conditioned on expert ⟦INLMATH146MATHEND⟧.</li>
</ul>

<p>In the three-policy TRPO setting, the surrogate objective we actually want to optimize can be written as</p>

<p>⟦DISPMATH42MATHEND⟧</p>

<p>where I use</p>

<p>⟦DISPMATH43MATHEND⟧</p>

<p>to denote the expert-level aggregation of advantages.</p>

<p>The key point is that <strong>in the original ⟦INLMATH147MATHEND⟧, the routing distribution is precisely the current router ⟦INLMATH148MATHEND⟧ that we are updating</strong>. In other words, RL on MoE is updating not only the token-generation distribution but also the router itself.</p>

<h3 id="1-replaying-behavior-policy-routing-behavior-router-replay--r3-style">(1) Replaying Behavior-Policy Routing (Behavior-Router Replay / R3-Style)</h3>

<p>R3-style methods record, during rollout, the set of experts ⟦INLMATH149MATHEND⟧ actually selected by the behavior policy on the inference side, and during training force the current policy to <strong>route only within this set</strong>. This can be written as a “conditional projection” of the routing distribution:</p>

<p>⟦DISPMATH44MATHEND⟧</p>

<p>The surrogate objective that is actually optimized during training becomes</p>

<p>⟦DISPMATH45MATHEND⟧</p>

<p>Compared to the original ⟦INLMATH150MATHEND⟧, R3 does <em>not</em> push ⟦INLMATH151MATHEND⟧ closer to ⟦INLMATH152MATHEND⟧ or ⟦INLMATH153MATHEND⟧. Instead, it:</p>

<ul>
  <li><strong>replaces the expectation over ⟦INLMATH154MATHEND⟧ by a conditional expectation over ⟦INLMATH155MATHEND⟧</strong>, and</li>
  <li>equivalently, <strong>shrinks the feasible routing support to ⟦INLMATH156MATHEND⟧</strong>.</li>
</ul>

<p>So R3 is optimizing a “behavior-routing-conditioned surrogate objective,” rather than the original ⟦INLMATH157MATHEND⟧. The benefit is substantially reduced variance and improved stability; the cost is that <strong>the router’s exploration and update freedom is constrained at every state</strong>.</p>

<h3 id="2-replaying-reference-policy-routing-reference-router-replay">(2) Replaying Reference-Policy Routing (Reference-Router Replay)</h3>

<p>Another class of routing-replay schemes instead reuses the reference policy’s router ⟦INLMATH158MATHEND⟧. This is equivalent to training a hybrid policy</p>

<p>⟦DISPMATH46MATHEND⟧</p>

<p>with surrogate objective</p>

<p>⟦DISPMATH47MATHEND⟧</p>

<p>This has the effect that:</p>

<ul>
  <li>In the surrogate objective, the router is <strong>frozen to the old router</strong> ⟦INLMATH159MATHEND⟧, so the “reference vs. target” discrepancy in routing is simply removed from the loss.</li>
  <li>Training becomes insensitive to how far the <em>new</em> router ⟦INLMATH160MATHEND⟧ drifts from ⟦INLMATH161MATHEND⟧, thereby sidestepping the instabilities caused by routing mismatch.</li>
</ul>

<p>Again, this is fundamentally a <strong>change of objective</strong>:</p>

<ul>
  <li>The deviation ⟦INLMATH162MATHEND⟧ in the true policy space is not reduced; it is merely rendered invisible by redefining the surrogate in terms of the old router.</li>
  <li>Learning of the router is effectively frozen or heavily suppressed.</li>
</ul>

<h3 id="routing-replay-as-a-change-of-surrogate-objective">Routing Replay as a Change of Surrogate Objective</h3>

<p>Putting these replay variants side by side, they share several properties:</p>

<ol>
  <li><strong>They optimize not the original ⟦INLMATH163MATHEND⟧, but a surrogate where routing has been conditioned or replaced.</strong></li>
  <li><strong>They do not directly shrink the three-policy TRPO bound’s ⟦INLMATH164MATHEND⟧ or ⟦INLMATH165MATHEND⟧</strong>. Routing mismatch is removed from the loss, but it still exists in the true policy distances.</li>
  <li><strong>In practice they trade bias for variance</strong>: replay typically lowers variance and improves stability, but may also limit the router’s ability to learn routing patterns that are optimal for the RL objective.</li>
</ol>

<p>So, in the three-policy TRPO view, a more accurate characterization is:</p>

<blockquote>
  <p><strong>Routing replay is best thought of as a rewrite of the surrogate objective, not as a direct implementation of a constraint on ⟦INLMATH166MATHEND⟧ or ⟦INLMATH167MATHEND⟧.</strong></p>
</blockquote>

<h2 id="conclusion">Conclusion</h2>

<p>If I had to compress this post into a single sentence, it would be:</p>

<blockquote>
  <p><strong>Many issues around “training–inference mismatch” and “asynchronous training” in large-scale LLM RL can be understood, in the TRPO framework, as severely underestimating the deviation between the behavior policy ⟦INLMATH168MATHEND⟧ and the reference policy ⟦INLMATH169MATHEND⟧ — i.e., the term ⟦INLMATH170MATHEND⟧.</strong></p>
</blockquote>

<p>From two policies to three, what we did is conceptually very small:</p>

<ul>
  <li>
    <p>We rewrote the TRPO lower bound from an “old vs. new policy” narrative into a “<strong>behavior–reference–target</strong>” three-policy relationship.</p>
  </li>
  <li>We explicitly separated two TV distances:
    <ul>
      <li><strong>Constraint 1: reference vs. target</strong>, ⟦INLMATH171MATHEND⟧, corresponding to the KL / clip / trust-region style constraints in PPO / GRPO / GSPO.</li>
      <li><strong>Constraint 2: behavior vs. reference</strong>, ⟦INLMATH172MATHEND⟧, capturing real-world factors like asynchronous frameworks, training–inference mismatch, MoE routing volatility, kernel-level nondeterminism, etc.</li>
    </ul>
  </li>
  <li>This leads to a simple conclusion:
The gap between the surrogate ⟦INLMATH173MATHEND⟧ and the true performance ⟦INLMATH174MATHEND⟧ scales with ⟦INLMATH175MATHEND⟧.</li>
</ul>

<p>Under this lens (which is of course only one of many possible perspectives):</p>

<ul>
  <li>
    <p>Decoupled PPO / AReaL can be viewed as <strong>formally acknowledging the existence of three policies</strong> and explicitly decoupling the behavior distribution from the reference policy in the objective.</p>
  </li>
  <li>TIS, IcePop, MIS, and WTRS can be seen as different ways of implementing <strong>Constraint 2</strong> using importance sampling truncation / masking:
    <ul>
      <li>TIS: token-level truncation of IS weights to soften the influence of extreme samples.</li>
      <li>IcePop: token-level two-sided masking in MoE to hard-drop tokens with severe mismatch.</li>
      <li>MIS: sequence-level masking to ignore entire trajectories whose behavior–reference mismatch is too large.</li>
      <li>WTRS: token-level detection of extremely small ratios, rejecting the entire trajectory once such a signal is found.</li>
    </ul>
  </li>
  <li>
    <p><strong>Routing replay</strong> (whether replaying behavior routing in R3-style schemes or replaying reference routing) is better viewed as <strong>changing the surrogate objective</strong> rather than directly implementing a constraint: both variants replace the original ⟦INLMATH176MATHEND⟧ with a routing-conditioned / routing-frozen surrogate, trading off some objective bias and reduced routing learning freedom for lower variance and greater stability, without actually shrinking ⟦INLMATH177MATHEND⟧ or ⟦INLMATH178MATHEND⟧—they simply make routing mismatch invisible in the loss.</p>
  </li>
  <li>Engineering advice such as in <em>RL 老训崩？训推差异是基石</em> and system-level work like <em>Defeating Nondeterminism in LLM Inference</em> can be interpreted as efforts to <strong>reduce ⟦INLMATH179MATHEND⟧ on the systems and numerical side</strong>, so that the assumptions underlying the algorithms do not break too badly.</li>
</ul>

<p>From this unified perspective, it may also be easier to think about the following practical questions (these are completely open and I don’t have definitive answers):</p>

<ul>
  <li>
    <p>Under what conditions can we still reasonably interpret “LLM RL training” as some approximate form of TRPO / PPO?</p>
  </li>
  <li>For a concrete RL system, where should we invest more effort:
    <ul>
      <li>tightening ⟦INLMATH180MATHEND⟧ (stronger KL control, more stable sequence-level objectives), or</li>
      <li>reducing ⟦INLMATH181MATHEND⟧ (better training–inference alignment, more aggressive MIS / TIS / IcePop)?</li>
    </ul>
  </li>
  <li>In the presence of MoE, asynchronous sampling, and complex agent workflows, how long can we safely pretend that “⟦INLMATH182MATHEND⟧”?</li>
</ul>

<p>This post is just a very <strong>minimal</strong> extension of the classic TRPO framework, making the “three policies” explicit and using them to organize some existing work. There are inevitably misunderstandings and omissions. If you also care about how RL training actually behaves in large LLM systems, I’d be very interested to see how your own setup can be abstracted into a relationship between ⟦INLMATH183MATHEND⟧, ⟦INLMATH184MATHEND⟧, and ⟦INLMATH185MATHEND⟧, and then re-examined through the inequality in Theorem 2. It might give a slightly different intuitive feel for what your system is really optimizing.</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025ThreePolicyTRPO</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{From Two Policies to Three: Extending TRPO under Behavior-Reference Policy Mismatch in LLM RL}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html}</span><span class="p">,</span>
  <span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-11-23}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[Modern LLM RL pipelines often train under an "old policy" that silently drifts away from the behavior policy that actually generates rollouts, breaking the usual on-policy assumptions. This post rewrites the classic TRPO lower bound in a three-policy form — behavior, reference, and target — so that the performance gap cleanly decomposes into two TV distances that we can reason about and control. Seen through this lens, methods like Decoupled PPO, AReaL, TIS, IcePop, sequence-level MIS, Worst Token Reject Sampling (WTRS), MoE routing replay, and common engineering tricks for training–inference alignment all become different ways of shrinking these two deviations.]]></summary></entry><entry xml:lang="zh"><title type="html">从两策略到三策略：LLM RL 中行为策略–参考策略不一致下的 TRPO 扩展</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-zh.html" rel="alternate" type="text/html" title="从两策略到三策略：LLM RL 中行为策略–参考策略不一致下的 TRPO 扩展" /><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-zh</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-zh.html"><![CDATA[<p><img src="/assets/img/three-policy/three-policy-mini-class-zh.jpg" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<h2 id="训推不一致和异步框架">训推不一致和异步框架</h2>

<p>最近看到不少关于大模型强化学习中”训推不一致”和”异步训推框架”的讨论。我的直觉是：这些看似复杂多样的问题，很大程度上都围绕着一个更基础的矛盾——<strong>行为策略（behavior policy）和参考策略（reference policy）不一致。</strong></p>

<p>本文先简单梳理一下我目前看到的相关工作，然后尝试从”行为策略 vs 参考策略”的角度，把它们串到同一条线上，为读者提供一个补充视角。</p>

<p>在本文中，我将使用以下记号：</p>

<ul>
  <li><strong>行为策略</strong> ⟦INLMATH48MATHEND⟧：实际负责生成 rollout 的策略，即”在什么分布下采样到了这些数据”。在现代 LLM-RL 系统中，它对应推理引擎里的实现（vLLM / SGLang 等），在异步框架下往往还是<strong>多个 worker 策略的混合分布</strong>。</li>
  <li><strong>参考策略</strong> ⟦INLMATH49MATHEND⟧：训练目标中用于重要性采样、clipping 或 KL 约束的策略，典型的就是 PPO / GRPO 里的”旧策略”（old policy）。</li>
  <li><strong>目标策略</strong> ⟦INLMATH50MATHEND⟧：训练目标中要优化的策略，即”希望模型变成什么样”。典型的就是 PPO / GRPO 里的”新策略”（new policy）。</li>
</ul>

<p>在最经典、理想化的设定中，我们通常<strong>默认</strong> ⟦INLMATH51MATHEND⟧。但在现实系统中，受异步更新、不同推理/训练后端、MoE 路由波动甚至硬件数值差异等因素影响，二者往往会出现不同程度的偏离。</p>

<h2 id="相关工作">相关工作</h2>

<p>下面按时间线简要列举一些我印象较深的工作（仅代表个人看到的片面子集）：</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2110.00641">Decoupled PPO</a> 率先指出，在信赖域策略优化（TRPO 和 PPO）方法中，”旧策略”（old policy）实际上承担了两个不同的角色：一是用于重要性采样以进行异策略修正，此时”旧策略”代表训练数据集所服从的行为策略（behavior policy）；二是用于限制新策略的更新幅度，此时”旧策略”被用于衡量新旧策略的变化程度，称为近端策略（proximal policy，对应本文中的”参考策略”）。文章指出这两个目的下的”旧策略”可以是不同的策略，从而提出了 Decoupled PPO 更新目标，将”采样用谁”和”对谁做 trust region”在形式上解耦开来。</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2505.24298">AReaL</a> 关注到异步训练框架下行为策略与参考策略不一致的问题：rollout 往往由滞后的参数版本或不同 worker 产生。文章在异步框架下采用了 Decoupled PPO 风格的目标，将”行为策略分布”和”参考策略”显式区分开来，从而在异步场景下仍能维持类似 PPO 的优化性质。</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2507.18071">GSPO</a> 从 GRPO 在长序列和 MoE 模型上的稳定性问题出发，指出 token-level 的 PPO / GRPO 在专家路由高度波动（尤其是新旧策略之间的路由差异）时，会引入巨大的方差与不稳定。GSPO 提出在 <strong>sequence-level</strong> 定义 PPO-style 目标与比率约束，用整条序列的比率来约束更新，从而在 MoE 场景下显著缓解由路由不一致带来的训练崩溃。</p>
  </li>
  <li>
    <p><a href="https://fengyao.notion.site/off-policy-rl#28b721e3f6c480c3a756f8fb319e860d">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training</a> 关注到现有的一些大模型强化学习训练框架（如 VeRL）中，推理框架和训练框架在不少相同功能模块上有不同实现（例如 vLLM 和 FSDP / Megatron 等算子差异），导致行为策略 ⟦INLMATH52MATHEND⟧ 与参考策略 ⟦INLMATH53MATHEND⟧ 不一致。这种不一致使得原本假定为同策略（on-policy）的训练，实际上变成了带有明显偏差的异策略（off-policy）训练。文章总结了两种处理这一问题的现有方法：PPO-IS 与 vanilla-IS，并提出在 <strong>token-level</strong> 进行截断重要性采样（truncated IS, TIS），以减少训推不一致程度较重的样本在训练中的影响。作者还写了两篇更基础的分析文章，从原理上分析训推不一致问题：<a href="https://fengyao.notion.site/pg-seq-token-part1-basics">Part I</a> 和 <a href="https://fengyao.notion.site/pg-seq-token-part2-mismatch">Part II</a>。</p>
  </li>
  <li>
    <p><a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference">Defeating Nondeterminism in LLM Inference</a> 指出，批处理大小不变性（batch-size invariance）的缺失是大模型推理框架随机性的核心来源之一：同一个输入在不同的 batch 组合和 kernel 路径下，得到的概率分布会发生可观差异。这意味着，即便”名义上”是同一套参数，真实运行时的行为策略 ⟦INLMATH54MATHEND⟧ 也会因系统负载和调度差异而波动，从而进一步加剧训推不一致。</p>
  </li>
  <li>
    <p><a href="https://ringtech.notion.site/icepop">Small Leak Can Sink a Great Ship—Boost RL Training on MoE with 𝑰𝒄𝒆𝑷𝒐𝒑!</a> 观察到，上述训推不一致问题在 MoE 模型上会进一步加剧：路由本身就对微小扰动高度敏感，再叠加推理/训练实现差异和异步采样，很容易放大偏差。文章提出 IcePop 方法：在 <strong>token-level</strong> 通过计算重要性采样比率，对过大或过小的比率进行双侧掩码（masking），将这些”噪声较大”的数据从梯度中丢弃，从而稳定 MoE 上的 RL 训练。</p>
  </li>
  <li>
    <p><a href="https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Capse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda">When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch</a> 系统性分析了训推不一致的各种成因，包括智能体工作流中引入的大量分布外和低概率信息、硬件和内核/kernel 实现带来的计算不确定性，并分析了在 <strong>token-level</strong> 进行重要性采样如何在长序列上引入严重偏差。文章进一步提出在 <strong>sequence-level</strong> 计算重要性采样掩码（sequence-level masked IS, sequence-level MIS）：只丢弃那些整条序列重要性采样比率过大的数据，从而在控制偏差的同时，显著抑制由极端样本导致的训练崩溃。文中给出了较为完整的理论推导和丰富的实验支撑。</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2510.11370">Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</a> 聚焦于 MoE 架构下特有的 <strong>路由不一致（Routing Inconsistency）</strong> 问题。文章发现，推理端和训练端即便在输入完全相同的情况下，由于算子实现或并行的微小差异，Router 选中的专家往往不同。这种”物理路径”上的不一致，使得行为策略 ⟦INLMATH55MATHEND⟧ 和参考策略 ⟦INLMATH56MATHEND⟧ 之间的差异远超预期，极易导致训练崩溃。文章提出了 <strong>Rollout Routing Replay (R3)</strong>：在推理阶段记录每个 token 实际命中的专家索引，并在训练阶段<strong>强制回放</strong>这些路由决策，不再重新计算。通过这种方式，R3 在 MoE 拓扑结构上强制对齐了训推两端的计算路径。</p>
  </li>
  <li>
    <p><a href="https://zhuanlan.zhihu.com/p/1959976628290590602">RL 老训崩？训推差异是基石</a> 更多从实践角度出发，分享了如何在实现上尽可能靠近”训推一致”的经验，包括如何选用一致的算子和精度配置、如何监控与约束训练端和推理端 log-prob 的偏差等，更着力于从训推框架层面入手，在工程上尽量从根本缓解训推差异问题。</p>
  </li>
  <li>
    <p><a href="https://verl.readthedocs.io/en/latest/algo/rollout_corr.html">verl Rollout Importance Sampling</a> 在其 rollout correction 模块中引入了 Token Veto（一票否决）机制：在 <strong>token-level</strong> 计算重要性比率 ⟦INLMATH57MATHEND⟧，若轨迹中存在任意 token 使得 ⟦INLMATH58MATHEND⟧，则将整条序列从训练中剔除。这种”token 粒度检测、sequence 粒度否决”的设计体现了一种”一票否决”的保守策略。</p>
  </li>
  <li>
    <p><a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf">INTELLECT-3 Technical Report</a> 在其异步分布式 RL 训练框架中采用了类似的拒绝采样策略。INTELLECT-3 对每条 rollout 计算 <strong>token-level</strong> 重要性比率，若任意 token 的比率低于阈值（文中使用 ⟦INLMATH59MATHEND⟧），则对整条轨迹进行 masking。</p>
  </li>
</ul>

<h2 id="三策略-trpo-视角下的最小统一理解">三策略 TRPO 视角下的最小统一理解</h2>

<p>上面列举的这些工作，看似各自解决不同层面的问题：</p>

<ul>
  <li>算法层：PPO / GRPO 的目标怎么写，token-level 还是 sequence-level，用 clip 还是 mask；</li>
  <li>系统层：推理框架和训练框架如何对齐；</li>
  <li>模型层：MoE 模型路由问题如何放大训练不稳定，等等。</li>
</ul>

<p>但如果我们把”行为策略 vs 参考策略”这条线拉直，会发现相当一部分问题其实都可以纳入一个相对简单的理论框架来理解：<strong>三策略 TRPO</strong>。</p>

<p>下面我将用尽量简洁的数学，把这个三策略版 TRPO 展开——它可以看作是”TRPO + 三角不等式”的一个小扩展，但在分析大模型 RL 中的训推不一致时非常好用：</p>

<ul>
  <li>一方面帮助我们重新理解”训推不一致”和”异步训练框架”到底在影响什么；</li>
  <li>另一方面，也帮助我们统一理解 TIS、IcePop、sequence-level MIS 等方法，在本文的视角下，它们其实都是在实施下文的”<strong>约束 2</strong>“。</li>
</ul>

<h3 id="三个策略">三个策略</h3>

<p>沿用前文的记号，我们在一个折扣 MDP 上工作，折扣因子为 ⟦INLMATH60MATHEND⟧：</p>

<ul>
  <li>状态 ⟦INLMATH61MATHEND⟧，动作 ⟦INLMATH62MATHEND⟧；</li>
  <li>策略 ⟦INLMATH63MATHEND⟧；</li>
  <li>折扣状态分布：
⟦DISPMATH10MATHEND⟧</li>
  <li>回报（episode 视角）：
⟦DISPMATH11MATHEND⟧</li>
  <li>值函数 / 优势函数：
⟦DISPMATH12MATHEND⟧</li>
</ul>

<p>简单说明一下，在”三策略”设定中，我们有：</p>

<ul>
  <li><strong>行为策略</strong>（behavior policy）：⟦INLMATH64MATHEND⟧，真正用来 rollout 的策略；数据 ⟦INLMATH65MATHEND⟧ 都来自它。</li>
  <li><strong>参考策略</strong>（reference policy）：⟦INLMATH66MATHEND⟧，优化目标中用来做 ratio、clip 或 KL 约束的那份”旧策略”。</li>
  <li><strong>目标策略</strong>（target policy）：⟦INLMATH67MATHEND⟧，我们这一步要优化的策略。</li>
</ul>

<p>在理想设定中，我们通常默认 ⟦INLMATH68MATHEND⟧；但在现实系统里，二者往往不等，这就是”训推不一致”的数学体现。</p>

<h3 id="两策略-trpo">两策略 TRPO</h3>

<blockquote>
  <p>熟悉 TRPO 的读者可以直接跳到后面的“三策略 TRPO”小节。</p>
</blockquote>

<p>TRPO 的所有理论保证，都建立在<strong>某个”基准策略”的优势函数</strong>之上。既然实际能算清楚的<strong>只有</strong> ⟦INLMATH69MATHEND⟧（数据是按 ⟦INLMATH70MATHEND⟧ 采的），我们就直接把 ⟦INLMATH71MATHEND⟧ 当作基准。</p>

<p>一个经典的结论是 <strong>性能差分引理（Performance Difference Lemma）</strong>：</p>

<blockquote>
  <p>对任意两策略 ⟦INLMATH72MATHEND⟧ 和 ⟦INLMATH73MATHEND⟧，有</p>

  <p>⟦DISPMATH1MATHEND⟧</p>
</blockquote>

<p>直觉很简单：</p>

<ul>
  <li>⟦INLMATH74MATHEND⟧ 表示”如果在状态 ⟦INLMATH75MATHEND⟧ 中本来按 ⟦INLMATH76MATHEND⟧ 行动，现在换成动作 ⟦INLMATH77MATHEND⟧，长期回报会增加或减少多少”；</li>
  <li>把所有时刻、所有状态、所有动作的”增益”累积起来，就得到新策略比行为策略总共多赚了多少。</li>
</ul>

<p>TRPO 的问题在于，我们无法准确计算</p>

<p>⟦DISPMATH13MATHEND⟧</p>

<p>因为 ⟦INLMATH78MATHEND⟧ 是”新策略”的状态分布，我们没有在这个分布下采样过。</p>

<p>于是 TRPO 引入了一个替代目标：将状态分布换成行为策略的：</p>

<p>⟦DISPMATH14MATHEND⟧</p>

<p>⟦INLMATH79MATHEND⟧ 的直觉解释是：在行为策略的状态分布下，让新策略试着去选动作，看优势有多大。</p>

<p>从性能差分引理出发，两者之差是：</p>

<p>⟦DISPMATH15MATHEND⟧</p>

<p>如果我们定义</p>

<p>⟦DISPMATH16MATHEND⟧</p>

<p>那么有一个直接的上界：</p>

<blockquote>
  <p><strong>Lemma 1</strong></p>

  <p>⟦DISPMATH2MATHEND⟧</p>
</blockquote>

<p>这里出现了第一个关键量：</p>

<blockquote>
  <p><strong>状态分布偏移</strong> ⟦INLMATH80MATHEND⟧，也就是“新策略和行为策略看到的世界，到底差了多少”。</p>
</blockquote>

<p>我们通常不会直接对 ⟦INLMATH81MATHEND⟧ 施加约束，反而是对“每一步 action 分布”的差异施加约束，比如 trust region、KL、clip 等。</p>

<p>记总变差距离（total variation）：</p>

<p>⟦DISPMATH17MATHEND⟧</p>

<p>假设存在常数 ⟦INLMATH82MATHEND⟧，使得</p>

<blockquote>
  <p>对所有 ⟦INLMATH83MATHEND⟧，行为策略和目标策略之间的 TV 被 ⟦INLMATH84MATHEND⟧ 上界：</p>

  <p>⟦DISPMATH3MATHEND⟧</p>
</blockquote>

<p>直观含义：在任意状态里，“新策略”和“生成数据的策略”选动作的分布都不会离太远。</p>

<p>一个经典结果（可以用 coupling 证明）是：</p>

<blockquote>
  <p><strong>Lemma 2</strong>
在上述条件下有</p>

  <p>⟦DISPMATH4MATHEND⟧</p>
</blockquote>

<p>把它和 Lemma 1 结合：</p>

<p>⟦DISPMATH18MATHEND⟧</p>

<p>于是我们得到一个形式上相当简洁的<strong>两策略 TRPO 下界（基准为行为策略）</strong>：</p>

<blockquote>
  <p><strong>Theorem 1（两策略 TRPO）</strong></p>

  <p>⟦DISPMATH5MATHEND⟧</p>
</blockquote>

<p>这说明：</p>

<ul>
  <li><strong>真正决定“替代目标 ⟦INLMATH85MATHEND⟧ 靠不靠谱”的，是行为策略 ⟦INLMATH86MATHEND⟧ 和目标策略 ⟦INLMATH87MATHEND⟧ 的差异：</strong>
⟦DISPMATH19MATHEND⟧</li>
</ul>

<p>如果你能直接约束住这个 ⟦INLMATH88MATHEND⟧，就能直接把 TRPO 的单调性保证搬到行为策略视角下。</p>

<h3 id="三策略-trpo">三策略 TRPO</h3>

<p>现实问题在于：<strong>在大模型强化学习训练中，我们可能无法直接控制 ⟦INLMATH89MATHEND⟧ 本身。</strong></p>

<p>在大部分 PPO / GRPO / GSPO 及现有 RLHF 框架中，实际发生的是：</p>

<ul>
  <li>rollout 数据由某个<strong>行为策略</strong> ⟦INLMATH90MATHEND⟧ 产生（推理引擎中的”那一版参数”加上若干系统细节）；</li>
  <li>更新时，我们希望利用<strong>参考策略</strong> ⟦INLMATH91MATHEND⟧ 来限制<strong>目标策略</strong> ⟦INLMATH92MATHEND⟧ 的更新幅度。</li>
</ul>

<p>也就是说，实际可以“动手”的是两个量：</p>

<ol>
  <li><strong>参考 vs 目标</strong>：我们可以通过 KL / clip 等手段控制
⟦DISPMATH20MATHEND⟧</li>
  <li><strong>行为 vs 参考</strong>：我们希望<strong>间接</strong>控制
⟦DISPMATH21MATHEND⟧</li>
</ol>

<p>于是自然就定义两个“proxy 差异”：</p>

<ul>
  <li><strong>约束 1：参考 vs 目标</strong>
⟦DISPMATH22MATHEND⟧</li>
  <li><strong>约束 2：行为 vs 参考</strong>
⟦DISPMATH23MATHEND⟧</li>
</ul>

<p>直觉上：</p>

<ul>
  <li>⟦INLMATH93MATHEND⟧：新策略离”你宣称的那份旧策略”有多远——这就是 trust region 控制的部分；</li>
  <li>⟦INLMATH94MATHEND⟧：用于训练的参考策略，与真实采样时的行为策略相差多少——这就是训推不一致或异步的影子。</li>
</ul>

<p>现在，我们可以把这两个量代回 TRPO 的下界中。</p>

<p>对任意状态 ⟦INLMATH95MATHEND⟧，有</p>

<p>⟦DISPMATH24MATHEND⟧</p>

<p>对 ⟦INLMATH96MATHEND⟧ 取上确界：</p>

<p>⟦DISPMATH25MATHEND⟧</p>

<p>把这个不等式塞回两策略 TRPO 的结论（Theorem 1）里，记</p>

<p>⟦DISPMATH26MATHEND⟧</p>

<p>即得到：</p>

<p>⟦DISPMATH27MATHEND⟧</p>

<p>于是，我们得到一个非常直接的<strong>三策略 TRPO 下界</strong>：</p>

<blockquote>
  <p><strong>Theorem 2（三策略 TRPO）</strong>
记</p>

  <p>⟦DISPMATH6MATHEND⟧</p>

  <p>以及</p>

  <p>⟦DISPMATH7MATHEND⟧</p>

  <p>则对任意目标策略 ⟦INLMATH97MATHEND⟧ 有</p>

  <p>⟦DISPMATH8MATHEND⟧</p>

  <p>其中</p>

  <p>⟦DISPMATH9MATHEND⟧</p>
</blockquote>

<p>这个结论的含义其实很直接：</p>

<ul>
  <li><strong>替代目标 ⟦INLMATH98MATHEND⟧ 与真实性能 ⟦INLMATH99MATHEND⟧ 之间的 gap，可以拆成两部分：</strong>
    <ul>
      <li>参考 vs 目标的偏移 ⟦INLMATH100MATHEND⟧；</li>
      <li>行为 vs 参考的偏移 ⟦INLMATH101MATHEND⟧。</li>
    </ul>
  </li>
</ul>

<p>只要这两个量都小，<strong>优化 ⟦INLMATH102MATHEND⟧ 就有希望有效提升 ⟦INLMATH103MATHEND⟧</strong>。</p>

<h3 id="这两个差异各自怎么约束">这两个差异各自怎么约束？</h3>

<p>现在，我们可以从 Theorem 2 回头看各种实际方法：</p>

<ul>
  <li>绝大多数 “PPO / GRPO / GSPO” 类工作，其实是在控制 <strong>约束 1：⟦INLMATH104MATHEND⟧</strong>；</li>
  <li>绝大多数 “TIS / IcePop / MIS” 类工作，在本文的统一视角下，可以理解为主要是在控制 <strong>约束 2：⟦INLMATH105MATHEND⟧</strong>。</li>
</ul>

<p>本文下面只讨论 <strong>约束 2</strong>。</p>

<p>约束 2 的目标是：<strong>保证用来训练的数据，尽可能来自“接近参考策略”的行为策略。</strong></p>

<p>这里通常既有<strong>系统层</strong>的机制，也有<strong>算法层（importance sampling）</strong>的机制。</p>

<ol>
  <li><strong>系统层：让行为策略别飘太远</strong>
    <ul>
      <li>异步框架：给每个样本打上策略版本号，只能用与 ⟦INLMATH106MATHEND⟧ 相差不大的参数版本采样的数据；</li>
      <li>训推对齐：强调训练框架和推理框架用相同精度、相同算子、相近的内核 / kernel 行为。</li>
    </ul>

    <p>这些机制的目标是：从“算法外部”让 ⟦INLMATH107MATHEND⟧ 和 ⟦INLMATH108MATHEND⟧ 靠近，从而压缩 ⟦INLMATH109MATHEND⟧。</p>
  </li>
  <li>
    <p><strong>算法层：样本修正</strong></p>

    <p>在算法层，我们不再试图“纠正整个行为策略”，而是用重要性采样比率在<strong>样本层面</strong>做筛选和重加权，让“真正参与训练的样本子集”上的行为策略尽量接近参考策略，或者减小差异较大的样本在训练上的权重。</p>

    <p>具体来说，就是下面这些方法，它们本质上都可以看作是“实现约束 2 的不同方式”。</p>
  </li>
</ol>

<h2 id="重要性采样与掩码四种约束-2-实现">重要性采样与掩码：四种约束 2 实现</h2>

<p>下面延续前文的记号体系来写这三种方法的目标函数，只聚焦在“行为策略 vs 参考策略”这一维的设计。记 token 级的 PPO / GRPO 风格更新项为</p>

<p>⟦DISPMATH28MATHEND⟧</p>

<p>其中</p>

<p>⟦DISPMATH29MATHEND⟧</p>

<p>也就是说：</p>

<ul>
  <li>⟦INLMATH110MATHEND⟧ 是 <strong>目标 vs 参考</strong> 的比率（对应约束 1）；</li>
  <li>⟦INLMATH111MATHEND⟧ 基于行为策略采样的数据，是我们能估到的优势函数。</li>
</ul>

<p>为了把 token 级的 ⟦INLMATH112MATHEND⟧ 与序列级的 ⟦INLMATH113MATHEND⟧ 记号打通，在以 RLHF（reinforcement learning from human feedback，人类反馈强化学习）为代表的 LLM-RL 设定中，我们约定：</p>

<ul>
  <li>prompt 记为 ⟦INLMATH114MATHEND⟧；回复记为 ⟦INLMATH115MATHEND⟧；</li>
  <li>token 级状态 ⟦INLMATH116MATHEND⟧，动作 ⟦INLMATH117MATHEND⟧；</li>
  <li>因此行为策略和参考策略在序列上的分布可写成
⟦DISPMATH30MATHEND⟧</li>
</ul>

<p>此外，为了描述“参考 vs 行为”的偏移，统一定义 token 级重要性比率</p>

<p>⟦DISPMATH31MATHEND⟧</p>

<p>以及其对应的序列级版本</p>

<p>⟦DISPMATH32MATHEND⟧</p>

<p>接下来，TIS / IcePop / MIS 的区别，就体现在“如何利用这些 ⟦INLMATH118MATHEND⟧ 来实现约束 2”。</p>

<h3 id="1-tistoken-level-截断-is">1. TIS：token-level 截断 IS</h3>

<p>TIS 直接对上述 ⟦INLMATH119MATHEND⟧ 做截断，记</p>

<p>⟦DISPMATH33MATHEND⟧</p>

<p>更新目标写成</p>

<p>⟦DISPMATH34MATHEND⟧</p>

<ul>
  <li>蓝色的 ⟦INLMATH120MATHEND⟧ 是被截断的 IS 权重：极端大的比率被压到常数 ⟦INLMATH121MATHEND⟧。</li>
  <li>从三策略 TRPO 的角度看，这相当于在 <strong>token 分布</strong> 上“软削弱”行为策略和参考策略严重不一致的样本，从而在梯度中有效减小那部分样本对 ⟦INLMATH122MATHEND⟧ 的贡献。</li>
</ul>

<h3 id="2-icepopmoe-场景下的-token-level-双侧-mask">2. IcePop：MoE 场景下的 token-level 双侧 Mask</h3>

<p>IcePop 同样以 ⟦INLMATH123MATHEND⟧ 为度量，但采用 <strong>双侧掩码</strong>：</p>

<p>⟦DISPMATH35MATHEND⟧</p>

<p>更新目标写成</p>

<p>⟦DISPMATH36MATHEND⟧</p>

<ul>
  <li>蓝色的 ⟦INLMATH124MATHEND⟧ 决定某个 token 是否参与更新：比率太大或太小的 token 直接被丢弃。</li>
  <li>这相当于硬性裁掉“行为策略和参考策略极度不一致”的 token，只在 ⟦INLMATH125MATHEND⟧ 适中的区域上优化，从样本集合层面实施更强的“约束 2”。</li>
</ul>

<h3 id="3-sequence-level-mis按整条序列-mask-的重要性采样">3. sequence-level MIS：按整条序列 Mask 的重要性采样</h3>

<p>MIS 的核心操作是：<strong>只保留 IS 比率不超过阈值 ⟦INLMATH126MATHEND⟧ 的序列，其余序列的损失直接置零</strong>。写成</p>

<p>⟦DISPMATH37MATHEND⟧</p>

<p>在统一的损失形式下，可以写成</p>

<p>⟦DISPMATH38MATHEND⟧</p>

<p>简而言之：</p>

<ul>
  <li>对于 <strong>IS 比率较小的序列</strong>：保留完整的 ⟦INLMATH127MATHEND⟧ 权重，正常做 off-policy 修正；</li>
  <li>对于 <strong>IS 比率超过阈值 ⟦INLMATH128MATHEND⟧ 的序列</strong>：整个序列的 policy loss 被 mask 掉（权重变成 ⟦INLMATH129MATHEND⟧）。</li>
</ul>

<p>从三策略 TRPO 的角度看，MIS 不再在 token 上做截断，而是直接在<strong>序列级</strong>筛掉“行为策略和参考策略严重不一致”的轨迹，只在 ⟦INLMATH130MATHEND⟧ 的子分布上优化，从而在 trajectory 粒度上实现对“约束 2”（⟦INLMATH131MATHEND⟧ vs ⟦INLMATH132MATHEND⟧ 偏移）的控制。</p>

<h3 id="4-worst-token-reject-sampling按最差-token-拒绝整条序列">4. Worst Token Reject Sampling：按最差 token 拒绝整条序列</h3>

<p>verl 中的 veto 机制 与 INTELLECT-3 分别在各自的训练框架中采用了一种可统称为 <strong>Worst Token Reject Sampling（WTRS）</strong> 的拒绝采样策略：</p>

<ul>
  <li>
    <p><strong>verl Token Veto</strong>：在其 rollout correction 模块中，若轨迹中存在任意 token 使得 ⟦INLMATH133MATHEND⟧，则通过 response*mask 将整条序列剔除。阈值 ⟦INLMATH134MATHEND⟧ 可由用户配置。</p>
  </li>
  <li>
    <p><strong>INTELLECT-3 Token Masking</strong>：在其异步分布式 RL 框架中，若任意 token 的比率低于 ⟦INLMATH135MATHEND⟧，则对整条轨迹进行 masking。</p>
  </li>
</ul>

<p>二者的核心操作一致：<strong>若轨迹中存在任意 token 的 IS 比率低于阈值 ⟦INLMATH136MATHEND⟧，则将整条序列从训练中剔除</strong>。写成</p>

<p>⟦DISPMATH39MATHEND⟧</p>

<p>在统一的损失形式下，可以写成</p>

<p>⟦DISPMATH40MATHEND⟧</p>

<p>简而言之：</p>

<ul>
  <li>对于 <strong>所有 token 的 IS 比率均不低于 ⟦INLMATH137MATHEND⟧ 的序列</strong>：正常参与训练；</li>
  <li>对于 <strong>存在任意 token 的 IS 比率低于 ⟦INLMATH138MATHEND⟧ 的序列</strong>：整条序列的 policy loss 被 mask 掉。</li>
</ul>

<p>从三策略 TRPO 的角度看，WTRS 采用了”token 粒度检测、sequence 粒度否决”的混合策略：在 <strong>token-level</strong> 检测极端不一致的信号，一旦发现则在 <strong>sequence-level</strong> 执行拒绝。这种”一票否决”的设计体现了一种保守思路——当轨迹中存在”行为策略生成但参考策略几乎不可能生成”的 token 时，<strong>整条轨迹的可信度都将受到质疑</strong>，从而在 trajectory 粒度上实现对”约束 2”（⟦INLMATH139MATHEND⟧ vs ⟦INLMATH140MATHEND⟧ 偏移）的控制。</p>

<h2 id="moe-路由回放它在三策略-trpo-中到底做了什么">MoE 路由回放：它在三策略 TRPO 中到底做了什么？</h2>

<p>在 MoE（Mixture-of-Experts）模型上，训推不一致往往首先表现为<strong>路由不一致（routing inconsistency）</strong>：即便参数相同，推理端与训练端也可能因为算子、并行或数值细节的微小差异而路由到不同专家。一个很自然的工程应对是<strong>路由回放（routing replay）</strong>：在 rollout（推理）时记录实际命中的专家路径，训练时强制复用这些路由决策。</p>

<p>这类方法经常被直觉性地理解为“在实现约束 2、压小 ⟦INLMATH141MATHEND⟧”。但从三策略 TRPO 的视角看，更准确的说法是：</p>

<blockquote>
  <p><strong>路由回放并不是在原 surrogate objective 上收紧约束，而是在把 surrogate objective 改写成另一个“带路由条件/替换”的目标。</strong>
它让路由不一致在 loss 里“不可见”，但并没有让真实策略距离里的 ⟦INLMATH142MATHEND⟧ 或 ⟦INLMATH143MATHEND⟧ 变小。</p>
</blockquote>

<p>下面用一个<strong>尽量简单</strong>但足够说明问题的建模来把这件事写清楚。</p>

<h3 id="moe-下的-surrogate-objective把路由和token-生成拆开">MoE 下的 surrogate objective：把“路由”和“token 生成”拆开</h3>

<p>把 MoE 抽象成两阶段随机决策：“先选专家 ⟦INLMATH144MATHEND⟧，再在该专家条件下生成 token ⟦INLMATH145MATHEND⟧”。
因此目标策略可以分解为</p>

<p>⟦DISPMATH41MATHEND⟧</p>

<p>其中：</p>

<ul>
  <li>⟦INLMATH146MATHEND⟧ 是路由器（router）的分布；</li>
  <li>⟦INLMATH147MATHEND⟧ 是在专家 ⟦INLMATH148MATHEND⟧ 条件下的 token 分布。</li>
</ul>

<p>在三策略 TRPO 中，我们真正想优化的 surrogate objective 为</p>

<p>⟦DISPMATH42MATHEND⟧</p>

<p>其中我把专家层的优势聚合写成</p>

<p>⟦DISPMATH43MATHEND⟧</p>

<p>关键点：<strong>在原始的 ⟦INLMATH149MATHEND⟧ 里，路由分布是当前要更新的 ⟦INLMATH150MATHEND⟧</strong>。也就是说，MoE 的 RL 训练不仅在更新 token 生成分布，也在更新路由器本身。</p>

<h3 id="1回放行为策略的路由behavior-router-replay--r3-类">1）回放行为策略的路由（behavior-router replay / R3 类）</h3>

<p>R3 的做法是：rollout 时记录推理端实际命中的专家集合 ⟦INLMATH151MATHEND⟧，训练时强制当前策略<strong>只在该集合内路由</strong>。可以把它写成对路由分布的“条件化投影”：</p>

<p>⟦DISPMATH44MATHEND⟧</p>

<p>从而训练时实际优化的 surrogate objective 变为</p>

<p>⟦DISPMATH45MATHEND⟧</p>

<p>和原始 ⟦INLMATH152MATHEND⟧ 对比可以看到，R3 并没有让 ⟦INLMATH153MATHEND⟧ 逼近 ⟦INLMATH154MATHEND⟧ 或 ⟦INLMATH155MATHEND⟧；它做的是：</p>

<ul>
  <li><strong>把对 ⟦INLMATH156MATHEND⟧ 的期望，改成了对 ⟦INLMATH157MATHEND⟧ 的条件期望</strong>；</li>
  <li>等价地说，把路由的可行 support 缩到了 ⟦INLMATH158MATHEND⟧。</li>
</ul>

<p>因此 R3 训练的是一个“被行为路由集合条件化后的 surrogate objective”，而不是原来的 ⟦INLMATH159MATHEND⟧。
好处是显著降方差、提升稳定性；代价是<strong>在每个状态上都收缩了路由器探索 / 更新的自由度</strong>。</p>

<h3 id="2回放参考策略的路由reference-router-replay">2）回放参考策略的路由（reference-router replay）</h3>

<p>另一类 routing replay 复用的是参考策略（old policy）的路由器 ⟦INLMATH160MATHEND⟧。这等价于训练一个混合策略</p>

<p>⟦DISPMATH46MATHEND⟧</p>

<p>对应 surrogate objective 为</p>

<p>⟦DISPMATH47MATHEND⟧</p>

<p>这意味着：</p>

<ul>
  <li>在 surrogate objective 中，路由器被<strong>固定为旧路由器</strong>，路由相关的“参考 vs 目标”差异在 loss 里被直接抹掉；</li>
  <li>训练对“新路由器 ⟦INLMATH161MATHEND⟧ 是否偏离 ⟦INLMATH162MATHEND⟧”不再敏感，于是路由不一致导致的不稳定被绕开。</li>
</ul>

<p>但注意这同样是<strong>换目标</strong>：</p>

<ul>
  <li>真实策略空间里的 ⟦INLMATH163MATHEND⟧ 并没有因此变小，只是被“用旧路由器重定义目标”而在 loss 中不可见；</li>
  <li>路由器的学习被强行冻结或极度削弱。</li>
</ul>

<h3 id="路由回放只是在改写-surrogate-objective">路由回放只是在改写 surrogate objective</h3>

<p>把两类 replay 放在一起看，它们的共同点是：</p>

<ol>
  <li><strong>优化的都不是原始的 ⟦INLMATH164MATHEND⟧</strong>，而是某个“路由被条件化 / 替换后的 surrogate objective”。</li>
  <li><strong>它们没有直接收缩三策略 TRPO 下界里的 ⟦INLMATH165MATHEND⟧</strong>。replay 让路由不匹配不再显式出现在 loss 中，但不匹配在真实策略距离里仍然存在。</li>
  <li><strong>实践上是在“用偏差换方差”</strong>：回放往往显著降低方差、提升稳定性，但也可能限制了 MoE 在 RL 目标下学到更优的路由模式。</li>
</ol>

<p>所以，从三策略 TRPO 的视角，更准确的理解是：</p>

<blockquote>
  <p><strong>routing replay 是一种 surrogate objective 的改写，而不是对 ⟦INLMATH166MATHEND⟧ 或 ⟦INLMATH167MATHEND⟧ 的直接实现。</strong></p>
</blockquote>

<h2 id="小结">小结</h2>

<p>如果把这篇文章压缩成一句话，那就是：</p>

<blockquote>
  <p><strong>许多”大模型 RL 训推不一致”和”异步训练”问题，在本文的视角下，其实都可以理解为：在 TRPO 框架下，当行为策略 ⟦INLMATH168MATHEND⟧ 和参考策略 ⟦INLMATH169MATHEND⟧ 不一致时，二者之间的偏移（⟦INLMATH170MATHEND⟧）被严重低估了。</strong></p>
</blockquote>

<p>从两策略到三策略，我们做的事情其实很简单：</p>

<ul>
  <li>将 TRPO 的下界从”旧策略 vs 新策略”的叙述，改写成”<strong>行为策略 – 参考策略 – 目标策略</strong>“三者的关系；</li>
  <li>显式地拆出了两个 TV 距离：
    <ul>
      <li><strong>约束 1：参考 vs 目标</strong> ⟦INLMATH171MATHEND⟧，对应 PPO / GRPO / GSPO 等工作中最常见的 KL / clip / trust region；</li>
      <li><strong>约束 2：行为 vs 参考</strong> ⟦INLMATH172MATHEND⟧，对应异步框架、训推差异、MoE 路由、kernel 非确定性等现实因素；</li>
    </ul>
  </li>
  <li>得到了一个非常直接的结论：
替代目标 ⟦INLMATH173MATHEND⟧ 与真实性能 ⟦INLMATH174MATHEND⟧ 的差距正比于 ⟦INLMATH175MATHEND⟧。</li>
</ul>

<p>在这个视角下（当然这只是众多可能视角之一）：</p>

<ul>
  <li>Decoupled PPO / AReaL 可以被看作是在<strong>形式上承认“三策略存在”</strong>，并尝试在目标函数上将“行为分布”和“参考策略”解耦；</li>
  <li>TIS、IcePop、MIS、WTRS 则是通过 IS 或者掩码机制在样本层面实施”约束 2”：
    <ul>
      <li>TIS：用 token-level 截断权重削弱比率过大样本的影响；</li>
      <li>IcePop：在 MoE 场景下用 token-level 双侧掩码硬性丢弃”极端不一致”的 token；</li>
      <li>MIS：在 sequence-level 直接屏蔽整条”比率过大”的轨迹；</li>
      <li>WTRS：在 token-level 检测比率过小的信号，一旦发现则在 sequence-level 拒绝整条轨迹；</li>
    </ul>
  </li>
  <li><strong>routing replay（路由回放）在三策略 TRPO 的视角下更像是“改写 surrogate objective”而非“直接实现约束”</strong>：无论回放行为路由（R3 类）还是回放参考路由，它们都把原本的 ⟦INLMATH176MATHEND⟧ 改成了一个路由被条件化/替换后的 surrogate objective，用<strong>一定的目标偏差与路由学习自由度的收缩</strong>换取<strong>降低方差与提升稳定性</strong>。因此它并不会真正收缩 ⟦INLMATH177MATHEND⟧ 或 ⟦INLMATH178MATHEND⟧，而是让路由不一致在 loss 中“不可见”；</li>
  <li>《RL 老训崩？训推差异是基石》、以及前文提到的 <em>Defeating Nondeterminism in LLM Inference</em> 等工程经验，则可以理解为在<strong>系统侧和数值实现侧</strong>，尽可能把 ⟦INLMATH179MATHEND⟧ 压低，让算法层的假设不至于完全失效。</li>
</ul>

<p>从这个统一视角出发，也许有助于回答几个实际问题（这里只是抛几个开放性问题）：</p>

<ul>
  <li>在什么条件下，我们还能把“大模型 RL 训练”理解成某种意义上的“近似 TRPO / PPO”？</li>
  <li>对一个具体的 RL 系统，我们究竟应该把主要精力花在：
    <ul>
      <li>收紧 ⟦INLMATH180MATHEND⟧（更强的 KL / 更稳的 sequence-level 目标），还是</li>
      <li>压低 ⟦INLMATH181MATHEND⟧（更一致的训推框架、更激进的 MIS / TIS / IcePop）？</li>
    </ul>
  </li>
  <li>在 MoE、异步采样、复杂 agent workflow 这些现实设定下，我们还能安全地假装“⟦INLMATH182MATHEND⟧”多久？</li>
</ul>

<p>本文只是在 TRPO 这个老框架上做了一个非常“<strong>最小化</strong>”的延展，把“三策略”显式写出来，并用它来整理现有的一些工作。难免有理解偏差或遗漏之处，如果你也关注实际大模型 RL 训练的情况，欢迎把你自己的设定抽象成“⟦INLMATH183MATHEND⟧ 三者的关系”，再回头看看 Theorem 2 里的那条不等式，或许会有不一样的直观感受。</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025ThreePolicyTRPO</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{From Two Policies to Three: Extending TRPO under Behavior-Reference Policy Mismatch in LLM RL}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy-en.html}</span><span class="p">,</span>
  <span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-11-23}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[现代 LLM RL 流程中，"旧策略"常常悄然偏离实际生成 rollout 的行为策略，破坏了通常的同策略假设。本文将经典的 TRPO 下界改写为三策略形式——行为策略、参考策略和目标策略——使得性能差距可以分解为两个可推理、可控制的 TV 距离。在这一视角下，Decoupled PPO、AReaL、TIS、IcePop、sequence-level MIS、最坏 Token 拒绝采样 (WTRS)、MoE 路由回放等方法，以及常见的训推对齐工程技巧，都可以看作是缩小这两个偏差的不同实现方式。]]></summary></entry></feed>