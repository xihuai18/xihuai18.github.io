<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://xihuai18.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xihuai18.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-11-15T11:29:52+00:00</updated><id>https://xihuai18.github.io/feed.xml</id><title type="html">Xihuai Wang’s Page</title><subtitle>Xihuai&apos;s personal page.
</subtitle><entry><title type="html">从两策略到三策略：采样策略不一致下的 PPO 扩展</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/01/15/three-policy.html" rel="alternate" type="text/html" title="从两策略到三策略：采样策略不一致下的 PPO 扩展" /><published>2025-01-15T00:00:00+00:00</published><updated>2025-01-15T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/01/15/three-policy</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/01/15/three-policy.html"><![CDATA[]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[在大模型强化学习中，因为推理框架和训练框架的不一致，以及异步训练框架下采样策略的多样性，采样策略与参考策略不一致的问题变得尤为突出。本文分析了采样策略与参考策略不一致问题在 TRPO 框架下的影响，并在这一分析基础上梳理了当前对这一问题的不同解决方法。]]></summary></entry></feed>