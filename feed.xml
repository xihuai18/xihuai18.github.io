<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://xihuai18.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xihuai18.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-11-15T17:52:30+00:00</updated><id>https://xihuai18.github.io/feed.xml</id><title type="html">Xihuai Wang’s Page</title><subtitle>Xihuai&apos;s personal page.
</subtitle><entry><title type="html">从两策略到三策略：行为策略和参考策略不一致下的 TRPO 扩展</title><link href="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy.html" rel="alternate" type="text/html" title="从两策略到三策略：行为策略和参考策略不一致下的 TRPO 扩展" /><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy</id><content type="html" xml:base="https://xihuai18.github.io/reinforcement-learning/2025/11/15/three-policy.html"><![CDATA[<h2 id="训推不一致和异步框架">训推不一致和异步框架</h2>

<p>最近看到不少关于大模型强化学习中“训推不一致”和“异步训推框架”的讨论，我自己的直觉是：这些看上去复杂多样的问题，很大一部分其实都围绕着一个更基础的矛盾——<strong>行为策略（behavior policy）和参考策略（reference policy）不一致。</strong></p>

<p>本文先简单梳理一下我目前看到的相关工作，然后再尝试从“行为策略 vs 参考策略”的角度，把它们串到同一条线上，为读者提供一个补充视角。</p>

<p>在本文中我会用：</p>

<ul>
  <li><strong>行为策略</strong> $\mu$：实际负责生成 rollout 的策略，也就是“你在什么分布下采样到了这些数据”。在现代 LLM-RL 系统里，它对应的是推理引擎里的那套实现（vLLM / SGLang 等），在异步框架下往往还是<strong>多个 worker 策略的混合分布</strong>。</li>
  <li><strong>参考策略</strong> $\pi_{\theta_{\text{old}}}$：训练目标里拿来做重要性采样、clipping 或 KL 约束的策略，典型地就是 PPO / GRPO 里的“旧策略”（old policy）。</li>
  <li><strong>目标策略</strong> $\pi_\theta$：训练目标里要优化的策略，也就是“你想让模型变成什么样”。典型地就是 PPO / GRPO 里的“新策略”（new policy）。</li>
</ul>

<p>在最经典、理想化的设定里，我们通常<strong>默认</strong> $\mu = \pi_{\theta_{\text{old}}}$。但在现实系统中，受异步更新、不同推理 / 训练后端、MoE 路由波动甚至硬件数值差异等因素影响，二者往往会出现不同程度的偏离。</p>

<h2 id="相关工作">相关工作</h2>

<p>下面按时间线简单列一下我印象比较深的一些工作（只代表我个人看到的片面子集）：</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/2110.00641">Decoupled PPO</a> 率先指出，在信赖域策略优化（TRPO 和 PPO）方法中，“旧策略”（old policy）实际承担了两个不同的角色：一是用于重要性采样进行异策略修正，在这个目的下，“旧策略”用于代表训练数据集所服从的行为策略（behavior policy）；二是用于限制新策略的更新幅度，在这个目的下，“旧策略”被用于衡量新旧策略的变化程度，称作近端策略（proximal policy，对应本文中的“参考策略”）。文章指出这两个目的下的“旧策略”可以是不同的策略，从而提出了 Decoupled PPO 更新目标，把“采样用谁”和“对谁做 trust region”在形式上解耦开来。</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2505.24298">AReaL</a> 关注到了异步训练框架下行为策略与参考策略不一致的问题：rollout 往往由滞后的参数版本或不同 worker 产生。文章在异步框架下采用了 Decoupled PPO 风格的目标，将“行为策略分布”和“参考策略”显式区分开来，从而在异步场景下仍然维持类似 PPO 的优化性质。</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2507.18071">GSPO</a> 从 GRPO 在长序列和 MoE 模型上的稳定性问题出发，指出 token-level 的 PPO / GRPO 在专家路由高度波动（尤其是新旧策略之间的路由差异）时，会引入巨大的方差与不稳定。GSPO 提出在 <strong>sequence-level</strong> 定义 PPO-style 目标与比率约束，用整条序列的比率来约束更新，从而在 MoE 场景下显著缓解由路由不一致带来的训练崩溃问题。</p>
  </li>
  <li>
    <p><a href="https://fengyao.notion.site/off-policy-rl#28b721e3f6c480c3a756f8fb319e860d">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training</a> 关注到了现有的一些大模型强化学习训练框架（如 VeRL）中，推理框架和训练框架在不少相同的功能模块上有不同的实现（例如 vLLM 和 FSDP / Megatron 等算子上的差异），导致行为策略 $\mu$ 与参考策略 $\pi_{\theta_{\text{old}}}$ 不一致。这种不一致使得原本假定为同策略（on-policy）的训练，实际上变成了带有明显偏差的异策略（off-policy）训练。文章总结了两种处理这一问题的现有方法：PPO-IS 与 vanilla-IS，并提出在 <strong>token-level</strong> 做截断重要性采样（truncated IS, TIS），以减少训推不一致程度较重的样本在训练中的影响。作者还写了两篇更为基础的分析文章，从原理上分析训推不一致问题：<a href="https://fengyao.notion.site/pg-seq-token-part1-basics">Part I</a> 和 <a href="https://fengyao.notion.site/pg-seq-token-part2-mismatch">Part II</a>。</p>
  </li>
  <li>
    <p><a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference">Defeating Nondeterminism in LLM Inference</a> 指出，批处理大小不变性（batch-size invariance）的缺失是大模型推理框架随机性的核心来源之一：同一个输入在不同的 batch 组合和 kernel 路径下，得到的概率分布会发生可观差异。这意味着，即便“名义上”是同一套参数，真实运行时的行为策略 $\mu$ 也会因为系统负载和调度差异而波动，从而进一步加剧训推不一致。</p>
  </li>
  <li>
    <p><a href="https://ringtech.notion.site/icepop">Small Leak Can Sink a Great Ship—Boost RL Training on MoE with 𝑰𝒄𝒆𝑷𝒐𝒑!</a> 观察到，上述训推不一致问题在 MoE 模型上会进一步加剧：路由本身就对微小扰动高度敏感，再叠加推理 / 训练实现差异和异步采样，很容易放大偏差。文章提出 IcePop 方法：在 <strong>token-level</strong> 通过计算重要性采样比率，对过于大或者过于小的比率进行双侧掩码（masking），将这些“噪声较大”的数据从梯度中丢弃，从而稳定 MoE 上的 RL 训练。</p>
  </li>
  <li>
    <p><a href="https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda">When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch</a> 系统性分析了训推不一致的各种成因，包括智能体工作流中引入的大量分布外和低概率信息、硬件和内核 / kernel 实现带来的计算不确定性，并分析了在 <strong>token-level</strong> 进行重要性采样如何在长序列上引入严重的偏差。文章进一步提出在 <strong>sequence-level</strong> 计算重要性采样掩码（sequence-level masked IS, sequence-level MIS）：只丢弃那些整条序列的重要性采样比率过大的数据，从而在控制偏差的同时，显著抑制由极端样本导致的训练崩溃。文中给出了较为完整的理论推导和丰富的实验支撑。</p>
  </li>
  <li>
    <p><a href="https://zhuanlan.zhihu.com/p/1959976628290590602">RL老训崩？训推差异是基石</a> 则更多从实践角度出发，分享了如何在实现上尽可能靠近“训推一致”的经验，包括如何选用一致的算子和精度配置、如何监控与约束训练端和推理端 log-prob 的偏差等，更着力于从训推框架层面入手，在工程上尽量从根本缓解训推差异问题。</p>
  </li>
</ul>

<h2 id="三策略-trpo-视角下的最小统一理解">三策略 TRPO 视角下的最小统一理解</h2>

<p>上面列的这些工作，看上去各自解决的是：</p>

<ul>
  <li>算法层：PPO / GRPO 的目标怎么写，token-level 还是 sequence-level，用 clip 还是 mask；</li>
  <li>系统层：推理框架和训练框架怎样对齐；</li>
  <li>模型层：MoE 模型路由问题如何放大训练不稳定，等等。</li>
</ul>

<p>但如果我们把“行为策略 vs 参考策略”这条线拉直，会发现相当一部分问题，其实都可以放到一个相对简单的理论框架里理解：<strong>三策略 TRPO</strong>。</p>

<p>下面这节我会用尽量简单的数学，把这个三策略版 TRPO 摊开——它可以被看作是“TRPO + 三角不等式”的一个小扩展，但在分析大模型 RL 里的训推不一致时非常好用：</p>

<ul>
  <li>一方面让我们重新理解“训推不一致”和“异步训练框架”到底在影响什么；</li>
  <li>另一方面，也帮我们统一理解 TIS、IcePop、sequence-level MIS 等，在本文的视角下，它们其实都是在实施下文的“<strong>约束 2</strong>”。</li>
</ul>

<h3 id="三个策略">三个策略</h3>

<p>沿用前文的记号，我们在一个折扣 MDP 上工作，折扣因子为 $\gamma\in(0,1)$：</p>

<ul>
  <li>状态 $s\in\mathcal{S}$，动作 $a\in\mathcal{A}$；</li>
  <li>策略 $\pi(a\mid s)$；</li>
  <li>折扣状态分布：
\(d_\pi(s) := (1-\gamma)\sum_{t=0}^\infty \gamma^t \Pr_\pi(s_t = s)。\)</li>
  <li>回报（episode 视角）：
\(\mathcal{J}(\pi) := \mathbb{E}_\pi\Big[\sum_{t=0}^\infty \gamma^t r_t\Big]。\)</li>
  <li>值函数 / 优势函数：
\(V_\pi(s),\quad Q_\pi(s,a),\quad A_\pi(s,a) := Q_\pi(s,a) - V_\pi(s)。\)</li>
</ul>

<p>稍微赘述一下，在“三策略”设定里，我们有：</p>

<ul>
  <li><strong>行为策略</strong>（behavior policy）：$\mu$，真正用来 rollout 的策略；数据 $(s,a,r,\dots)$ 都是从它来的。</li>
  <li><strong>参考策略</strong>（reference policy）：$\pi_{\theta_{\text{old}}}$，优化目标里拿来做 ratio、clip 或 KL 约束的那一份“旧策略”。</li>
  <li><strong>目标策略</strong>（target policy）：$\pi_\theta$，我们这一步想要优化的策略。</li>
</ul>

<p>在理想设定里我们默认 $\mu = \pi_{\theta_{\text{old}}}$；现实系统里二者往往不等，这就是“训推不一致”的数学影子。</p>

<h3 id="两策略-trpo">两策略 TRPO</h3>

<blockquote>
  <p>熟悉 TRPO 的读者可以直接跳到后面的“三策略 TRPO”小节。</p>
</blockquote>

<p>TRPO 的所有理论保证，都是建立在<strong>某个“基准策略”的优势函数</strong>之上的。既然实际能算清楚的<strong>只有</strong> $A_\mu$（数据是按 $\mu$ 采的），那我们就直接把 $\mu$ 当成基准。</p>

<p>一个经典的结论是 <strong>性能差分引理（Performance Difference Lemma）</strong>：</p>

<blockquote>
  <p>对任意两策略 $\mu$ 和 $\pi_\theta$，有<br />
\(\mathcal{J}(\pi_\theta) - \mathcal{J}(\mu)
= \frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d_{\pi_\theta},\, a\sim\pi_\theta}[A_\mu(s,a)]。\)</p>
</blockquote>

<p>直觉非常简单：</p>

<ul>
  <li>$A_\mu(s,a)$ 就是在说“如果在 $s$ 里本来按 $\mu$ 行动，现在换成动作 $a$，长期回报会多或少多少”；</li>
  <li>把所有时刻、所有状态、所有动作的“增益”累积起来，就得到新策略比行为策略总共赚了多少。</li>
</ul>

<p>TRPO 的问题在于，我们没法准确算
\(\mathbb{E}_{s\sim d_{\pi_\theta}, a\sim\pi_\theta}[A_\mu(s,a)]，\)
因为 $d_{\pi_\theta}$ 是“新策略”的状态分布，我们没有在它下面采样过。</p>

<p>于是 TRPO 引入了一个替代目标：把状态分布换成行为策略的：</p>

\[L_\mu(\pi_\theta)
:= \mathcal{J}(\mu) + \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_\mu,\,a\sim \pi_\theta}[A_\mu(s,a)]。\]

<p>$L_\mu$ 的直觉解释是：在行为策略的状态分布下，让新策略试着去选动作，看优势有多大。</p>

<p>从性能差分引理出发，两者之差是：</p>

\[\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)
= \frac{1}{1-\gamma}\;
  \sum_s \big(d_{\pi_\theta}(s) - d_\mu(s)\big)
  \,\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[A_\mu(s,a)]。\]

<p>如果我们定义</p>

\[\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|，\]

<p>那么有一个直接的上界：</p>

<blockquote>
  <p><strong>Lemma 1</strong><br />
\(|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\;
    \|d_{\pi_\theta} - d_\mu\|_1。\)</p>
</blockquote>

<p>这里出现了第一个关键量：</p>

<blockquote>
  <p><strong>状态分布偏移</strong> $|d_{\pi_\theta} - d_\mu|_1$，也就是“新策略和行为策略看到的世界，到底差了多少”。</p>
</blockquote>

<p>我们通常不会直接对 $|d_{\pi_\theta} - d_\mu|_1$ 施加约束，反而是对“每一步 action 分布”的差异施加约束，比如信赖域、KL、clip 全是干这个的。</p>

<p>记总变差距离（total variation）：</p>

\[D_{\mathrm{TV}}(p,q) := \frac{1}{2}\|p-q\|_1。\]

<p>假设存在常数 $\beta$，使得</p>

<blockquote>
  <p>对所有 $s$，行为策略和目标策略之间的 TV 被 $\beta$ 上界：
\(D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big) \le \beta。\)</p>
</blockquote>

<p>直观含义：在任意状态里，“新策略”和“生成数据的策略”选动作的分布都不会离太远。</p>

<p>一个经典结果（可以用 coupling 证明）是：</p>

<blockquote>
  <p><strong>Lemma 2</strong><br />
在上述条件下有
\(\|d_{\pi_\theta} - d_\mu\|_1
\le \frac{2\gamma}{1-\gamma}\,\beta。\)</p>
</blockquote>

<p>把它和 Lemma 1 结合：</p>

\[|\mathcal{J}(\pi_\theta) - L_\mu(\pi_\theta)|
\le \frac{\epsilon_\mu}{1-\gamma}\; \frac{2\gamma}{1-\gamma}\,\beta
= \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\beta。\]

<p>于是我们得到一个形式上相当简洁的<strong>两策略 TRPO 下界（基准为行为策略）</strong>：</p>

<blockquote>
  <p><strong>Theorem 1（两策略 TRPO）</strong><br />
\(\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
\frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}\,\beta。\)</p>
</blockquote>

<p>这说明：</p>

<ul>
  <li><strong>真正决定“替代目标 $L_\mu$ 靠不靠谱”的，是行为策略 $\mu$ 和目标策略 $\pi_\theta$ 的差异：</strong>
\(\beta = \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s), \pi_\theta(\cdot\mid s)\big)。\)</li>
</ul>

<p>如果你能直接约束住这个 $\beta$，就能直接把 TRPO 的单调性保证搬到行为策略视角下。</p>

<h3 id="三策略-trpo">三策略 TRPO</h3>

<p>现实问题在于：<strong>大模型强化学习训练里我们可能无法直接控制 $\beta$ 本身。</strong></p>

<p>在大部分 PPO / GRPO / GSPO / 现有 RLHF 框架里，实际发生的是：</p>

<ul>
  <li>rollout 数据是由某个<strong>行为策略</strong> $\mu$ 产生的（推理引擎里的“那一版参数” + 若干系统细节）；</li>
  <li>更新时，我们希望利用<strong>参考策略</strong> $\pi_{\theta_{\text{old}}}$ 来限制<strong>目标策略</strong> $\pi_\theta$ 的更新幅度。</li>
</ul>

<p>也就是说，实际可以“动手”的是两个量：</p>

<ol>
  <li><strong>参考 vs 目标</strong>：我们可以通过 KL / clip 等手段控制
\(D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)。\)</li>
  <li><strong>行为 vs 参考</strong>：我们希望<strong>间接</strong>控制
\(D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)。\)</li>
</ol>

<p>于是自然就定义两个“proxy 差异”：</p>

<ul>
  <li><strong>约束 1：参考 vs 目标</strong>
\(\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big)；\)</li>
  <li><strong>约束 2：行为 vs 参考</strong>
\(\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big)。\)</li>
</ul>

<p>直觉上：</p>

<ul>
  <li>$\alpha_0$：新策略到底离“你宣称的那份旧策略”有多远——这就是信赖域的那部分；</li>
  <li>$\alpha_1$：你用来训练的参考策略，到底跟真实采样时的行为策略差了多少——这就是训推不一致或异步的影子。</li>
</ul>

<p>现在，可以把这两个量塞回 TRPO 的下界里。</p>

<p>对任意状态 $s$，有</p>

\[\begin{aligned}
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
&amp;\le
D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_{\theta_{\text{old}}}(\cdot\mid s)\big)
\\
&amp;\quad +
D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)。
\end{aligned}\]

<p>对 $s$ 取上确界：</p>

\[\beta
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),\pi_\theta(\cdot\mid s)\big)
\;\le\;
\alpha_1 + \alpha_0。\]

<p>把这个不等式塞回两策略 TRPO 的结论（Theorem 1）里，记</p>

\[C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}，\]

<p>即得到：</p>

\[\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,\beta
\;\ge\;
L_\mu(\pi_\theta)
\;-\;
C\,(\alpha_0 + \alpha_1)。\]

<p>于是，我们得到一个非常直接的<strong>三策略 TRPO 下界</strong>：</p>

<blockquote>
  <p><strong>Theorem 2（三策略 TRPO）</strong><br />
记
\(\epsilon_\mu := \max_{s,a} |A_\mu(s,a)|,\quad
C := \frac{2\epsilon_\mu\gamma}{(1-\gamma)^2}，\)
以及
\(\alpha_0
:= \max_s D_{\mathrm{TV}}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s),
                              \pi_\theta(\cdot\mid s)\big)，
\quad
\alpha_1
:= \max_s D_{\mathrm{TV}}\big(\mu(\cdot\mid s),
                              \pi_{\theta_{\text{old}}}(\cdot\mid s)\big)。\)
则对任意目标策略 $\pi_\theta$ 有</p>

\[\boxed{
\mathcal{J}(\pi_\theta)
\;\ge\;
L_\mu(\pi_\theta)
\;-\; C\,(\alpha_0 + \alpha_1)
}\]

  <p>其中
\(L_\mu(\pi_\theta)
:=
\mathcal{J}(\mu) + \frac{1}{1-\gamma}
  \mathbb{E}_{s\sim d_\mu,a\sim\pi_\theta}[A_\mu(s,a)]。\)</p>
</blockquote>

<p>这个结论的含义其实很直接：</p>

<ul>
  <li><strong>替代目标 $L_\mu(\pi_\theta)$ 与真实性能 $\mathcal{J}(\pi_\theta)$ 之间的 gap，可以拆成两部分：</strong>
    <ul>
      <li>参考 vs 目标的偏移 $\alpha_0$；</li>
      <li>行为 vs 参考的偏移 $\alpha_1$。</li>
    </ul>
  </li>
</ul>

<p>只要这两个量都小，<strong>优化 $L_\mu$ 就有希望有效提升 $\mathcal{J}$</strong>。</p>

<h3 id="这两个差异各自怎么约束">这两个差异各自怎么约束？</h3>

<p>现在，我们可以从 Theorem 2 回头看各种实际方法：</p>

<ul>
  <li>绝大多数 “PPO / GRPO / GSPO” 类工作，其实是在控制 <strong>约束 1：$\alpha_0$</strong>；</li>
  <li>绝大多数 “TIS / IcePop / MIS” 类工作，在本文的统一视角下，可以理解为主要是在控制 <strong>约束 2：$\alpha_1$</strong>。</li>
</ul>

<p>本文下面只讨论 <strong>约束 2</strong>。</p>

<p>约束 2 的目标是：<strong>保证用来训练的数据，尽可能来自“接近参考策略”的行为策略。</strong></p>

<p>这里通常既有<strong>系统层</strong>的机制，也有<strong>算法层（importance sampling）</strong>的机制。</p>

<ol>
  <li>
    <p><strong>系统层：让行为策略别飘太远</strong></p>

    <ul>
      <li>异步框架：<br />
给每个样本打上策略版本号，只能用与 $\pi_{\theta_{\text{old}}}$ 相差不大的参数版本采样的数据；</li>
      <li>训推对齐：<br />
强调训练框架和推理框架用相同精度、相同算子、相近的内核 / kernel 行为。</li>
    </ul>

    <p>这些机制的目标是：从“算法外部”让 $\mu$ 和 $\pi_{\theta_{\text{old}}}$ 靠近，从而压缩 $\alpha_1$。</p>
  </li>
  <li>
    <p><strong>算法层：样本修正</strong></p>

    <p>在算法层，我们不再试图“纠正整个行为策略”，而是用重要性采样比率在<strong>样本层面</strong>做筛选和重加权，让“真正参与训练的样本子集”上的行为策略尽量接近参考策略，或者减小差异较大的样本在训练上的权重。</p>

    <p>具体来说，就是下面这些方法，它们本质上都可以看作是“实现约束 2 的不同方式”。</p>
  </li>
</ol>

<h2 id="tisicepopsequence-level-mis都是约束-2的不同实现">TIS、IcePop、sequence-level MIS：都是“约束 2”的不同实现</h2>

<p>下面延续前文的记号体系来写这三种方法的目标函数，只聚焦在“行为策略 vs 参考策略”这一维的设计。记 token 级的 PPO / GRPO 风格更新项为</p>

\[g_\theta(t)
= \min\big(r_t(\theta) A_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon) A_t\big),\]

<p>其中</p>

\[r_t(\theta) = \frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)},
\quad (s_t,a_t)\sim\mu,\quad A_t := A_\mu(s_t,a_t)。\]

<p>也就是说：</p>

<ul>
  <li>$r_t(\theta)$ 是 <strong>目标 vs 参考</strong> 的比率（对应约束 1）；</li>
  <li>$A_t$ 基于行为策略采样的数据，是我们能估到的优势函数。</li>
</ul>

<p>为了把 token 级的 $(s_t,a_t)$ 与序列级的 $(x,y)$ 记号打通，在以 RLHF（reinforcement learning from human feedback，人类反馈强化学习）为代表的 LLM-RL 设定中，我们约定：</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>prompt 记为 $x$；回复记为 $y = (y_1,\dots,y_{</td>
          <td>y</td>
          <td>})$；</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>token 级状态 $s_t := (x, y_{&lt;t})$，动作 $a_t := y_t$；</li>
  <li>因此行为策略和参考策略在序列上的分布可写成
\(\mu(y\mid x) = \prod_{t=1}^{|y|}\mu(a_t=y_t\mid s_t),\quad
\pi_{\theta_{\text{old}}}(y\mid x) = \prod_{t=1}^{|y|}\pi_{\theta_{\text{old}}}(a_t=y_t\mid s_t)。\)</li>
</ul>

<p>此外，为了描述“参考 vs 行为”的偏移，统一定义 token 级重要性比率</p>

\[\rho_t^{(\text{ref}\leftarrow\text{beh})} := 
\frac{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}{\mu(a_t\mid s_t)}，\]

<p>以及其对应的序列级版本</p>

\[\rho(y\mid x) := \frac{\pi_{\theta_{\text{old}}}(y\mid x)}{\mu(y\mid x)} 
= \prod_{t=1}^{|y|} \rho_t^{(\text{ref}\leftarrow\text{beh})}。\]

<p>接下来，TIS / IcePop / MIS 的区别，就体现在“如何利用这些 $\rho$ 来实现约束 2”。</p>

<h3 id="1-tistoken-level-截断-is">1. TIS：token-level 截断 IS</h3>

<p>TIS 直接对上述 $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ 做截断，记</p>

\[\color{blue}{w_t = \min\big(\rho_t^{(\text{ref}\leftarrow\text{beh})},\ C_{\text{IS}}\big)}。\]

<p>更新目标写成</p>

\[L_{\text{TIS}}(\theta) 
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{w_t}\; g_\theta(t)\big]。\]

<ul>
  <li>蓝色的 $\color{blue}{w_t}$ 是被截断的 IS 权重：极端大的比率被压到常数 $C_{\text{IS}}$。</li>
  <li>从三策略 TRPO 的角度看，这相当于在 <strong>token 分布</strong> 上“软削弱”行为策略和参考策略严重不一致的样本，从而在梯度中有效减小那部分样本对 $\alpha_1$ 的贡献。</li>
</ul>

<hr />

<h3 id="2-icepopmoe-场景下的-token-level-双侧-mask">2. IcePop：MoE 场景下的 token-level 双侧 Mask</h3>

<p>IcePop 同样以 $\rho_t^{(\text{ref}\leftarrow\text{beh})}$ 为度量，但采用 <strong>双侧掩码</strong>：</p>

\[\color{blue}{m_t = \mathbf{1}\big[C_{\text{low}} \le \rho_t^{(\text{ref}\leftarrow\text{beh})} \le C_{\text{high}}\big]}。\]

<p>更新目标写成</p>

\[L_{\text{IcePop}}(\theta) 
= - \mathbb{E}_{(s_t,a_t)\sim\mu}\big[\,\color{blue}{m_t}\; g_\theta(t)\big]。\]

<ul>
  <li>蓝色的 $\color{blue}{m_t}$ 决定某个 token 是否参与更新：比率太大或太小的 token 直接被丢弃。</li>
  <li>这相当于硬性裁掉“行为策略和参考策略极度不一致”的 token，只在 $\rho_t$ 适中的区域上优化，从样本集合层面实施更强的“约束 2”。</li>
</ul>

<hr />

<h3 id="3-sequence-level-mis按整条序列-mask-的重要性采样">3. sequence-level MIS：按整条序列 Mask 的重要性采样</h3>

<p>MIS 的核心操作是：<strong>只保留 IS 比率不超过阈值 $C$ 的序列，其余序列的损失直接置零</strong>。写成</p>

\[\color{blue}{
\rho(y\mid x)
\leftarrow
\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}
}\]

<p>在统一的损失形式下，可以写成</p>

\[L_{\text{MIS}}(\theta)
=-\,\mathbb{E}_{(x,y)\sim\mu}
\Big[
\color{blue}{\rho(y\mid x)\,\mathbf{1}\{\rho(y\mid x)\le C\}} 
\;\cdot\; \sum_{t=1}^{|y|}g_\theta(t)
\Big],\]

<p>简而言之：</p>

<ul>
  <li>对于 <strong>IS 比率较小的序列</strong>：保留完整的 $\rho(y\mid x)$ 权重，正常做 off-policy 修正；</li>
  <li>对于 <strong>IS 比率超过阈值 $C$ 的序列</strong>：整个序列的 policy loss 被 mask 掉（权重变成 $0$）。</li>
</ul>

<p>从三策略 TRPO 的角度看，MIS 不再在 token 上做截断，而是直接在<strong>序列级</strong>筛掉“行为策略和参考策略严重不一致”的轨迹，只在 $\rho(y\mid x)\le C$ 的子分布上优化，从而在 trajectory 粒度上实现对“约束 2”（$\mu$ vs $\pi_{\theta_{\text{old}}}$ 偏移）的控制。</p>

<h2 id="小结">小结</h2>

<p>如果把这篇文章压缩成一句话，就是：</p>

<blockquote>
  <p><strong>许多“大模型 RL 训推不一致”和“异步训练”问题，在本文的视角下，其实都可以理解为：在 TRPO 框架下，当行为策略 $\mu$ 和参考策略 $\pi_{\theta_{\text{old}}}$ 不一致时，二者之间的偏移（$\alpha_1$）被严重低估了。</strong></p>
</blockquote>

<p>从两策略到三策略，我们做的事情其实很简单：</p>

<ul>
  <li>把 TRPO 的下界从“旧策略 vs 新策略”的叙述，改写成“<strong>行为策略 – 参考策略 – 目标策略</strong>”三者的关系；</li>
  <li>显式地拆出了两个 TV 距离：
    <ul>
      <li><strong>约束 1：参考 vs 目标</strong> $\alpha_0$，对应 PPO / GRPO / GSPO 等工作里最常见的 KL / clip / trust region；</li>
      <li><strong>约束 2：行为 vs 参考</strong> $\alpha_1$，对应异步框架、训推差异、MoE 路由、kernel 非确定性等现实因素；</li>
    </ul>
  </li>
  <li>得到了一个非常直接的结论：<br />
替代目标 $L_\mu(\pi_\theta)$ 和真实性能 $\mathcal{J}(\pi_\theta)$ 的 gap 正比于 $\alpha_0 + \alpha_1$。</li>
</ul>

<p>在这个视角下（当然这只是众多可能视角之一）：</p>

<ul>
  <li>Decoupled PPO / AReaL 可以被看作是在<strong>形式上承认“三策略存在”</strong>，并尝试在目标函数上将“行为分布”和“参考策略”解耦；</li>
  <li>TIS、IcePop、sequence-level MIS，则是在不同粒度（token / sequence）上，<strong>试图通过 IS 截断 / 掩码把“约束 2”落到样本层面</strong>：
    <ul>
      <li>TIS：用 token-level 截断权重削弱极端样本的影响；</li>
      <li>IcePop：在 MoE 场景下用 token-level 双侧掩码硬性丢弃“极端不一致”的 token；</li>
      <li>MIS：在 sequence-level 直接屏蔽整条“偏差过大”的轨迹；</li>
    </ul>
  </li>
  <li>《RL老训崩？训推差异是基石》、以及前文提到的 <em>Defeating Nondeterminism in LLM Inference</em> 等工程经验，则可以理解为在<strong>系统侧和数值实现侧</strong>，尽可能把 $\alpha_1$ 压低，让算法层的假设不至于完全失效。</li>
</ul>

<p>从这个统一视角出发，也许有助于回答几个实际问题（这里只是抛几个开放性问题）：</p>

<ul>
  <li>在什么条件下，我们还能把“大模型 RL 训练”理解成某种意义上的“近似 TRPO / PPO”？</li>
  <li>对一个具体的 RL 系统，我们究竟应该把主要精力花在：
    <ul>
      <li>收紧 $\alpha_0$（更强的 KL / 更稳的 sequence-level 目标），还是</li>
      <li>压低 $\alpha_1$（更一致的训推框架、更激进的 MIS / TIS / IcePop）？</li>
    </ul>
  </li>
  <li>在 MoE、异步采样、复杂 agent workflow 这些现实设定下，我们还能安全地假装“$\mu \approx \pi_{\theta_{\text{old}}}$”多久？</li>
</ul>

<p>本文只是在 TRPO 这个老框架上做了一个非常“<strong>最小化</strong>”的延展，把“三策略”显式写出来，并用它来整理现有的一些工作。难免有理解偏差或遗漏之处，如果你也关注实际大模型 RL 训练的情况，欢迎把你自己的设定抽象成“$\mu,\pi_{\theta_{\text{old}}},\pi_\theta$ 三者的关系”，再回头看看 Theorem 2 里的那条不等式，或许会有不一样的直观感受。</p>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[在大模型强化学习中，因为推理框架和训练框架的不一致，以及异步训练框架下行为策略分布的多样性，行为策略与参考策略不一致的问题变得尤为突出。本文分析了行为策略与参考策略不一致问题在 TRPO 框架下的影响，并在这一分析基础上梳理了当前对这一问题的不同解决方法。]]></summary></entry></feed>