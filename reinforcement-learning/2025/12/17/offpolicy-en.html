<!DOCTYPE html><html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees | Xihuai Wang's Page</title>
    <meta name="author" content="Xihuai Leo Wang" />
    <meta name="description" content="A systematic derivation of off-policy training theory for LLM reinforcement learning: starting from single-policy sampling performance improvement bounds, extending to multi-policy static/dynamic mixture sampling, establishing sufficient conditions for monotonic improvement, decomposing constraints via the triangle inequality into update increment shift (controllable by optimization) and sampling staleness (controllable by sampling), and ultimately translating these into actionable clipping mechanisms and data filtering strategies." />
    <meta name="keywords" content="Reinforcement Learning, Multi-agent System, Language Model" />

    <!-- OpenGraph -->
    <meta property="og:site_name" content="Xihuai Wang's Page" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Xihuai Wang's Page | Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees" />
    <meta property="og:url" content="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html" />
    <meta property="og:description" content="A systematic derivation of off-policy training theory for LLM reinforcement learning: starting from single-policy sampling performance improvement bounds, extending to multi-policy static/dynamic mixture sampling, establishing sufficient conditions for monotonic improvement, decomposing constraints via the triangle inequality into update increment shift (controllable by optimization) and sampling staleness (controllable by sampling), and ultimately translating these into actionable clipping mechanisms and data filtering strategies." />
    <meta property="og:locale" content="en_US" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees" />
    <meta name="twitter:description" content="A systematic derivation of off-policy training theory for LLM reinforcement learning: starting from single-policy sampling performance improvement bounds, extending to multi-policy static/dynamic mixture sampling, establishing sufficient conditions for monotonic improvement, decomposing constraints via the triangle inequality into update increment shift (controllable by optimization) and sampling staleness (controllable by sampling), and ultimately translating these into actionable clipping mechanisms and data filtering strategies." />
    
    

    <!-- Schema.org -->
    <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Xihuai Leo Wang"
        },
        "url": "https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html",
        "@type": "WebSite",
        "description": "A systematic derivation of off-policy training theory for LLM reinforcement learning: starting from single-policy sampling performance improvement bounds, extending to multi-policy static/dynamic mixture sampling, establishing sufficient conditions for monotonic improvement, decomposing constraints via the triangle inequality into update increment shift (controllable by optimization) and sampling staleness (controllable by sampling), and ultimately translating these into actionable clipping mechanisms and data filtering strategies.",
        "headline": "Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees",
        "sameAs": ["https://scholar.google.com/citations?user=hy6v3qUAAAAJ", "https://www.semanticscholar.org/author/2178008", "https://github.com/xihuai18"],
        "name": "Xihuai Leo Wang",
        "@context": "https://schema.org"
      }
    </script>


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">Xihuai Wang's Page</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about</a>
          </li>
          

          <!-- CV -->
          <!-- 
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/" target="_blank"
              rel="noopener noreferrer">cv</a>
          </li> -->
          <!-- Other pages -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">Xihuai's Blog</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">publications</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/cv/">cv</a>
          </li>

          <!-- Toogle theme mode -->
          <li class="toggle-container">
            <button id="light-toggle" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Taming Stale Data: Off-Policy Reinforcement Learning for LLMs with Monotonic Improvement Guarantees</h1>
    <p class="post-meta">December 17, 2025</p>
    <p class="post-tags">
  <a href="/blog/2025"> üìÖ 2025 </a>
      &nbsp; &middot; &nbsp;
        <a href="/blog/category/reinforcement-learning">
          üè∑Ô∏è reinforcement-learning</a> &nbsp;
          

    </p>
  </header>

  <article class="post-content">
    <ul id="markdown-toc">
  <li><a href="#introduction-why-should-we-care-about-off-policy" id="markdown-toc-introduction-why-should-we-care-about-off-policy">Introduction: Why Should We Care About ‚ÄúOff-Policy‚Äù?</a></li>
  <li><a href="#part-i-theoretical-foundations" id="markdown-toc-part-i-theoretical-foundations">Part I: Theoretical Foundations</a>    <ul>
      <li><a href="#11-basic-setup" id="markdown-toc-11-basic-setup">1.1 Basic Setup</a></li>
      <li><a href="#12-core-tool-policy-performance-difference-lemma" id="markdown-toc-12-core-tool-policy-performance-difference-lemma">1.2 Core Tool: Policy Performance Difference Lemma</a></li>
    </ul>
  </li>
  <li><a href="#part-ii-performance-improvement-bounds-for-single-policy-sampling" id="markdown-toc-part-ii-performance-improvement-bounds-for-single-policy-sampling">Part II: Performance Improvement Bounds for Single-Policy Sampling</a>    <ul>
      <li><a href="#21-the-distribution-mismatch-problem" id="markdown-toc-21-the-distribution-mismatch-problem">2.1 The Distribution Mismatch Problem</a></li>
      <li><a href="#22-controlling-state-distribution-differences" id="markdown-toc-22-controlling-state-distribution-differences">2.2 Controlling State Distribution Differences</a></li>
      <li><a href="#23-policy-performance-improvement-lower-bound" id="markdown-toc-23-policy-performance-improvement-lower-bound">2.3 Policy Performance Improvement Lower Bound</a></li>
    </ul>
  </li>
  <li><a href="#part-iii-multi-policy-static-mixture-sampling" id="markdown-toc-part-iii-multi-policy-static-mixture-sampling">Part III: Multi-Policy Static Mixture Sampling</a>    <ul>
      <li><a href="#31-practical-scenarios" id="markdown-toc-31-practical-scenarios">3.1 Practical Scenarios</a></li>
      <li><a href="#32-core-idea-augmented-state-space" id="markdown-toc-32-core-idea-augmented-state-space">3.2 Core Idea: Augmented State Space</a></li>
      <li><a href="#33-structural-simplification-for-trajectory-level-mixture" id="markdown-toc-33-structural-simplification-for-trajectory-level-mixture">3.3 Structural Simplification for Trajectory-Level Mixture</a></li>
      <li><a href="#34-performance-improvement-lower-bound-for-trajectory-level-mixture" id="markdown-toc-34-performance-improvement-lower-bound-for-trajectory-level-mixture">3.4 Performance Improvement Lower Bound for Trajectory-Level Mixture</a></li>
    </ul>
  </li>
  <li><a href="#part-iv-dynamic-mixture-sampling-and-monotonic-improvement-conditions" id="markdown-toc-part-iv-dynamic-mixture-sampling-and-monotonic-improvement-conditions">Part IV: Dynamic Mixture Sampling and Monotonic Improvement Conditions</a>    <ul>
      <li><a href="#41-the-core-challenge" id="markdown-toc-41-the-core-challenge">4.1 The Core Challenge</a></li>
      <li><a href="#42-unified-modeling-framework" id="markdown-toc-42-unified-modeling-framework">4.2 Unified Modeling Framework</a></li>
      <li><a href="#43-core-decomposition" id="markdown-toc-43-core-decomposition">4.3 Core Decomposition</a></li>
      <li><a href="#44-monotonic-improvement-lower-bound" id="markdown-toc-44-monotonic-improvement-lower-bound">4.4 Monotonic Improvement Lower Bound</a></li>
      <li><a href="#45-infeasibility-of-direct-constraints" id="markdown-toc-45-infeasibility-of-direct-constraints">4.5 Infeasibility of Direct Constraints</a></li>
      <li><a href="#46-triangle-inequality-decomposition" id="markdown-toc-46-triangle-inequality-decomposition">4.6 Triangle Inequality Decomposition</a></li>
    </ul>
  </li>
  <li><a href="#part-v-theoretical-foundations-of-clipping-mechanisms" id="markdown-toc-part-v-theoretical-foundations-of-clipping-mechanisms">Part V: Theoretical Foundations of Clipping Mechanisms</a>    <ul>
      <li><a href="#51-from-tv-distance-to-computable-quantities" id="markdown-toc-51-from-tv-distance-to-computable-quantities">5.1 From TV Distance to Computable Quantities</a></li>
      <li><a href="#52-sample-representation-of-u_k" id="markdown-toc-52-sample-representation-of-u_k">5.2 Sample Representation of $U_k$</a></li>
      <li><a href="#53-two-methods-for-constraining-u_k" id="markdown-toc-53-two-methods-for-constraining-u_k">5.3 Two Methods for Constraining $U_k$</a></li>
      <li><a href="#54-complete-objective-functions-for-three-clipping-mechanisms" id="markdown-toc-54-complete-objective-functions-for-three-clipping-mechanisms">5.4 Complete Objective Functions for Three Clipping Mechanisms</a></li>
      <li><a href="#55-comparison-of-three-methods" id="markdown-toc-55-comparison-of-three-methods">5.5 Comparison of Three Methods</a></li>
      <li><a href="#56-controlling-sampling-staleness" id="markdown-toc-56-controlling-sampling-staleness">5.6 Controlling Sampling Staleness</a></li>
      <li><a href="#57-operational-meaning-of-clipping" id="markdown-toc-57-operational-meaning-of-clipping">5.7 Operational Meaning of Clipping</a></li>
      <li><a href="#58-section-summary" id="markdown-toc-58-section-summary">5.8 Section Summary</a></li>
    </ul>
  </li>
  <li><a href="#part-vi-comparison-of-trajectory-level-and-stepsegment-level-mixture" id="markdown-toc-part-vi-comparison-of-trajectory-level-and-stepsegment-level-mixture">Part VI: Comparison of Trajectory-Level and Step/Segment-Level Mixture</a>    <ul>
      <li><a href="#61-core-differences-between-the-two-mechanisms" id="markdown-toc-61-core-differences-between-the-two-mechanisms">6.1 Core Differences Between the Two Mechanisms</a></li>
      <li><a href="#62-differences-in-sampling-staleness-s_k" id="markdown-toc-62-differences-in-sampling-staleness-s_k">6.2 Differences in Sampling Staleness $S_k$</a></li>
      <li><a href="#63-differences-in-surrogate-objective-estimation" id="markdown-toc-63-differences-in-surrogate-objective-estimation">6.3 Differences in Surrogate Objective Estimation</a></li>
      <li><a href="#64-variance-amplification-risk" id="markdown-toc-64-variance-amplification-risk">6.4 Variance Amplification Risk</a></li>
      <li><a href="#65-applicable-scenarios" id="markdown-toc-65-applicable-scenarios">6.5 Applicable Scenarios</a></li>
    </ul>
  </li>
  <li><a href="#part-vii-handling-training-inference-inconsistency" id="markdown-toc-part-vii-handling-training-inference-inconsistency">Part VII: Handling Training-Inference Inconsistency</a>    <ul>
      <li><a href="#71-background" id="markdown-toc-71-background">7.1 Background</a></li>
      <li><a href="#72-effective-staleness" id="markdown-toc-72-effective-staleness">7.2 Effective Staleness</a></li>
      <li><a href="#73-actionable-control" id="markdown-toc-73-actionable-control">7.3 Actionable Control</a></li>
    </ul>
  </li>
  <li><a href="#summary-practical-guidelines" id="markdown-toc-summary-practical-guidelines">Summary: Practical Guidelines</a>    <ul>
      <li><a href="#core-theoretical-framework" id="markdown-toc-core-theoretical-framework">Core Theoretical Framework</a></li>
      <li><a href="#separation-of-concerns-principle" id="markdown-toc-separation-of-concerns-principle">Separation of Concerns Principle</a></li>
      <li><a href="#clipping-method-selection" id="markdown-toc-clipping-method-selection">Clipping Method Selection</a></li>
      <li><a href="#handling-training-inference-inconsistency" id="markdown-toc-handling-training-inference-inconsistency">Handling Training-Inference Inconsistency</a></li>
    </ul>
  </li>
  <li><a href="#appendix-quick-reference-for-key-symbols" id="markdown-toc-appendix-quick-reference-for-key-symbols">Appendix: Quick Reference for Key Symbols</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<p><a href="/reinforcement-learning/2025/12/17/offpolicy-zh.html">‰∏≠ÊñáÁâà</a></p>

<h2 id="introduction-why-should-we-care-about-off-policy">Introduction: Why Should We Care About ‚ÄúOff-Policy‚Äù?</h2>

<p>Consider the following scenario: you are training a large language model with reinforcement learning to improve its question-answering capabilities. Ideally, each time the model generates a batch of responses, you would immediately update the model with this data, then use the updated model to generate new data, and so on. This approach of ‚Äúupdating with data from the same policy that generated it‚Äù is called <strong>on-policy</strong> training.</p>

<p>Reality, however, is not so simple. In large-scale distributed training, hundreds of GPUs generate data in parallel, while model updates take time. When a new model is deployed, much data generated by ‚Äúolder versions‚Äù of the model remains unused‚Äîdiscarding it seems wasteful, yet using it raises concerns about whether ‚Äústale data‚Äù might harm training effectiveness.</p>

<p>This is the core problem faced by <strong>off-policy</strong> training: <strong>Can we guarantee continued performance improvement when using data collected by older policies to update newer policies?</strong></p>

<p>This article systematically addresses this question. Starting from foundational theory, we progressively derive actionable conditions that specify when mixing data from multiple policy versions can still guarantee monotonic training improvement.</p>

<h2 id="part-i-theoretical-foundations">Part I: Theoretical Foundations</h2>

<h3 id="11-basic-setup">1.1 Basic Setup</h3>

<p>We consider a standard Markov Decision Process (MDP) comprising a state space $\mathcal{S}$, action space $\mathcal{A}$, transition probability $p(s‚Äô|s,a)$, reward function $r(s,a)$, initial distribution $\rho_0$, and discount factor $\gamma \in (0,1)$.</p>

<p>The <strong>expected cumulative discounted return</strong> of policy $\pi$ is:</p>

\[J(\pi) := \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid \pi\right]\]

<p>The <strong>discounted state visitation distribution</strong> represents the weighted frequency of visiting each state during long-term policy execution:</p>

\[d_\pi(s) := (1-\gamma) \sum_{t=0}^{\infty} \gamma^t \Pr(s_t = s \mid \pi)\]

<p>The <strong>advantage function</strong> measures how much better action $a$ is compared to the policy‚Äôs average:</p>

\[A^\pi(s,a) := Q^\pi(s,a) - V^\pi(s)\]

<p>The <strong>total variation distance</strong> (TV distance) measures the difference between two policies‚Äô action distributions at state $s$:</p>

\[D_{\mathrm{TV}}(\pi, \pi'; s) := \frac{1}{2} \sum_{a \in \mathcal{A}} |\pi(a \mid s) - \pi'(a \mid s)|\]

<h3 id="12-core-tool-policy-performance-difference-lemma">1.2 Core Tool: Policy Performance Difference Lemma</h3>

<p>The cornerstone of the entire theory is this elegant result:</p>

<blockquote>
  <p><strong>Lemma 1.1 (Policy Performance Difference Lemma)</strong></p>

  <p>For any policies $\pi_k$ (old) and $\pi$ (new), the performance difference can be expressed as:</p>

\[J(\pi) - J(\pi_k) = \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_\pi}\left[ \mathbb{E}_{a \sim \pi(\cdot \mid s)}[A^{\pi_k}(s,a)] \right]\]
</blockquote>

<p><strong>Intuitive understanding</strong>: How much better the new policy is than the old equals the ‚Äúaverage advantage‚Äù obtained by selecting actions according to the new policy under the state distribution visited by the new policy.</p>

<h2 id="part-ii-performance-improvement-bounds-for-single-policy-sampling">Part II: Performance Improvement Bounds for Single-Policy Sampling</h2>

<h3 id="21-the-distribution-mismatch-problem">2.1 The Distribution Mismatch Problem</h3>

<p>The Policy Performance Difference Lemma has a practical issue: the expectation on the right-hand side is computed under $d_\pi$ (the new policy‚Äôs state distribution), while we can only sample from $d_{\pi_k}$ (the old policy).</p>

<p>The solution is to decompose the expectation into ‚Äúexpectation under the old distribution + bias term,‚Äù then control the bias. The key question is: <strong>What is the quantitative relationship between the difference in state distributions and the difference in policies?</strong></p>

<h3 id="22-controlling-state-distribution-differences">2.2 Controlling State Distribution Differences</h3>

<blockquote>
  <p><strong>Lemma 1.2 (Relationship Between State Distribution Difference and Policy TV Distance)</strong></p>

\[\|d_\pi - d_{\pi_k}\|_1 \leq \frac{2\gamma}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]\]
</blockquote>

<p><strong>Physical interpretation</strong>: Small differences in policies in action space are ‚Äúamplified‚Äù through environment dynamics into differences in state visitation distributions. The coefficient $\frac{\gamma}{1-\gamma}$ reflects the <strong>temporal accumulation effect</strong>‚Äîin long-horizon tasks ($\gamma$ close to 1), the amplification is stronger.</p>

<p><strong>Proof sketch</strong>: By deriving the fixed-point equation for discounted visitation distributions and exploiting the $\ell_1$ non-expansiveness of stochastic matrices, one can show that state distribution differences are amplified by policy differences through transition dynamics, with the amplification factor being precisely $\frac{\gamma}{1-\gamma}$.</p>

<h3 id="23-policy-performance-improvement-lower-bound">2.3 Policy Performance Improvement Lower Bound</h3>

<blockquote>
  <p><strong>Theorem 1.1 (Policy Performance Improvement Lower Bound)</strong></p>

  <p>Define the expected advantage upper bound constant $C_{\pi,\pi_k} := \max_{s} \lvert \mathbb{E}_{a \sim \pi}[A^{\pi_k}(s,a)] \rvert$. Then:</p>

\[J(\pi) - J(\pi_k) \geq L_{\pi_k}(\pi) - \frac{2\gamma C_{\pi,\pi_k}}{(1-\gamma)^2} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]\]

  <p>where the <strong>surrogate objective</strong> is:</p>

\[L_{\pi_k}(\pi) := \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}, a \sim \pi_k} \left[ \frac{\pi(a \mid s)}{\pi_k(a \mid s)} A^{\pi_k}(s,a) \right]\]
</blockquote>

<p>This lower bound consists of two parts:</p>

<ol>
  <li>
    <p><strong>Surrogate objective</strong> $L_{\pi_k}(\pi)$: Can be directly estimated from old policy data via importance sampling; this is the optimization objective of TRPO/PPO.</p>
  </li>
  <li>
    <p><strong>Policy shift penalty</strong>: Increases with the TV distance between new and old policies, explaining why PPO needs to constrain update magnitude.</p>
  </li>
</ol>

<p><strong>Core conclusion</strong>: Maximizing the surrogate objective while controlling policy shift guarantees performance improvement.</p>

<h2 id="part-iii-multi-policy-static-mixture-sampling">Part III: Multi-Policy Static Mixture Sampling</h2>

<h3 id="31-practical-scenarios">3.1 Practical Scenarios</h3>

<p>In practice, a batch of data may come from multiple policy versions ${\pi^{(1)}, \ldots, \pi^{(M)}}$, with respective proportions $\alpha_1, \ldots, \alpha_M$. How do we extend Theorem 1.1 to this setting?</p>

<h3 id="32-core-idea-augmented-state-space">3.2 Core Idea: Augmented State Space</h3>

<p>The solution is an elegant modeling technique: <strong>treat the policy version index as part of the state</strong>.</p>

<p>Define the augmented state space $\tilde{\mathcal{S}} := \mathcal{S} \times \mathcal{I}$, where $\mathcal{I} = {1, \ldots, M}$ is the policy index set. Under augmented state $(s, i)$, the <strong>mixture behavior policy</strong> is defined as $\beta(a \mid s, i) := \pi^{(i)}(a \mid s)$.</p>

<p>The evolution of indices is characterized by the <strong>index transition kernel</strong> $q(i‚Äô \mid i)$. The augmented MDP inherits the original MDP‚Äôs rewards and environment transitions, with indices evolving independently according to $q(i‚Äô|i)$.</p>

<p>This technique works because the new policy $\pi$‚Äôs return in the augmented MDP equals its return in the original MDP, allowing direct application of Theorem 1.1.</p>

<h3 id="33-structural-simplification-for-trajectory-level-mixture">3.3 Structural Simplification for Trajectory-Level Mixture</h3>

<p>The most common scenario is <strong>using a single old policy per trajectory</strong>: at trajectory start, sample index $I_0 \sim \alpha$, and use $\pi^{(I_0)}$ throughout. In this case, the index transition kernel is the identity: $q(i‚Äô \mid i) = \mathbf{1}_{i‚Äô=i}$.</p>

<p>From an engineering perspective, in many <strong>actor-learner asynchronous training</strong> setups (when sampling and training organize data by ‚Äúentire trajectories/complete episodes belonging to a certain policy version‚Äù), this approximately corresponds to what we call <strong>trajectory-level mixture</strong>: actors use a fixed policy snapshot within a sampling unit to generate data, while learners mix trajectories from different versions for updates. We say ‚Äúapproximately‚Äù because different systems may not have identical boundaries for ‚Äútrajectory/sampling unit.‚Äù</p>

<blockquote>
  <p><strong>Lemma 2.1 (Structural Simplification for Trajectory-Level Mixture)</strong></p>

  <p>(a) The augmented state visitation distribution decomposes as: $d_{\beta}(s, i) = \alpha_i \cdot d_{\pi^{(i)}}(s)$</p>

  <p>(b) The advantage function reduces to: $A^{\beta}((s, i), a) = A^{\pi^{(i)}}(s, a)$</p>
</blockquote>

<p><strong>Intuition for (b)</strong>: Since the index never changes, <strong>all future trajectories</strong> starting from augmented state $(s,i)$ are generated by the same policy $\pi^{(i)}$. Therefore, future cumulative returns are entirely determined by $\pi^{(i)}$, and value functions and advantage functions naturally reduce to their $\pi^{(i)}$ counterparts.</p>

<p>Consequently, the mixture policy‚Äôs return is the weighted average of individual old policies‚Äô returns: $J_{\mathrm{mix}} = \sum_{i=1}^{M} \alpha_i J(\pi^{(i)})$.</p>

<h3 id="34-performance-improvement-lower-bound-for-trajectory-level-mixture">3.4 Performance Improvement Lower Bound for Trajectory-Level Mixture</h3>

<blockquote>
  <p><strong>Corollary 2.1 (Performance Improvement Lower Bound for Trajectory-Level Mixture)</strong></p>

\[J(\pi) - \sum_{i=1}^{M} \alpha_i J(\pi^{(i)}) \geq \sum_{i=1}^{M} \alpha_i L_{\pi^{(i)}}(\pi) - \frac{2\gamma \max_i C_{\pi, \pi^{(i)}}}{(1-\gamma)^2} \sum_{i=1}^{M} \alpha_i \mathbb{E}_{s \sim d_{\pi^{(i)}}} \big[ D_{\mathrm{TV}}(\pi, \pi^{(i)}; s) \big]\]
</blockquote>

<p>This result shows that when mixing trajectories from multiple old policy versions for training, if we construct the loss using importance ratios corresponding to each trajectory‚Äôs source policy while controlling the new policy‚Äôs deviation from each old policy, the new policy‚Äôs performance has a clear improvement lower bound.</p>

<h2 id="part-iv-dynamic-mixture-sampling-and-monotonic-improvement-conditions">Part IV: Dynamic Mixture Sampling and Monotonic Improvement Conditions</h2>

<h3 id="41-the-core-challenge">4.1 The Core Challenge</h3>

<p>Part III discussed <strong>static mixture</strong>‚Äîwhere mixture weights $\alpha_i$ remain fixed. This section considers the more general <strong>dynamic mixture</strong>‚Äîwhere sampling gradually transitions to the new policy after it is released.</p>

<p>The previous results characterize improvement of ‚Äúthe new policy relative to the mixture behavior policy.‚Äù However, in actual training, what we truly care about is: <strong>Does the latest policy $\pi_{k+1}$ after each update monotonically improve over the previous latest policy $\pi_k$?</strong></p>

\[J(\pi_{k+1}) \geq J(\pi_k)\]

<h3 id="42-unified-modeling-framework">4.2 Unified Modeling Framework</h3>

<p>Two typical forms of dynamic mixture sampling can be uniformly characterized by the index transition kernel $q(i‚Äô|i)$:</p>

<p><strong>Trajectory-level mixture</strong> (can be viewed as an abstraction of conventional asynchronous training; identity index transition): $q(i‚Äô|i) = \mathbf{1}{i‚Äô=i}$</p>

<p><strong>Step/segment-level mixture</strong> (an abstraction of partial rollout / segment-based sampling; allows switching): $q(i‚Äô|i) = (1-\sigma(i))\mathbf{1}{i‚Äô=i} + \sigma(i)\kappa(i‚Äô|i)$</p>

<p>where $\sigma(i)$ is the switching probability and $\kappa(\cdot|i)$ is the target index distribution.</p>

<h3 id="43-core-decomposition">4.3 Core Decomposition</h3>

<p>By introducing the mixture return $J_{\mathrm{mix}}^{(k)}$ as an intermediate bridge, the performance difference decomposes as:</p>

\[J(\pi_{k+1}) - J(\pi_k) = \underbrace{[J(\pi_{k+1}) - J_{\mathrm{mix}}^{(k)}]}_{\text{improvement over mixture policy}} + \underbrace{[J_{\mathrm{mix}}^{(k)} - J(\pi_k)]}_{\text{mixture bias term}}\]

<p>The first term can be handled using Theorem 1.1. The second term is the <strong>mixture bias term</strong>, which can be shown to satisfy:</p>

\[J_{\mathrm{mix}}^{(k)} - J(\pi_k) \geq -\frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]\]

<h3 id="44-monotonic-improvement-lower-bound">4.4 Monotonic Improvement Lower Bound</h3>

<p>Combining the above results yields the core theorem:</p>

<blockquote>
  <p><strong>Theorem 3.1 (Monotonic Improvement Lower Bound Under Dynamic Mixture Sampling)</strong></p>

\[\begin{aligned}
J(\pi_{k+1}) - J(\pi_k) \geq\;&amp; L_{\beta^{(k)}}(\pi_{k+1}) \\
&amp;- \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \big] \\
&amp;- \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
\end{aligned}\]
</blockquote>

<p>This lower bound reveals the necessity of <strong>dual control</strong>:</p>
<ul>
  <li><strong>Update shift penalty</strong>: Deviation of the new policy $\pi_{k+1}$ from the sampling source policy $\pi^{(i)}$</li>
  <li><strong>Sampling staleness penalty</strong>: Staleness of the sampling source policy $\pi^{(i)}$ relative to the current policy $\pi_k$</li>
</ul>

<h3 id="45-infeasibility-of-direct-constraints">4.5 Infeasibility of Direct Constraints</h3>

<p>The update shift penalty term in Theorem 3.1 might appear controllable by constraining $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s)$, but this is actually <strong>infeasible</strong>:</p>

<blockquote>
  <p><strong>Observation 3.1 (Infeasibility of Update Shift Constraints)</strong></p>

  <p>Suppose the mixture sampling includes two old policies $\pi^{(1)}$ and $\pi^{(2)}$. If there exists some state $s$ such that $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) &gt; 2\delta$, then no policy $\pi_{k+1}$ can simultaneously satisfy $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(1)}; s) \leq \delta$ and $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(2)}; s) \leq \delta$.</p>
</blockquote>

<p><strong>Proof</strong>: By the triangle inequality, if both constraints were satisfied, then $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) \leq 2\delta$, a contradiction.</p>

<p><strong>Root cause</strong>: The update shift penalty directly couples $\pi_{k+1}$ with the historical policy family ${\pi^{(i)}}$, whose internal structure is a product of historical training and not controllable by the current update.</p>

<h3 id="46-triangle-inequality-decomposition">4.6 Triangle Inequality Decomposition</h3>

<p>The solution leverages the triangle inequality of TV distance:</p>

\[D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \leq D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s) + D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\]

<p>This decomposes the coupled constraint into two independent parts:</p>

<ul>
  <li><strong>Update increment shift</strong> $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)$: Deviation of the new policy from the current policy, <strong>controllable by the optimization side</strong></li>
  <li><strong>Sampling staleness</strong> $D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)$: Deviation of the current policy from each old policy, <strong>must be controlled by the sampling side</strong></li>
</ul>

<p>Define:</p>

\[U_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)\big], \quad S_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\big]\]

<blockquote>
  <p><strong>Corollary 3.2 (Decomposed Monotonic Improvement Lower Bound)</strong></p>

\[J(\pi_{k+1}) - J(\pi_k) \geq L_{\beta^{(k)}}(\pi_{k+1}) - \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} U_k - \left( \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} + \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \right) S_k\]
</blockquote>

<p><strong>Why does decomposition solve the problem?</strong> The key is that after decomposition, $U_k$ only involves the new policy $\pi_{k+1}$ and the current policy $\pi_k$, <strong>completely independent of the structure of the old policy family ${\pi^{(i)}}$</strong>. Therefore, regardless of how different the old policies are from each other, constraining $U_k$ is always feasible‚Äîthis is precisely the resolution to the infeasibility issue revealed in Observation 3.1.</p>

<p>This reveals an important engineering principle‚Äî<strong>separation of concerns</strong>:</p>

<div>
<table>
<thead>
<tr><th>Control Term</th><th>Responsible Party</th><th>Control Mechanism</th></tr>
</thead>
<tbody>
<tr><td>$U_k$ (update increment shift)</td><td>Optimization algorithm</td><td>Policy clipping</td></tr>
<tr><td>$S_k$ (sampling staleness)</td><td>Sampling system</td><td>Data filtering, version window</td></tr>
</tbody>
</table>
</div>

<h2 id="part-v-theoretical-foundations-of-clipping-mechanisms">Part V: Theoretical Foundations of Clipping Mechanisms</h2>

<h3 id="51-from-tv-distance-to-computable-quantities">5.1 From TV Distance to Computable Quantities</h3>

<p>Corollary 3.2 tells us that to guarantee monotonic improvement, we need to control the update increment shift $U_k = \mathbb{E}[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)]$. However, TV distance is a distribution-level quantity‚Äîhow can we control it using samples?</p>

<p>The key bridge is the following identity:</p>

<blockquote>
  <p><strong>Lemma 3.3 (Ratio Difference Representation of TV Distance)</strong></p>

  <p>Suppose policy $\pi_1$‚Äôs support covers the supports of $\pi$ and $\pi_2$. Then for any state distribution $\mu$:</p>

\[\mathbb{E}_{s\sim \mu} \big[D_{\mathrm{TV}}(\pi, \pi_2; s)\big] = \frac{1}{2} \mathbb{E}_{s\sim \mu, a\sim\pi_1(\cdot|s)} \left| \frac{\pi(a|s)}{\pi_1(a|s)} - \frac{\pi_2(a|s)}{\pi_1(a|s)} \right|\]
</blockquote>

<p><strong>Intuitive understanding</strong>: The left side is the TV distance between two distributions (requiring enumeration over all actions), while the right side is the absolute difference of two importance ratios when sampling under $\pi_1$. This enables us to estimate and control TV distance using samples.</p>

<h3 id="52-sample-representation-of-u_k">5.2 Sample Representation of $U_k$</h3>

<p>Using Lemma 3.3, setting $\pi = \pi_{k+1}$, $\pi_2 = \pi_k$, $\pi_1 = \pi^{(i)}$ (the sampling source policy), we obtain:</p>

\[U_k = \frac{1}{2} \mathbb{E}_{(s,i) \sim d_{\beta^{(k)}}, a \sim \pi^{(i)}(\cdot|s)} \left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right|\]

<p>Denoting $\rho_{k+1} := \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)}$ and $\rho_k := \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)}$, we have:</p>

\[U_k = \frac{1}{2} \mathbb{E}_{(s,i,a) \sim \text{training data}} \big| \rho_{k+1} - \rho_k \big|\]

<p>This means: <strong>If we can ensure $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$ for each sample, we can guarantee $U_k \leq \epsilon/2$</strong>.</p>

<h3 id="53-two-methods-for-constraining-u_k">5.3 Two Methods for Constraining $U_k$</h3>

<p><strong>Method 1: Direct Constraint on Ratio Difference</strong></p>

<p>For each sample $(s, i, a)$, require:</p>

\[\left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right| \leq \epsilon\]

<p>The clipping interval is $\left[\frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} - \epsilon, \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} + \epsilon\right]$, with <strong>clipping center at $\rho_k$ rather than 1</strong>.</p>

<p><strong>Method 2: Constraint on Incremental Ratio</strong></p>

<p>Noting that $\rho_{k+1} - \rho_k = \rho_k \cdot \left(\frac{\pi_{k+1}}{\pi_k} - 1\right)$, we have:</p>

\[|\rho_{k+1} - \rho_k| = \rho_k \cdot \left|\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right|\]

<p>If we constrain $\left\lvert\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right\rvert \leq \epsilon$, since $\mathbb{E}_{a\sim\pi^{(i)}}[\rho_k] = 1$, one can show $U_k \leq \epsilon/2$.</p>

<p>This method clips $\pi_{k+1}/\pi_k$ with center at 1, <strong>completely independent of the old policy $\pi^{(i)}$</strong>.</p>

<h3 id="54-complete-objective-functions-for-three-clipping-mechanisms">5.4 Complete Objective Functions for Three Clipping Mechanisms</h3>

<p>For comparison, we present the complete objective functions for three clipping mechanisms. Suppose the current sample comes from old policy $\pi^{(i)}$, and denote:</p>
<ul>
  <li>$\rho_{k+1} = \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)}$ (new policy‚Äôs ratio relative to sampling policy)</li>
  <li>$\rho_k = \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)}$ (current policy‚Äôs ratio relative to sampling policy)</li>
  <li>$r = \frac{\pi_{k+1}(a|s)}{\pi_k(a|s)}$ (new policy‚Äôs incremental ratio relative to current policy)</li>
</ul>

<p><strong>Standard PPO</strong>: Clip $\rho_{k+1}$ with center at 1</p>

\[L^{\mathrm{PPO}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\pi^{(i)}}, \; \mathrm{clip}(\rho_{k+1}, 1-\epsilon, 1+\epsilon) \cdot A^{\pi^{(i)}} \right) \right]\]

<p><strong>Method 1</strong>: Clip $\rho_{k+1}$ with center at $\rho_k$</p>

\[L^{\mathrm{M1}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\beta^{(k)}}, \; \mathrm{clip}(\rho_{k+1}, \rho_k-\epsilon, \rho_k+\epsilon) \cdot A^{\beta^{(k)}} \right) \right]\]

<p><strong>Method 2</strong>: Clip incremental ratio $r$ with center at 1</p>

\[L^{\mathrm{M2}} = \mathbb{E} \left[ \min\left( r \cdot \hat{A}, \; \mathrm{clip}(r, 1-\epsilon, 1+\epsilon) \cdot \hat{A} \right) \right]\]

<p>where $\hat{A} = \rho_k \cdot A^{\beta^{(k)}}$ is the importance-weighted advantage estimate.</p>

<h3 id="55-comparison-of-three-methods">5.5 Comparison of Three Methods</h3>

<p><strong>Table 5.1„ÄÄComparison of Three Clipping Mechanisms</strong></p>

<div>
<table>
<thead>
<tr><th>Method</th><th>Clipped Variable</th><th>Clipping Center</th><th>Clipping Interval</th><th>Constrained TV Distance</th></tr>
</thead>
<tbody>
<tr><td>Standard PPO</td><td>$\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$</td><td>$1$</td><td>$[1-\epsilon, 1+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$</td></tr>
<tr><td>Method 1</td><td>$\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$</td><td>$\rho_k = \pi_k/\pi^{(i)}$</td><td>$[\rho_k-\epsilon, \rho_k+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$</td></tr>
<tr><td>Method 2</td><td>$r = \pi_{k+1}/\pi_k$</td><td>$1$</td><td>$[1-\epsilon, 1+\epsilon]$</td><td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$</td></tr>
</tbody>
</table>
</div>

<p><strong>The Fundamental Problem with Standard PPO Under Multi-Policy Mixture</strong></p>

<p>Standard PPO constrains $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$, requiring the new policy to be simultaneously close to all sampling source policies. By Observation 3.1, when the old policies $\pi^{(1)}, \pi^{(2)}, \ldots$ differ significantly from each other, <strong>no $\pi_{k+1}$ can simultaneously satisfy all constraints</strong>. This causes the trust region intersection to shrink or even become empty, with updates being limited by the most stale policy.</p>

<p><strong>Common Advantages of Methods 1 and 2</strong></p>

<p>Both methods constrain $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$‚Äîthe deviation of the new policy from the <strong>current policy</strong> (rather than the sampling policy). Since $\pi_k$ is uniquely determined, this constraint is consistent across all sample sources, completely avoiding the infeasibility problem.</p>

<p><strong>Method 1 vs Method 2</strong></p>

<div>
<table>
<thead>
<tr><th>Comparison Dimension</th><th>Method 1 (Adaptive Clipping)</th><th>Method 2 (Incremental Clipping)</th></tr>
</thead>
<tbody>
<tr><td>Stale samples ($\rho_k \gg 1$)</td><td>Automatically tightens constraints, more conservative</td><td>May produce large gradient variance</td></tr>
<tr><td>LLM large vocabulary low-probability tokens</td><td>Allows larger absolute changes (additive)</td><td>Absolute changes are limited (multiplicative)</td></tr>
<tr><td>Implementation complexity</td><td>Requires storing $\pi^{(i)}(a|s)$ and $\pi_k(a|s)$</td><td>Only requires $\pi_k(a|s)$</td></tr>
<tr><td>Advantage function</td><td>Uses $A^{\beta^{(k)}}$</td><td>Uses weighted advantage $\rho_k \cdot A^{\beta^{(k)}}$</td></tr>
</tbody>
</table>
</div>

<p><strong>Detailed Explanations</strong>:</p>

<p><strong>(1) Handling Stale Samples</strong></p>

<p>When samples come from very old policies, $\rho_k = \pi_k/\pi^{(i)}$ can be large.</p>

<ul>
  <li>Method 2‚Äôs integrand is $\rho_k \cdot \lvert r - 1\rvert$; even if $\lvert r-1\rvert \leq \epsilon$, the integrand can reach $\epsilon \cdot \rho_k$, producing spikes.</li>
  <li>Method 1 directly constrains $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$; the integrand‚Äôs upper bound is always $\epsilon$, unaffected by $\rho_k$ amplification.</li>
</ul>

<p><strong>(2) LLM Large Vocabulary Issue</strong></p>

<p>Large language models have many tokens having very small probabilities.</p>

<ul>
  <li>Method 2 constrains $\pi_{k+1} \in [(1-\epsilon)\pi_k, (1+\epsilon)\pi_k]$, which is a <strong>multiplicative constraint</strong>: if $\pi_k(a|s) = 10^{-6}$, the allowed absolute change is only $\epsilon \times 10^{-6}$.</li>
  <li>Method 1 constrains $\lvert\pi_{k+1} - \pi_k\rvert \leq \epsilon \cdot \pi^{(i)}$, which is an <strong>additive constraint</strong>: if that token has higher probability under the old policy (e.g., $\pi^{(i)}(a|s) = 0.1$), even if the current probability is very low, faster improvement is allowed.</li>
</ul>

<h3 id="56-controlling-sampling-staleness">5.6 Controlling Sampling Staleness</h3>

<p>Corollary 3.2 shows that $S_k$ also affects the monotonic improvement lower bound, but it <strong>cannot be controlled through optimization-side clipping</strong> and must be implemented by the sampling system:</p>

<p><strong>(1) Discarding Stale Data</strong></p>

<p>Set a threshold $\epsilon_{\mathrm{stale}}$. For each sample, compute $\lvert\rho_k - 1\rvert = \lvert\pi_k(a|s)/\pi^{(i)}(a|s) - 1\rvert$, and discard samples exceeding the threshold.</p>

<p><strong>(2) Controlling Policy Version Window</strong></p>

<p>Limit the number of old policy versions in the mixture sampling, e.g., using only data from the most recent $W$ versions.</p>

<h3 id="57-operational-meaning-of-clipping">5.7 Operational Meaning of Clipping</h3>

<p>Finally, we clarify the relationship between clipping and the theoretical lower bound.</p>

<p>In Corollary 3.2, the coefficient of $U_k$, namely $C_{\pi_{k+1},\beta^{(k)}}$, depends on the new policy $\pi_{k+1}$, so the penalty term <strong>cannot be simply replaced by a constant</strong>. The correct operational meaning is:</p>

<blockquote>
  <p><strong>Maximize the surrogate objective $L_{\beta^{(k)}}(\pi_{k+1})$ subject to the constraint $U_k \leq \epsilon/2$</strong></p>
</blockquote>

<p>The clipping objective function is precisely an implementation of this constrained optimization‚Äîclipping <strong>hard limits</strong> the update magnitude to ensure $U_k$ is controllable; under this premise, gradient ascent improves the surrogate objective, thereby providing guarantees for monotonic policy improvement.</p>

<h3 id="58-section-summary">5.8 Section Summary</h3>

<p>This section established the theoretical foundations of clipping mechanisms:</p>

<ol>
  <li><strong>Lemma 3.3</strong> converts TV distance to sample-level ratio differences, serving as the bridge between theory and implementation</li>
  <li><strong>Two constraint methods</strong>: Method 1 (adaptive clipping center) and Method 2 (fixed incremental clipping), both guaranteeing $U_k \leq \epsilon/2$</li>
  <li><strong>Comparison with standard PPO</strong>: Standard PPO constrains $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$, which is infeasible under multi-policy mixture; Methods 1/2 constrain $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$, avoiding this issue</li>
  <li><strong>Method selection</strong>: Method 1 (adaptive) is recommended for high staleness or LLM large vocabulary scenarios; Method 2 (incremental) is recommended when implementation simplicity is prioritized</li>
  <li><strong>$S_k$ control</strong> is the sampling side‚Äôs responsibility, implemented through data filtering and version windows</li>
  <li><strong>Clipping is constrained optimization</strong>: Maximize the surrogate objective subject to $U_k$ constraints</li>
</ol>

<h2 id="part-vi-comparison-of-trajectory-level-and-stepsegment-level-mixture">Part VI: Comparison of Trajectory-Level and Step/Segment-Level Mixture</h2>

<h3 id="61-core-differences-between-the-two-mechanisms">6.1 Core Differences Between the Two Mechanisms</h3>

<p>The essential difference between the two mixture mechanisms lies in the structure of the index transition kernel:</p>

<ul>
  <li><strong>Trajectory-level mixture</strong>: $q(i‚Äô|i) = \mathbf{1}{i‚Äô=i}$, index never changes</li>
  <li><strong>Step/segment-level mixture</strong>: $\sigma(i) &gt; 0$, allows within-trajectory switching</li>
</ul>

<p>The correspondence with common engineering terminology is:</p>

<ul>
  <li><strong>Trajectory-level mixture</strong> here can be roughly understood as an idealized abstraction of ‚Äú<strong>conventional asynchronous training</strong>‚Äù: data is organized by entire trajectories/episodes belonging to a certain policy version;</li>
  <li><strong>Step/segment-level mixture</strong> here can be roughly understood as an abstraction of ‚Äú<strong>partial rollout</strong>‚Äù: due to asynchrony between actors and learners, and possible refresh to new policy versions at segment boundaries, using an index transition kernel that allows ‚Äúwithin-trajectory version switching‚Äù can better approximate this phenomenon.</li>
</ul>

<p>The key watershed is <strong>whether Lemma 2.1‚Äôs structural simplification holds</strong>: trajectory-level mixture satisfies advantage function reduction; step/segment-level mixture generally does not, because future returns are affected by the index transition kernel.</p>

<h3 id="62-differences-in-sampling-staleness-s_k">6.2 Differences in Sampling Staleness $S_k$</h3>

<p><strong>Trajectory-level mixture</strong>‚Äôs staleness arises from: mixture weights $\alpha_i^{(k)}$ retaining probability mass on old policies after new policy release.</p>

<p><strong>Step/segment-level mixture</strong> has an <strong>exponential compression effect</strong>: Consider a simplified model with switching probability $\sigma$ from old to new. The marginal probability mass on old indices under the discounted visitation distribution is $\frac{1-\gamma}{1-\gamma(1-\sigma)}$. As long as $\sigma \gg 1-\gamma$, the old policy weight can be significantly compressed.</p>

<h3 id="63-differences-in-surrogate-objective-estimation">6.3 Differences in Surrogate Objective Estimation</h3>

<p><strong>Trajectory-level mixture</strong>: The advantage function reduces to $A^{\pi^{(i)}}(s,a)$, with a clear estimation path.</p>

<p><strong>Advantage substitution bias in step/segment-level mixture</strong>: If single-policy advantage estimates are used, systematic bias will arise. The reason is that $A^{\beta^{(k)}}((s,i),a)$ requires taking expectations over future index switching, while $A^{\pi^{(i)}}(s,a)$ implicitly assumes ‚Äúthe future always follows $\pi^{(i)}$.‚Äù</p>

<p><strong>Unification under bandit setting</strong>: In single-step episode LLM training, with no subsequent state transitions, the estimation problems of both mechanisms unify, with no such bias.</p>

<h3 id="64-variance-amplification-risk">6.4 Variance Amplification Risk</h3>

<p>Step/segment-level mixture has another hidden concern: even if single-step importance ratios are clipped, multi-step noise accumulation over long trajectories can still amplify gradient estimation variance. When policy changes per update are large, ‚Äúbehavioral discontinuities‚Äù within trajectories may induce heavier-tailed ratio distributions. This is why trajectory-level mixture is recommended for ‚Äúlarge policy change per update‚Äù scenarios in the table below.</p>

<h3 id="65-applicable-scenarios">6.5 Applicable Scenarios</h3>

<p><strong>Table 6.1„ÄÄApplicable Scenarios for Two Mixture Mechanisms</strong></p>

<div>
<table>
<thead>
<tr><th>Scenario Characteristics</th><th>Recommended Mechanism</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>Long trajectories, high-frequency updates, strong asynchrony</td><td>Step/segment-level</td><td>Can significantly compress $S_k$</td></tr>
<tr><td>Short trajectories (non-bandit)</td><td>Trajectory-level</td><td>$S_k$ is naturally low</td></tr>
<tr><td>Large policy change per update</td><td>Trajectory-level</td><td>Avoids variance amplification</td></tr>
<tr><td>Single-step episode (bandit)</td><td>Either</td><td>Choose based on implementation convenience</td></tr>
<tr><td>Need for compromise</td><td>Segment-level</td><td>Switch at natural boundaries</td></tr>
</tbody>
</table>
</div>

<p><strong>Core trade-off</strong>: Step/segment-level mixture is stronger on the sampling side (fast staleness removal), while trajectory-level mixture is more stable on the estimation side (easier surrogate objective estimation).</p>

<h2 id="part-vii-handling-training-inference-inconsistency">Part VII: Handling Training-Inference Inconsistency</h2>

<h3 id="71-background">7.1 Background</h3>

<p>In large-scale distributed training, policies on the inference side and training side may be inconsistent:</p>

<ul>
  <li><strong>Numerical implementation differences</strong>: softmax normalization, quantization, kernel fusion</li>
  <li><strong>Decoding rule differences</strong>: temperature scaling, top-p/top-k sampling</li>
</ul>

<p>Let the behavior policy modeled on the training side be $\pi^{(i)}$, while the policy actually sampling on the inference side is $\hat{\pi}^{(i)}$.</p>

<h3 id="72-effective-staleness">7.2 Effective Staleness</h3>

<p>Define <strong>effective staleness</strong>:</p>

\[\hat{S}_k := \mathbb{E}_{(s,i) \sim d_{\hat{\beta}^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_k, \hat{\pi}^{(i)}; s) \big]\]

<p>This definition simultaneously covers version staleness and training-inference implementation differences.</p>

<h3 id="73-actionable-control">7.3 Actionable Control</h3>

<p>By Lemma 3.3, $\hat{S}_k$ can be expressed in sample-level computable form. Given threshold $\epsilon_{\mathrm{stale}}$, if training only uses samples satisfying $\lvert\pi_k(a|s)/\hat{\pi}^{(i)}(a|s) - 1\rvert \leq \epsilon_{\mathrm{stale}}$, then $\hat{S}_k \leq \epsilon_{\mathrm{stale}}/2$.</p>

<p><strong>Key implementation points</strong>:</p>

<ol>
  <li><strong>Behavior denominator alignment</strong>: The behavior probability in the loss should use the inference-side recorded $\hat{\pi}^{(i)}(a|s)$</li>
  <li><strong>Probability smoothing</strong>: If the inference side has truncation (e.g., top-k), ensure ratios are valid</li>
</ol>

<h2 id="summary-practical-guidelines">Summary: Practical Guidelines</h2>

<h3 id="core-theoretical-framework">Core Theoretical Framework</h3>

<p>The structure of the monotonic improvement lower bound is:</p>

\[J(\pi_{k+1}) - J(\pi_k) \geq \underbrace{L_{\beta^{(k)}}(\pi_{k+1})}_{\text{surrogate objective}} - \underbrace{C_1 \cdot U_k}_{\text{update shift penalty}} - \underbrace{C_2 \cdot S_k}_{\text{sampling staleness penalty}}\]

<h3 id="separation-of-concerns-principle">Separation of Concerns Principle</h3>

<div>
<table>
<thead>
<tr><th>Control Term</th><th>Responsible Party</th><th>Control Mechanism</th><th>Specific Operation</th></tr>
</thead>
<tbody>
<tr><td>$U_k$</td><td>Optimization algorithm</td><td>Policy clipping</td><td>Clip $\pi_{k+1}/\pi_k$</td></tr>
<tr><td>$S_k$</td><td>Sampling system</td><td>Data filtering</td><td>Discard stale samples</td></tr>
<tr><td>$S_k$</td><td>Sampling system</td><td>Version window</td><td>Use only most recent $W$ versions</td></tr>
</tbody>
</table>
</div>

<h3 id="clipping-method-selection">Clipping Method Selection</h3>

<div>
<table>
<thead>
<tr><th>Scenario</th><th>Recommended Method</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>High staleness</td><td>Method 1 (adaptive)</td><td>Automatically tightens constraints for stale samples</td></tr>
<tr><td>Implementation simplicity prioritized</td><td>Method 2 (incremental)</td><td>No need to store old policy information</td></tr>
<tr><td>LLM large vocabulary</td><td>Method 1</td><td>Avoids slow updates for low-probability tokens</td></tr>
</tbody>
</table>
</div>

<h3 id="handling-training-inference-inconsistency">Handling Training-Inference Inconsistency</h3>

<ul>
  <li>Use inference-side recorded $\hat{\pi}^{(i)}$ as the behavior denominator</li>
  <li>Compress effective staleness through sample filtering</li>
</ul>

<h2 id="appendix-quick-reference-for-key-symbols">Appendix: Quick Reference for Key Symbols</h2>

<div>
<table>
<thead>
<tr><th>Symbol</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td>$\pi_k$, $\pi^{(i)}$</td><td>Latest policy at round $k$, $i$-th old policy</td></tr>
<tr><td>$d_\pi(s)$, $A^\pi(s,a)$</td><td>Discounted state visitation distribution, advantage function</td></tr>
<tr><td>$D_{\mathrm{TV}}(\pi, \pi'; s)$</td><td>TV distance between two policies at state $s$</td></tr>
<tr><td>$\beta^{(k)}(a \mid s, i) := \pi^{(i)}(a \mid s)$</td><td>Mixture behavior policy at round $k$</td></tr>
<tr><td>$q(i' \mid i)$, $\alpha_i^{(k)}$</td><td>Index transition kernel, initial index distribution</td></tr>
<tr><td>$U_k$, $S_k$</td><td>Update increment shift, sampling staleness</td></tr>
<tr><td>$\epsilon$, $\epsilon_{\mathrm{stale}}$, $W$</td><td>Clipping radius, staleness threshold, version window</td></tr>
<tr><td>$C_{\pi,\pi_k}$</td><td>Expected advantage upper bound constant</td></tr>
</tbody>
</table>
</div>

<h2 id="references">References</h2>

<ol>
  <li>
    <p>John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. ‚ÄúTrust Region Policy Optimization‚Äù (TRPO). arXiv:1502.05477. <a href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a></p>
  </li>
  <li>
    <p>Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel. ‚ÄúConstrained Policy Optimization‚Äù (CPO). arXiv:1705.10528. <a href="https://arxiv.org/abs/1705.10528">https://arxiv.org/abs/1705.10528</a></p>
  </li>
  <li>
    <p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. ‚ÄúProximal Policy Optimization Algorithms‚Äù (PPO). arXiv:1707.06347. <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
  </li>
  <li>
    <p>James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras. ‚ÄúGeneralized Proximal Policy Optimization with Sample Reuse‚Äù (GePPO). arXiv:2111.00072. <a href="https://arxiv.org/abs/2111.00072">https://arxiv.org/abs/2111.00072</a></p>
  </li>
  <li>
    <p>Yuzhen Zhou, Jiajun Li, Yusheng Su, et al. ‚ÄúAPRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation‚Äù (APRIL; partial rollout). arXiv:2509.18521. <a href="https://arxiv.org/abs/2509.18521">https://arxiv.org/abs/2509.18521</a></p>
  </li>
  <li>
    <p>Jacob Hilton, Karl Cobbe, John Schulman. ‚ÄúBatch size-invariance for policy optimization‚Äù (Decoupled PPO). arXiv:2110.00641. <a href="https://arxiv.org/abs/2110.00641">https://arxiv.org/abs/2110.00641</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025OffPolicyLLMRL</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Off-Policy Training in LLM Reinforcement Learning: From Theory to Practice}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html}</span><span class="p">,</span>
	<span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-12-17}</span>
<span class="p">}</span>
</code></pre></div></div>

  </article></div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2025 Xihuai Leo Wang. Last updated: December 23, 2025.
      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    <!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2923RQZBXG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-2923RQZBXG');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
