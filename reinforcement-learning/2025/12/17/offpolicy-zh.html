<!DOCTYPE html><html lang="zh-CN">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证 | Xihuai Wang's Page</title><meta name="author" content="Xihuai Leo Wang" />
    <meta name="description" content="系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。" />
    <meta name="keywords" content="Reinforcement Learning, Multi-agent System, Language Model" />

    <!-- 
    Unified Share Image URL Calculation
    Priority:
    1) page.disable_share_image / page.share_image: none  -> no image
    2) page.og_image                                     -> explicit per-page image
    3) dynamic generation (posts)                         -> horizontal OG via Tailgraph
    4) site.og_image == 'auto'                            -> horizontal OG via Tailgraph for non-post pages
    5) site.og_image                                      -> site default
    This keeps homepage free to use a portrait image while giving blog pages a better horizontal card.
    --><!-- Itemprop meta tags for Zhihu and other platforms using Schema.org microdata -->
    <meta itemprop="name" content="Xihuai&#39;s Blog | 驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证" />
    <meta itemprop="description" content="系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。" /><meta itemprop="image" content="https://og.tailgraph.com/og?fontFamily=Inter&title=Xihuai%27s+Blog+%7C+%E9%A9%AF%E6%9C%8D%E9%99%88%E6%97%A7%E6%95%B0%E6%8D%AE%EF%BC%9ALLM+%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%8D%95%E8%B0%83%E6%94%B9%E8%BF%9B%E4%BF%9D%E8%AF%81&titleTailwind=font-bold%20text-4xl%20text-white&titleFontFamily=Inter&text=%E7%B3%BB%E7%BB%9F%E6%8E%A8%E5%AF%BC%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E7%90%86%E8%AE%BA%EF%BC%9A%E4%BB%8E%E5%8D%95%E7%AD%96%E7%95%A5%E9%87%87%E6%A0%B7%E7%9A%84%E6%80%A7%E8%83%BD%E6%94%B9%E8%BF%9B%E4%B8%8B%E7%95%8C%E5%87%BA%E5%8F%91%EF%BC%8C%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%9A%E7%AD%96%E7%95%A5%E9%9D%99%E6%80%81%2F%E5%8A%A8%E6%80%81%E6%B7%B7%E5%90%88%E9%87%87%E6%A0%B7%EF%BC%8C%E7%BB%99%E5%87%BA%E5%8D%95%E8%B0%83%E6%8F%90%E5%8D%87%E7%9A%84%E5%85%85%E5%88%86%E6%9D%A1%E4%BB%B6%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E4%B8%89%E8%A7%92%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%88%86%E8%A7%A3%E5%B0%86%E7%BA%A6%E6%9D%9F%E6%8B%86%E5%88%86%E4%B8%BA%E6%9B%B4%E6%96%B0%E5%A2%9E%E9%87%8F%E5%81%8F%E7%A7%BB%EF%BC%88%E4%BC%98%E5%8C%96%E4%BE%A7%E5%8F%AF%E6%8E%A7%EF%BC%89%E4%B8%8E%E9%87%87%E6%A0%B7...&textTailwind=text-lg%20mt-4%20text-gray-300&textFontFamily=Inter&logoTailwind=h-12&bgTailwind=bg-gradient-to-br%20from-green-900%20via-emerald-800%20to-teal-900&footer=Xihuai+Wang%20%C2%B7%202025-12-17&footerTailwind=text-gray-400&containerTailwind=p-12" /><!-- WeChat/Weibo/QQ specific meta tags for Chinese social platforms --><meta name="weibo:article:title" content="Xihuai&#39;s Blog | 驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证" />
    <meta name="weibo:article:description" content="系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。" /><meta name="weibo:article:image" content="https://og.tailgraph.com/og?fontFamily=Inter&title=Xihuai%27s+Blog+%7C+%E9%A9%AF%E6%9C%8D%E9%99%88%E6%97%A7%E6%95%B0%E6%8D%AE%EF%BC%9ALLM+%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%8D%95%E8%B0%83%E6%94%B9%E8%BF%9B%E4%BF%9D%E8%AF%81&titleTailwind=font-bold%20text-4xl%20text-white&titleFontFamily=Inter&text=%E7%B3%BB%E7%BB%9F%E6%8E%A8%E5%AF%BC%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E7%90%86%E8%AE%BA%EF%BC%9A%E4%BB%8E%E5%8D%95%E7%AD%96%E7%95%A5%E9%87%87%E6%A0%B7%E7%9A%84%E6%80%A7%E8%83%BD%E6%94%B9%E8%BF%9B%E4%B8%8B%E7%95%8C%E5%87%BA%E5%8F%91%EF%BC%8C%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%9A%E7%AD%96%E7%95%A5%E9%9D%99%E6%80%81%2F%E5%8A%A8%E6%80%81%E6%B7%B7%E5%90%88%E9%87%87%E6%A0%B7%EF%BC%8C%E7%BB%99%E5%87%BA%E5%8D%95%E8%B0%83%E6%8F%90%E5%8D%87%E7%9A%84%E5%85%85%E5%88%86%E6%9D%A1%E4%BB%B6%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E4%B8%89%E8%A7%92%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%88%86%E8%A7%A3%E5%B0%86%E7%BA%A6%E6%9D%9F%E6%8B%86%E5%88%86%E4%B8%BA%E6%9B%B4%E6%96%B0%E5%A2%9E%E9%87%8F%E5%81%8F%E7%A7%BB%EF%BC%88%E4%BC%98%E5%8C%96%E4%BE%A7%E5%8F%AF%E6%8E%A7%EF%BC%89%E4%B8%8E%E9%87%87%E6%A0%B7...&textTailwind=text-lg%20mt-4%20text-gray-300&textFontFamily=Inter&logoTailwind=h-12&bgTailwind=bg-gradient-to-br%20from-green-900%20via-emerald-800%20to-teal-900&footer=Xihuai+Wang%20%C2%B7%202025-12-17&footerTailwind=text-gray-400&containerTailwind=p-12" />

    <!-- OpenGraph -->
    <meta property="og:site_name" content="Xihuai Wang's Page" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-12-17T00:00:00+00:00" /><meta property="article:author" content="Xihuai Leo Wang" /><meta property="og:title" content="Xihuai&#39;s Blog | 驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证" />
    <meta property="og:url" content="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh.html" />
    <meta property="og:description" content="系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。" /><meta property="og:image" content="https://og.tailgraph.com/og?fontFamily=Inter&title=Xihuai%27s+Blog+%7C+%E9%A9%AF%E6%9C%8D%E9%99%88%E6%97%A7%E6%95%B0%E6%8D%AE%EF%BC%9ALLM+%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%8D%95%E8%B0%83%E6%94%B9%E8%BF%9B%E4%BF%9D%E8%AF%81&titleTailwind=font-bold%20text-4xl%20text-white&titleFontFamily=Inter&text=%E7%B3%BB%E7%BB%9F%E6%8E%A8%E5%AF%BC%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E7%90%86%E8%AE%BA%EF%BC%9A%E4%BB%8E%E5%8D%95%E7%AD%96%E7%95%A5%E9%87%87%E6%A0%B7%E7%9A%84%E6%80%A7%E8%83%BD%E6%94%B9%E8%BF%9B%E4%B8%8B%E7%95%8C%E5%87%BA%E5%8F%91%EF%BC%8C%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%9A%E7%AD%96%E7%95%A5%E9%9D%99%E6%80%81%2F%E5%8A%A8%E6%80%81%E6%B7%B7%E5%90%88%E9%87%87%E6%A0%B7%EF%BC%8C%E7%BB%99%E5%87%BA%E5%8D%95%E8%B0%83%E6%8F%90%E5%8D%87%E7%9A%84%E5%85%85%E5%88%86%E6%9D%A1%E4%BB%B6%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E4%B8%89%E8%A7%92%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%88%86%E8%A7%A3%E5%B0%86%E7%BA%A6%E6%9D%9F%E6%8B%86%E5%88%86%E4%B8%BA%E6%9B%B4%E6%96%B0%E5%A2%9E%E9%87%8F%E5%81%8F%E7%A7%BB%EF%BC%88%E4%BC%98%E5%8C%96%E4%BE%A7%E5%8F%AF%E6%8E%A7%EF%BC%89%E4%B8%8E%E9%87%87%E6%A0%B7...&textTailwind=text-lg%20mt-4%20text-gray-300&textFontFamily=Inter&logoTailwind=h-12&bgTailwind=bg-gradient-to-br%20from-green-900%20via-emerald-800%20to-teal-900&footer=Xihuai+Wang%20%C2%B7%202025-12-17&footerTailwind=text-gray-400&containerTailwind=p-12" />
    <meta property="og:image:secure_url" content="https://og.tailgraph.com/og?fontFamily=Inter&title=Xihuai%27s+Blog+%7C+%E9%A9%AF%E6%9C%8D%E9%99%88%E6%97%A7%E6%95%B0%E6%8D%AE%EF%BC%9ALLM+%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%8D%95%E8%B0%83%E6%94%B9%E8%BF%9B%E4%BF%9D%E8%AF%81&titleTailwind=font-bold%20text-4xl%20text-white&titleFontFamily=Inter&text=%E7%B3%BB%E7%BB%9F%E6%8E%A8%E5%AF%BC%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E7%90%86%E8%AE%BA%EF%BC%9A%E4%BB%8E%E5%8D%95%E7%AD%96%E7%95%A5%E9%87%87%E6%A0%B7%E7%9A%84%E6%80%A7%E8%83%BD%E6%94%B9%E8%BF%9B%E4%B8%8B%E7%95%8C%E5%87%BA%E5%8F%91%EF%BC%8C%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%9A%E7%AD%96%E7%95%A5%E9%9D%99%E6%80%81%2F%E5%8A%A8%E6%80%81%E6%B7%B7%E5%90%88%E9%87%87%E6%A0%B7%EF%BC%8C%E7%BB%99%E5%87%BA%E5%8D%95%E8%B0%83%E6%8F%90%E5%8D%87%E7%9A%84%E5%85%85%E5%88%86%E6%9D%A1%E4%BB%B6%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E4%B8%89%E8%A7%92%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%88%86%E8%A7%A3%E5%B0%86%E7%BA%A6%E6%9D%9F%E6%8B%86%E5%88%86%E4%B8%BA%E6%9B%B4%E6%96%B0%E5%A2%9E%E9%87%8F%E5%81%8F%E7%A7%BB%EF%BC%88%E4%BC%98%E5%8C%96%E4%BE%A7%E5%8F%AF%E6%8E%A7%EF%BC%89%E4%B8%8E%E9%87%87%E6%A0%B7...&textTailwind=text-lg%20mt-4%20text-gray-300&textFontFamily=Inter&logoTailwind=h-12&bgTailwind=bg-gradient-to-br%20from-green-900%20via-emerald-800%20to-teal-900&footer=Xihuai+Wang%20%C2%B7%202025-12-17&footerTailwind=text-gray-400&containerTailwind=p-12" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:image:alt" content="Xihuai&#39;s Blog | 驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证" /><meta property="og:locale" content="zh_CN" />

        <!-- Twitter card --><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="Xihuai&#39;s Blog | 驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证" />
    <meta name="twitter:description" content="系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。" /><meta name="twitter:image" content="https://og.tailgraph.com/og?fontFamily=Inter&title=Xihuai%27s+Blog+%7C+%E9%A9%AF%E6%9C%8D%E9%99%88%E6%97%A7%E6%95%B0%E6%8D%AE%EF%BC%9ALLM+%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%8D%95%E8%B0%83%E6%94%B9%E8%BF%9B%E4%BF%9D%E8%AF%81&titleTailwind=font-bold%20text-4xl%20text-white&titleFontFamily=Inter&text=%E7%B3%BB%E7%BB%9F%E6%8E%A8%E5%AF%BC%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E7%90%86%E8%AE%BA%EF%BC%9A%E4%BB%8E%E5%8D%95%E7%AD%96%E7%95%A5%E9%87%87%E6%A0%B7%E7%9A%84%E6%80%A7%E8%83%BD%E6%94%B9%E8%BF%9B%E4%B8%8B%E7%95%8C%E5%87%BA%E5%8F%91%EF%BC%8C%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%9A%E7%AD%96%E7%95%A5%E9%9D%99%E6%80%81%2F%E5%8A%A8%E6%80%81%E6%B7%B7%E5%90%88%E9%87%87%E6%A0%B7%EF%BC%8C%E7%BB%99%E5%87%BA%E5%8D%95%E8%B0%83%E6%8F%90%E5%8D%87%E7%9A%84%E5%85%85%E5%88%86%E6%9D%A1%E4%BB%B6%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E4%B8%89%E8%A7%92%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%88%86%E8%A7%A3%E5%B0%86%E7%BA%A6%E6%9D%9F%E6%8B%86%E5%88%86%E4%B8%BA%E6%9B%B4%E6%96%B0%E5%A2%9E%E9%87%8F%E5%81%8F%E7%A7%BB%EF%BC%88%E4%BC%98%E5%8C%96%E4%BE%A7%E5%8F%AF%E6%8E%A7%EF%BC%89%E4%B8%8E%E9%87%87%E6%A0%B7...&textTailwind=text-lg%20mt-4%20text-gray-300&textFontFamily=Inter&logoTailwind=h-12&bgTailwind=bg-gradient-to-br%20from-green-900%20via-emerald-800%20to-teal-900&footer=Xihuai+Wang%20%C2%B7%202025-12-17&footerTailwind=text-gray-400&containerTailwind=p-12" />
    <meta name="twitter:image:alt" content="Xihuai&#39;s Blog | 驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证" />
    

    <!-- Schema.org -->
    <script type="application/ld+json">
      {
        "@context": "https://schema.org","@type": "BlogPosting",
        "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh.html"
        },
        "headline": "Xihuai&#39;s Blog | 驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证",
        "description": "系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。","image": "https://og.tailgraph.com/og?fontFamily=Inter&title=Xihuai%27s+Blog+%7C+%E9%A9%AF%E6%9C%8D%E9%99%88%E6%97%A7%E6%95%B0%E6%8D%AE%EF%BC%9ALLM+%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%8D%95%E8%B0%83%E6%94%B9%E8%BF%9B%E4%BF%9D%E8%AF%81&titleTailwind=font-bold%20text-4xl%20text-white&titleFontFamily=Inter&text=%E7%B3%BB%E7%BB%9F%E6%8E%A8%E5%AF%BC%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BC%82%E7%AD%96%E7%95%A5%E8%AE%AD%E7%BB%83%E7%90%86%E8%AE%BA%EF%BC%9A%E4%BB%8E%E5%8D%95%E7%AD%96%E7%95%A5%E9%87%87%E6%A0%B7%E7%9A%84%E6%80%A7%E8%83%BD%E6%94%B9%E8%BF%9B%E4%B8%8B%E7%95%8C%E5%87%BA%E5%8F%91%EF%BC%8C%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%9A%E7%AD%96%E7%95%A5%E9%9D%99%E6%80%81%2F%E5%8A%A8%E6%80%81%E6%B7%B7%E5%90%88%E9%87%87%E6%A0%B7%EF%BC%8C%E7%BB%99%E5%87%BA%E5%8D%95%E8%B0%83%E6%8F%90%E5%8D%87%E7%9A%84%E5%85%85%E5%88%86%E6%9D%A1%E4%BB%B6%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E4%B8%89%E8%A7%92%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%88%86%E8%A7%A3%E5%B0%86%E7%BA%A6%E6%9D%9F%E6%8B%86%E5%88%86%E4%B8%BA%E6%9B%B4%E6%96%B0%E5%A2%9E%E9%87%8F%E5%81%8F%E7%A7%BB%EF%BC%88%E4%BC%98%E5%8C%96%E4%BE%A7%E5%8F%AF%E6%8E%A7%EF%BC%89%E4%B8%8E%E9%87%87%E6%A0%B7...&textTailwind=text-lg%20mt-4%20text-gray-300&textFontFamily=Inter&logoTailwind=h-12&bgTailwind=bg-gradient-to-br%20from-green-900%20via-emerald-800%20to-teal-900&footer=Xihuai+Wang%20%C2%B7%202025-12-17&footerTailwind=text-gray-400&containerTailwind=p-12","author": {
          "@type": "Person",
          "name": "Xihuai Leo Wang"
        },
        "publisher": {
          "@type": "Person",
          "name": "Xihuai Leo Wang"
        },"datePublished": "2025-12-17T00:00:00+00:00","dateModified": "2025-12-17T00:00:00+00:00","url": "https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh.html",
        "sameAs": ["https://scholar.google.com/citations?user=hy6v3qUAAAAJ","https://github.com/xihuai18"]
      }
    </script>


    <!-- DNS Prefetch & Preconnect for faster external resource loading -->
    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://fonts.googleapis.com">
    <link rel="dns-prefetch" href="https://fonts.gstatic.com">
    <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" /><!-- Pseudocode -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"><!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🤖</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-zh.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

    <!-- Prefetch/Preload for faster navigation -->
    <link rel="prefetch" href="/" as="document">
    <link rel="prefetch" href="/blog/" as="document">
    <link rel="prefetch" href="/publications/" as="document">
    <link rel="prefetch" href="/cv/" as="document">
    
    <!-- Instant.page for instant page loads on hover -->
    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module" defer></script>

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">Xihuai Wang's Page</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">About</a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">Blog</a>
          </li>

          <!-- CV -->
          <!-- 
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/" target="_blank"
              rel="noopener noreferrer">cv</a>
          </li> -->
          <!-- Other pages -->
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">Publications</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/cv/">CV</a>
          </li>
          <!-- Toggle theme mode -->
          <li class="nav-item toggle-container">
            <button id="light-toggle" class="nav-link" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post toc-layout">
  <header class="post-header">
    <h1 class="post-title">驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证</h1>
    <div class="post-meta-container">
      <div class="post-meta-row">
        <span class="post-date">
          <i class="far fa-calendar-alt"></i>
          December 17, 2025
        </span></div>
      <div class="post-tags-row">
        <a href="/blog/?year=2025" data-filter-link data-filter-type="year" data-filter-value="2025">
          📅 2025
        </a>
          &nbsp; &middot; &nbsp;
          
            <a href="/blog/?category=reinforcement-learning" data-filter-link data-filter-type="category" data-filter-value="reinforcement-learning">
              🏷️ reinforcement-learning
            </a>
            
          
      </div>
    </div>
  </header>

  <div class="post-links">
  

  <!-- Bilingual Links -->
  

  

  <!-- External Platform Links -->
  

  

  <!-- External Source (from plugin) -->
  

  <!-- Output links -->
  
    <a href="/reinforcement-learning/2025/12/17/offpolicy-en.html">English Version</a>
  
</div>


  <div class="row">
    
      <div class="col-lg toc-content">
    

      <article class="post-content">
        <h2 id="引言为什么我们需要关心异策略">引言：为什么我们需要关心”异策略”？</h2>

<p>设想这样一个场景：你正在使用强化学习训练一个大语言模型，希望它能够更好地回答问题。理想情况下，每当模型生成一批回答后，你都会立即用这些数据来更新模型，接着用更新后的模型生成新的数据，如此循环往复。这种”谁的数据就用来更新谁”的方式称为<strong>同策略</strong>（on-policy）训练。</p>

<p>然而，现实情况往往更为复杂。在大规模分布式训练中，数百个GPU并行生成数据，而模型更新需要时间。当新模型发布时，大量由旧版本模型生成的数据尚未被使用——直接丢弃过于浪费，但继续使用又担心数据”过时”会影响训练效果。</p>

<p>这便是<strong>异策略</strong>（off-policy）训练所面临的核心问题：<strong>用旧策略采集的数据来更新新策略，能否保证性能持续提升？</strong></p>

<p>本文将系统地回答这个问题。我们从基础理论出发，逐步推导出可操作的条件，阐明在何种情况下，混合使用多个版本策略的数据仍然能够保证训练的单调改进。</p>

<h2 id="第一部分理论基础">第一部分：理论基础</h2>

<h3 id="11-基本设定">1.1 基本设定</h3>

<p>我们考虑一个标准的马尔可夫决策过程（MDP），包含状态空间 $\mathcal{S}$、动作空间 $\mathcal{A}$、转移概率 $p(s'\mid s,a)$、奖励函数 $r(s,a)$、初始状态分布 $\rho_0$ 和折扣因子 $\gamma \in (0,1)$。</p>

<p>策略 $\pi$ 的<strong>期望累计折扣回报</strong>为：</p>

<p>$$
J(\pi) := \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid \pi\right]
$$</p>

<h4 id="折扣状态访问分布">折扣状态访问分布</h4>

<p>定义为策略长期运行中访问各状态的加权频率：</p>

<p>$$
d_\pi(s) := (1-\gamma) \sum_{t=0}^{\infty} \gamma^t \Pr(s_t = s \mid \pi)
$$</p>

<h4 id="优势函数">优势函数</h4>

<p>用于衡量在状态 $s$ 下选择动作 $a$ 相对于策略平均水平的优劣：</p>

<p>$$
A^\pi(s,a) := Q^\pi(s,a) - V^\pi(s)
$$</p>

<h4 id="全变差距离tv-距离">全变差距离（TV 距离）</h4>

<p>用于衡量两个策略在给定状态 $s$ 下动作分布的差异：</p>

<p>$$
D_{\mathrm{TV}}(\pi, \pi'; s) := \frac{1}{2} \sum_{a \in \mathcal{A}} |\pi(a \mid s) - \pi'(a \mid s)|
$$</p>

<p>本文统一用 $\mid$ 表示条件概率（例如 $\pi(a\mid s)$），并保留 $\|\cdot\|$ 表示范数。</p>

<h3 id="12-核心工具策略性能差异引理">1.2 核心工具：策略性能差异引理</h3>

<p>整个理论的基石是如下简洁的结论：</p>

<blockquote>
  <p><strong>引理1.1（策略性能差异引理）</strong></p>

  <p>对于任意旧策略 $\pi_k$ 和新策略 $\pi$，性能差异可表示为：</p>

  <p>$$
J(\pi) - J(\pi_k) = \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_\pi}\left[ \mathbb{E}_{a \sim \pi(\cdot \mid s)}[A^{\pi_k}(s,a)] \right]
$$</p>
</blockquote>

<p><strong>直观理解</strong>：新策略相较于旧策略的改进程度，等于在新策略访问的状态分布下，采用新策略选择动作所能获得的“平均优势”。</p>

<h2 id="第二部分单策略采样的性能改进下界">第二部分：单策略采样的性能改进下界</h2>

<h3 id="21-分布不匹配与分布差异控制">2.1 分布不匹配与分布差异控制</h3>

<p>策略性能差异引理存在一个实际问题：右侧的期望是在新策略的状态分布 $d_\pi$ 下计算的，而我们只能从旧策略的分布 $d_{\pi_k}$ 中采样。</p>

<p>解决思路是：将期望分解为“旧分布下的期望”与“偏差项”两部分，然后对偏差项加以控制。关键问题在于：<strong>状态分布的差异与策略的差异之间存在怎样的定量关系？</strong></p>

<h4 id="状态分布差异的控制">状态分布差异的控制</h4>

<blockquote>
  <p><strong>引理1.2（状态分布差异与策略TV距离的关系）</strong></p>

  <p>$$
\|d_\pi - d_{\pi_k}\|_1 \leq \frac{2\gamma}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]
$$</p>
</blockquote>

<h4 id="物理意义">物理意义</h4>

<p>策略在动作空间上的微小差异，会通过环境动力学被“放大”为状态访问分布的差异。系数 $\frac{\gamma}{1-\gamma}$ 反映了<strong>时间累积效应</strong>——在长时域任务中（$\gamma$ 接近1），放大效应更为显著。</p>

<h4 id="证明思路">证明思路</h4>

<p>通过推导折扣访问分布的不动点方程，并利用随机矩阵的 $\ell_1$ 非扩张性，可以证明状态分布差异被策略差异通过转移动力学放大，且放大系数正是 $\frac{\gamma}{1-\gamma}$。</p>

<h3 id="22-策略性能改进下界">2.2 策略性能改进下界</h3>

<blockquote>
  <p><strong>定理1.1（策略性能改进下界）</strong></p>

  <p>定义期望优势上界常数 $C_{\pi,\pi_k} := \max_{s} \lvert \mathbb{E}_{a \sim \pi}[A^{\pi_k}(s,a)] \rvert$，则：</p>

  <p>$$
J(\pi) - J(\pi_k) \geq L_{\pi_k}(\pi) - \frac{2\gamma C_{\pi,\pi_k}}{(1-\gamma)^2} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]
$$</p>

  <p>其中<strong>代理目标</strong>为：</p>

  <p>$$
L_{\pi_k}(\pi) := \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}, a \sim \pi_k} \left[ \frac{\pi(a \mid s)}{\pi_k(a \mid s)} A^{\pi_k}(s,a) \right]
$$</p>
</blockquote>

<p>该下界由两部分组成：</p>

<ol>
  <li>
    <p><strong>代理目标</strong> $L_{\pi_k}(\pi)$：可通过旧策略数据利用重要性采样直接估计，它是TRPO/PPO等算法的优化目标。</p>
  </li>
  <li>
    <p><strong>策略偏移惩罚</strong>：随着新旧策略的TV距离增大而增加，这解释了为何PPO等算法需要限制更新幅度。</p>
  </li>
</ol>

<p><strong>核心结论</strong>：在最大化代理目标的同时控制策略偏移，即可保证性能的改进。</p>

<h2 id="第三部分多策略静态混合采样">第三部分：多策略静态混合采样</h2>

<h3 id="31-问题设定与统一建模静态混合">3.1 问题设定与统一建模（静态混合）</h3>

<p>在实际训练中，一个批次的数据可能来自多个策略版本 $\{\pi^{(1)}, \ldots, \pi^{(M)}\}$，各版本占比为 $\alpha_1, \ldots, \alpha_M$。如何将定理1.1推广到这种情形？</p>

<p><strong>核心思想：扩展状态空间</strong></p>

<p>解决方案采用了一个优雅的建模技巧：<strong>将策略版本索引作为状态的一部分</strong>。</p>

<p>定义扩展状态空间 $\tilde{\mathcal{S}} := \mathcal{S} \times \mathcal{I}$，其中 $\mathcal{I} = \{1, \ldots, M\}$ 是策略索引集合。在扩展状态 $(s, i)$ 下，<strong>混合行为策略</strong>定义为 $\beta(a \mid s, i) := \pi^{(i)}(a \mid s)$。</p>

<p>索引的演化由<strong>索引转移核</strong> $q(i' \mid i)$ 刻画。扩展MDP继承原始MDP的奖励和环境转移，索引按 $q(i'\mid i)$ 独立演化。</p>

<p>这个技巧之所以有效，是因为新策略 $\pi$ 在扩展MDP上的回报与在原始MDP中的回报相同，从而可以直接应用定理1.1。</p>

<h3 id="32-轨迹级混合结构简化与改进下界">3.2 轨迹级混合：结构简化与改进下界</h3>

<p>最常见的情形是<strong>每条轨迹仅使用一个旧策略</strong>：在轨迹开始时采样索引 $I_0 \sim \alpha$，随后整条轨迹都使用策略 $\pi^{(I_0)}$。此时索引转移核为恒等转移：$q(i' \mid i) = \mathbf{1}_{i'=i}$。</p>

<p>从工程实现的角度看，在许多 <strong>actor-learner 的异步训练</strong>架构中（如果采样端与训练端将数据按“整条轨迹/完整 episode 归属于某个策略版本”的方式组织），这可以近似对应这里的<strong>轨迹级混合</strong>：actor 在一个采样单元内固定使用某个策略快照生成数据，learner 再混合使用来自不同版本的整条轨迹数据进行更新。这里使用“近似”一词，是因为不同系统对“轨迹/采样单元”的切分边界可能不完全一致。</p>

<blockquote>
  <p><strong>引理2.1（轨迹级混合的结构简化）</strong></p>

  <p>(a) 扩展状态访问分布分解为：$d_{\beta}(s, i) = \alpha_i \cdot d_{\pi^{(i)}}(s)$</p>

  <p>(b) 优势函数还原为：$A^{\beta}((s, i), a) = A^{\pi^{(i)}}(s, a)$</p>
</blockquote>

<p><strong>(b) 的直观理解</strong>：由于索引永不改变，从扩展状态 $(s,i)$ 出发的<strong>所有未来轨迹</strong>都由同一个策略 $\pi^{(i)}$ 生成。因此，未来的累计回报完全由 $\pi^{(i)}$ 决定，价值函数和优势函数自然还原为 $\pi^{(i)}$ 的对应量。</p>

<p>因此，混合策略的回报等于各旧策略回报的加权平均：$J_{\mathrm{mix}} = \sum_{i=1}^{M} \alpha_i J(\pi^{(i)})$。</p>

<p><strong>改进下界</strong></p>

<blockquote>
  <p><strong>推论2.1（轨迹级混合的性能改进下界）</strong></p>

  <p>$$
J(\pi) - \sum_{i=1}^{M} \alpha_i J(\pi^{(i)}) \geq \sum_{i=1}^{M} \alpha_i L_{\pi^{(i)}}(\pi) - \frac{2\gamma \max_i C_{\pi, \pi^{(i)}}}{(1-\gamma)^2} \sum_{i=1}^{M} \alpha_i \mathbb{E}_{s \sim d_{\pi^{(i)}}} \big[ D_{\mathrm{TV}}(\pi, \pi^{(i)}; s) \big]
$$</p>
</blockquote>

<p>该结论表明：当混合使用多个旧策略版本的轨迹进行训练时，若对每条轨迹使用对应旧策略的重要性比率来构造损失，并同时控制新策略与各旧策略之间的偏移，则新策略的性能将有明确的改进下界。</p>

<h2 id="第四部分动态混合采样与单调提升条件">第四部分：动态混合采样与单调提升条件</h2>

<h3 id="41-问题与统一建模动态混合">4.1 问题与统一建模（动态混合）</h3>

<p>第三部分讨论的是<strong>静态混合</strong>——混合权重 $\alpha_i$ 固定不变。本节考虑更一般的<strong>动态混合</strong>——即新策略发布后，采样逐步由新策略接管的过程。</p>

<p>前面的结论刻画了“新策略相对于混合行为策略”的改进。但在实际训练中，我们真正关心的是：<strong>每轮更新后的最新策略 $\pi_{k+1}$ 相对于上一轮最新策略 $\pi_k$ 是否具有单调提升性？</strong></p>

<p>$$
J(\pi_{k+1}) \geq J(\pi_k)
$$</p>

<h4 id="统一建模框架">统一建模框架</h4>

<p>动态混合采样的两种典型形式都可以用索引转移核 $q(i'\mid i)$ 统一刻画：</p>

<p><strong>轨迹级混合</strong>（可类比为常规异步训练的一个抽象；索引恒等转移）：$q(i'\mid i) = \mathbf{1}\{i'=i\}$</p>

<p><strong>步/段级混合</strong>（partial rollout / 段式采样的一个抽象；允许切换）：$q(i'\mid i) = (1-\sigma(i))\mathbf{1}\{i'=i\} + \sigma(i)\kappa(i'\mid i)$</p>

<p>其中 $\sigma(i)$ 为切换概率，$\kappa(\cdot\mid i)$ 为目标索引分布。</p>

<h3 id="42-分解与单调提升下界">4.2 分解与单调提升下界</h3>

<p>通过引入混合回报 $J_{\mathrm{mix}}^{(k)}$ 作为中间桥梁，性能差异可分解为：</p>

<p>$$
J(\pi_{k+1}) - J(\pi_k) = \underbrace{[J(\pi_{k+1}) - J_{\mathrm{mix}}^{(k)}]}_{\text{相对混合策略的改进}} + \underbrace{[J_{\mathrm{mix}}^{(k)} - J(\pi_k)]}_{\text{混合偏差项}}
$$</p>

<p>第一项可用定理1.1处理。第二项是<strong>混合偏差项</strong>，可以证明它满足以下不等式：</p>

<p>$$
J_{\mathrm{mix}}^{(k)} - J(\pi_k) \geq -\frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
$$</p>

<h4 id="单调提升下界">单调提升下界</h4>

<p>合并上述结果，我们得到核心定理：</p>

<blockquote>
  <p><strong>定理3.1（动态混合采样下的单调提升下界）</strong></p>

  <p>$$
\begin{aligned}
J(\pi_{k+1}) - J(\pi_k) \geq\;& L_{\beta^{(k)}}(\pi_{k+1}) \\
&- \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \big] \\
&- \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
\end{aligned}
$$</p>
</blockquote>

<p>其中 $L_{\beta^{(k)}}(\pi_{k+1})$ 表示“相对行为策略 $\beta^{(k)}$ 的代理目标”（与第二部分的 $L_{\pi_k}(\pi)$ 同形，只是把行为策略从单一 $\pi_k$ 推广到混合 $\beta^{(k)}$）。</p>

<p>更具体地，可写为
$$
L_{\beta^{(k)}}(\pi_{k+1}) := \frac{1}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}},\, a\sim \pi^{(i)}(\cdot\mid s)}\left[\frac{\pi_{k+1}(a\mid s)}{\pi^{(i)}(a\mid s)}\,A^{\beta^{(k)}}((s,i),a)\right].
$$</p>

<p>类似地，记
$$
C_{\pi_{k+1},\beta^{(k)}} := \max_{(s,i)}\left|\mathbb{E}_{a\sim \pi_{k+1}(\cdot\mid s)}\big[A^{\beta^{(k)}}((s,i),a)\big]\right|.
$$</p>

<p>该下界揭示了<strong>双重控制</strong>的必要性：</p>
<ul>
  <li><strong>更新偏移惩罚</strong>：新策略 $\pi_{k+1}$ 相对于采样来源策略 $\pi^{(i)}$ 的偏移</li>
  <li><strong>采样陈旧性惩罚</strong>：采样来源策略 $\pi^{(i)}$ 相对于当前策略 $\pi_k$ 的陈旧性</li>
</ul>

<h3 id="43-直接约束为何不可行三角不等式分解">4.3 直接约束为何不可行：三角不等式分解</h3>

<p>定理3.1中的更新偏移惩罚项看似可以通过约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s)$ 来控制，但这实际上<strong>不可行</strong>：</p>

<blockquote>
  <p><strong>观察3.1（更新偏移约束的不可行性）</strong></p>

  <p>假设混合采样包含两个旧策略 $\pi^{(1)}$ 和 $\pi^{(2)}$，若存在某个状态 $s$ 使得 $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) > 2\delta$，则不存在任何策略 $\pi_{k+1}$ 能够同时满足 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(1)}; s) \leq \delta$ 与 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(2)}; s) \leq \delta$。</p>
</blockquote>

<h4 id="证明">证明</h4>

<p>由三角不等式，若同时满足两个约束，则 $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) \leq 2\delta$，矛盾。</p>

<h4 id="问题根源">问题根源</h4>

<p>更新偏移惩罚项将 $\pi_{k+1}$ 与历史策略族 $\{\pi^{(i)}\}$ 直接耦合，而后者的内部结构是历史训练的产物，不受当前更新控制。</p>

<h4 id="三角不等式分解">三角不等式分解</h4>

<p>解决方案是利用TV距离的三角不等式：</p>

<p>$$
D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \leq D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s) + D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)
$$</p>

<p>这将耦合约束拆分为两个独立部分：</p>

<ul>
  <li><strong>更新增量偏移</strong> $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)$：新策略相对于当前策略的偏离，<strong>可由优化侧控制</strong></li>
  <li><strong>采样陈旧性</strong> $D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)$：当前策略相对于各旧策略的偏离，<strong>需由采样侧控制</strong></li>
</ul>

<p>定义：</p>

<p>$$
U_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)\big], \quad S_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\big]
$$</p>

<blockquote>
  <p><strong>推论3.2（分解后的单调提升下界）</strong></p>

  <p>$$
J(\pi_{k+1}) - J(\pi_k) \geq L_{\beta^{(k)}}(\pi_{k+1}) - \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} U_k - \left( \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} + \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \right) S_k
$$</p>
</blockquote>

<h4 id="为何分解能解决问题">为何分解能解决问题？</h4>

<p>关键在于：分解后的 $U_k$ 只涉及新策略 $\pi_{k+1}$ 和当前策略 $\pi_k$，<strong>与旧策略族 $\{\pi^{(i)}\}$ 的结构完全无关</strong>。因此，无论旧策略之间差异多大，约束 $U_k$ 都是可行的——这正是观察3.1所揭示的不可行性问题的解决之道。</p>

<p>这揭示了一个重要的工程原则——<strong>职责分离</strong>：</p>

<table>
  <thead>
    <tr>
      <th>控制项</th>
      <th>负责方</th>
      <th>控制手段</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$U_k$（更新增量偏移）</td>
      <td>优化算法</td>
      <td>策略裁剪</td>
    </tr>
    <tr>
      <td>$S_k$（采样陈旧性）</td>
      <td>采样系统</td>
      <td>数据过滤、版本窗口</td>
    </tr>
  </tbody>
</table>

<h2 id="第五部分裁剪机制的理论基础">第五部分：裁剪机制的理论基础</h2>

<h3 id="51-从-tv-距离到样本可控量">5.1 从 TV 距离到样本可控量</h3>

<p>推论3.2告诉我们，要保证单调提升，需要控制更新增量偏移 $U_k = \mathbb{E}[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)]$。但TV距离是分布层面的量，如何用样本来控制它？</p>

<p>关键桥梁是下面这个恒等式：</p>

<blockquote>
  <p><strong>引理3.3（TV距离的比值差表示）</strong></p>

  <p>设策略 $\pi_1$ 的支撑覆盖 $\pi$ 和 $\pi_2$ 的支撑，则对任意状态分布 $\mu$：</p>

  <p>$$
\mathbb{E}_{s\sim \mu} \big[D_{\mathrm{TV}}(\pi, \pi_2; s)\big] = \frac{1}{2} \mathbb{E}_{s\sim \mu, a\sim\pi_1(\cdot\mid s)} \left| \frac{\pi(a\mid s)}{\pi_1(a\mid s)} - \frac{\pi_2(a\mid s)}{\pi_1(a\mid s)} \right|
$$</p>
</blockquote>

<h4 id="直观理解">直观理解</h4>

<p>左侧是两个分布之间的TV距离（需要遍历所有动作），右侧是在 $\pi_1$ 下采样时两个重要性比值的差的绝对值。这使我们能够通过样本来估计和控制TV距离。</p>

<h4 id="inlmath115mathend-的样本表示">$U_k$ 的样本表示</h4>

<p>利用引理3.3，取 $\pi = \pi_{k+1}$，$\pi_2 = \pi_k$，$\pi_1 = \pi^{(i)}$（采样来源策略），可得：</p>

<p>$$
U_k = \frac{1}{2} \mathbb{E}_{(s,i) \sim d_{\beta^{(k)}}, a \sim \pi^{(i)}(\cdot\mid s)} \left| \frac{\pi_{k+1}(a\mid s)}{\pi^{(i)}(a\mid s)} - \frac{\pi_k(a\mid s)}{\pi^{(i)}(a\mid s)} \right|
$$</p>

<p>记 $\rho_{k+1} := \frac{\pi_{k+1}(a\mid s)}{\pi^{(i)}(a\mid s)}$ 和 $\rho_k := \frac{\pi_k(a\mid s)}{\pi^{(i)}(a\mid s)}$，则：</p>

<p>$$
U_k = \frac{1}{2} \mathbb{E}_{(s,i,a) \sim \text{训练数据}} \big| \rho_{k+1} - \rho_k \big|
$$</p>

<p>这意味着：<strong>如果我们能使每个样本满足 $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$，就能保证 $U_k \leq \epsilon/2$</strong>。</p>

<h3 id="52-约束-inlmath123mathend两种裁剪方式">5.2 约束 $U_k$：两种裁剪方式</h3>

<h4 id="方法一直接约束比值差">方法一：直接约束比值差</h4>

<p>对每个样本 $(s, i, a)$，要求满足：</p>

<p>$$
\left| \frac{\pi_{k+1}(a\mid s)}{\pi^{(i)}(a\mid s)} - \frac{\pi_k(a\mid s)}{\pi^{(i)}(a\mid s)} \right| \leq \epsilon
$$</p>

<p>即裁剪区间为 $\left[\frac{\pi_k(a\mid s)}{\pi^{(i)}(a\mid s)} - \epsilon, \frac{\pi_k(a\mid s)}{\pi^{(i)}(a\mid s)} + \epsilon\right]$，<strong>裁剪中心是 $\rho_k$ 而非 1</strong>。</p>

<h4 id="方法二约束增量比值">方法二：约束增量比值</h4>

<p>注意到 $\rho_{k+1} - \rho_k = \rho_k \cdot \left(\frac{\pi_{k+1}}{\pi_k} - 1\right)$，因此有：</p>

<p>$$
|\rho_{k+1} - \rho_k| = \rho_k \cdot \left|\frac{\pi_{k+1}(a\mid s)}{\pi_k(a\mid s)} - 1\right|
$$</p>

<p>如果约束 $\left\lvert\frac{\pi_{k+1}(a\mid s)}{\pi_k(a\mid s)} - 1\right\rvert \leq \epsilon$，由于 $\mathbb{E}_{a\sim\pi^{(i)}}[\rho_k] = 1$，可以证明 $U_k \leq \epsilon/2$。</p>

<p>这种方法直接对 $\pi_{k+1}/\pi_k$ 以 1 为中心进行裁剪，<strong>裁剪约束本身不依赖旧策略 $\pi^{(i)}$</strong>。但如果采用后文的 $\hat{A}=\rho_k\cdot A^{\beta^{(k)}}$，仍需要每条样本的行为概率 $\pi^{(i)}(a\mid s)$（或记录的 logprob）来计算 $\rho_k$。下面我们给出三种裁剪机制的完整目标函数。设当前样本来自旧策略 $\pi^{(i)}$，记：</p>
<ul>
  <li>$\rho_{k+1} = \frac{\pi_{k+1}(a\mid s)}{\pi^{(i)}(a\mid s)}$（新策略相对采样策略的比值）</li>
  <li>$\rho_k = \frac{\pi_k(a\mid s)}{\pi^{(i)}(a\mid s)}$（当前策略相对采样策略的比值）</li>
  <li>$r = \frac{\pi_{k+1}(a\mid s)}{\pi_k(a\mid s)}$（新策略相对当前策略的增量比值）</li>
</ul>

<p>说明：若采用<strong>轨迹级混合</strong>（索引不变），则 $A^{\beta^{(k)}}((s,i),a)=A^{\pi^{(i)}}(s,a)$，可直接用每条轨迹对应旧策略的优势估计；若为<strong>步/段级混合</strong>，直接用 $A^{\pi^{(i)}}$ 代替 $A^{\beta^{(k)}}$ 会引入优势替代偏差（第六部分详述），需要使用能反映未来索引切换的优势/价值估计。</p>

<h4 id="标准-ppo">标准 PPO</h4>

<p>以 1 为中心裁剪 $\rho_{k+1}$</p>

<p>$$
L^{\mathrm{PPO}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\pi^{(i)}}, \; \mathrm{clip}(\rho_{k+1}, 1-\epsilon, 1+\epsilon) \cdot A^{\pi^{(i)}} \right) \right]
$$</p>

<h4 id="方法一">方法一</h4>

<p>以 $\rho_k$ 为中心裁剪 $\rho_{k+1}$</p>

<p>$$
L^{\mathrm{M1}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\beta^{(k)}}, \; \mathrm{clip}(\rho_{k+1}, \rho_k-\epsilon, \rho_k+\epsilon) \cdot A^{\beta^{(k)}} \right) \right]
$$</p>

<h4 id="方法二">方法二</h4>

<p>以 1 为中心裁剪增量比值 $r$</p>

<p>$$
L^{\mathrm{M2}} = \mathbb{E} \left[ \min\left( r \cdot \hat{A}, \; \mathrm{clip}(r, 1-\epsilon, 1+\epsilon) \cdot \hat{A} \right) \right]
$$</p>

<p>其中 $\hat{A} = \rho_k \cdot A^{\beta^{(k)}}$ 是经过重要性加权的优势估计。</p>

<h3 id="53-对比与落地选型与采样侧控制">5.3 对比与落地：选型与采样侧控制</h3>

<h4 id="表-51三种裁剪机制的对比">表 5.1　三种裁剪机制的对比</h4>

<table>
  <thead>
    <tr>
      <th>方法</th>
      <th>裁剪变量</th>
      <th>裁剪中心</th>
      <th>裁剪区间</th>
      <th>约束的TV距离</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>标准PPO</td>
      <td>$\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$</td>
      <td>$1$</td>
      <td>$[1-\epsilon, 1+\epsilon]$</td>
      <td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$</td>
    </tr>
    <tr>
      <td>方法一</td>
      <td>$\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$</td>
      <td>$\rho_k = \pi_k/\pi^{(i)}$</td>
      <td>$[\rho_k-\epsilon, \rho_k+\epsilon]$</td>
      <td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$</td>
    </tr>
    <tr>
      <td>方法二</td>
      <td>$r = \pi_{k+1}/\pi_k$</td>
      <td>$1$</td>
      <td>$[1-\epsilon, 1+\epsilon]$</td>
      <td>$D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$</td>
    </tr>
  </tbody>
</table>

<h4 id="标准-ppo-的根本问题多策略混合">标准 PPO 的根本问题（多策略混合）</h4>

<p>标准PPO约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$，要求新策略同时接近所有采样来源策略。根据观察3.1，当各旧策略 $\pi^{(1)}, \pi^{(2)}, \ldots$ 之间差异显著时，<strong>不存在能够同时满足所有约束的 $\pi_{k+1}$</strong>。这导致信赖域交集收缩甚至为空，更新被最陈旧的策略所限制。</p>

<h4 id="方法一与方法二的共同优势">方法一与方法二的共同优势</h4>

<p>两者都约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$——新策略相对于<strong>当前策略</strong>（而非采样策略）的偏离。由于 $\pi_k$ 是唯一确定的，这个约束对所有来源的样本一致，完全规避了不可行性问题。</p>

<h4 id="方法一-vs-方法二">方法一 vs 方法二</h4>

<table>
  <thead>
    <tr>
      <th>比较维度</th>
      <th>方法一（自适应裁剪）</th>
      <th>方法二（增量裁剪）</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>陈旧样本（$\rho_k \gg 1$）</td>
      <td>自动收紧约束，更保守</td>
      <td>可能产生大梯度方差</td>
    </tr>
    <tr>
      <td>LLM大词表低概率token</td>
      <td>允许较大绝对变化（加法型）</td>
      <td>绝对变化受限（乘法型）</td>
    </tr>
    <tr>
      <td>实现复杂度</td>
      <td>需存储 $\pi^{(i)}(a\mid s)$ 和 $\pi_k(a\mid s)$</td>
      <td>需 $\pi_k(a\mid s)$ 与 $\pi^{(i)}(a\mid s)$（或存储的 logprob）以计算 $\rho_k$；裁剪本身仅用 $\pi_{k+1}/\pi_k$</td>
    </tr>
    <tr>
      <td>优势函数</td>
      <td>使用 $A^{\beta^{(k)}}$</td>
      <td>使用加权优势 $\rho_k \cdot A^{\beta^{(k)}}$</td>
    </tr>
  </tbody>
</table>

<h4 id="详细解释">详细解释</h4>

<h4 id="一-陈旧样本处理">(一) 陈旧样本处理</h4>

<p>当样本来自很旧的策略时，$\rho_k = \pi_k/\pi^{(i)}$ 可能很大。</p>

<ul>
  <li>方法二的被积函数为 $\rho_k \cdot \lvert r - 1\rvert$，即便 $\lvert r-1\rvert \leq \epsilon$，被积函数仍可达 $\epsilon \cdot \rho_k$，产生尖峰。</li>
  <li>方法一直接约束 $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$，被积函数上界恒为 $\epsilon$，不受 $\rho_k$ 放大。</li>
</ul>

<h4 id="二-llm-大词表问题">(二) LLM 大词表问题</h4>

<p>大语言模型词表规模巨大，大量token的概率极小。</p>

<ul>
  <li>方法二约束 $\pi_{k+1} \in [(1-\epsilon)\pi_k, (1+\epsilon)\pi_k]$，这是<strong>乘法型约束</strong>：若 $\pi_k(a\mid s) = 10^{-6}$，允许的绝对变化仅为 $\epsilon \times 10^{-6}$。</li>
  <li>方法一约束 $\lvert\pi_{k+1} - \pi_k\rvert \leq \epsilon \cdot \pi^{(i)}$，这是<strong>加法型约束</strong>：若该 token 在旧策略下概率较高（例如 $\pi^{(i)}(a\mid s) = 0.1$），即便当前概率很低，也允许较快提升。</li>
</ul>

<h4 id="采样陈旧性的控制">采样陈旧性的控制</h4>

<p>推论3.2表明，$S_k$ 同样影响单调提升下界，但它<strong>无法通过优化侧的裁剪来控制</strong>，需要由采样系统实现：</p>

<h4 id="一-丢弃陈旧数据">(一) 丢弃陈旧数据</h4>

<p>设定阈值 $\epsilon_{\mathrm{stale}}$，对每个样本计算 $\lvert\rho_k - 1\rvert = \lvert\pi_k(a\mid s)/\pi^{(i)}(a\mid s) - 1\rvert$，丢弃超过该阈值的样本。</p>

<h4 id="二-控制策略版本窗口">(二) 控制策略版本窗口</h4>

<p>限制混合采样的旧策略版本数量，例如仅使用最近 $W$ 个版本的数据。</p>

<h4 id="裁剪的操作含义">裁剪的操作含义</h4>

<p>最后，需要澄清裁剪与理论下界的关系。</p>

<p>推论3.2中，$U_k$ 的系数 $C_{\pi_{k+1},\beta^{(k)}}$ 依赖于新策略 $\pi_{k+1}$，因此惩罚项<strong>不能简单地替换为常数</strong>。正确的操作含义是：</p>

<blockquote>
  <p><strong>在 $U_k \leq \epsilon/2$ 的约束下，最大化代理目标 $L_{\beta^{(k)}}(\pi_{k+1})$</strong></p>
</blockquote>

<p>裁剪目标函数正是这一约束优化的实现——通过裁剪<strong>硬性限制</strong>更新幅度，确保 $U_k$ 可控；在此前提下，通过梯度上升提升代理目标，从而为策略的单调改进提供保障。</p>

<h4 id="本节小结">本节小结</h4>

<p>本节建立了裁剪机制的理论基础：</p>

<ol>
  <li><strong>引理3.3</strong>将TV距离转化为样本层面的比值差，是连接理论与实现的桥梁</li>
  <li><strong>两种约束方法</strong>：方法一（自适应裁剪中心）和方法二（固定增量裁剪），均保证 $U_k \leq \epsilon/2$</li>
  <li><strong>与标准PPO对比</strong>：标准PPO约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$，在多策略混合下不可行；方法一/二约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$，规避了该问题</li>
  <li><strong>方法选择</strong>：陈旧性高或LLM大词表场景推荐方法一；若更关注“裁剪/信赖域不再依赖旧策略族”，可选方法二（但仍需数据侧提供行为 logprob 以计算 $\rho_k$）</li>
  <li><strong>$S_k$ 的控制</strong>由采样侧负责，通过数据过滤和版本窗口实现</li>
  <li><strong>裁剪是约束优化</strong>：在 $U_k$ 约束下最大化代理目标</li>
</ol>

<h2 id="第六部分轨迹级与步段级混合的比较">第六部分：轨迹级与步/段级混合的比较</h2>

<h3 id="61-机制差异与估计影响">6.1 机制差异与估计影响</h3>

<p>两类混合机制的本质区别在于索引转移核的结构：</p>

<ul>
  <li><strong>轨迹级混合</strong>：$q(i'\mid i) = \mathbf{1}\{i'=i\}$，索引永不改变</li>
  <li><strong>步/段级混合</strong>：$\sigma(i) > 0$，允许轨迹内切换</li>
</ul>

<p>与常见工程术语的对应关系如下：</p>

<ul>
  <li>这里的<strong>轨迹级混合</strong>可以大致理解为<strong>常规异步训练</strong>的一个理想化抽象：数据按整条轨迹/episode 归属于某个策略版本；</li>
  <li>这里的<strong>步/段级混合</strong>可以大致理解为<strong>partial rollout</strong>的一个抽象：由于 actor 与 learner 异步，且 segment 边界处可能刷新到新策略版本，使用索引转移核允许“轨迹内部版本切换”，可以更好地近似刻画这种现象。</li>
</ul>

<p>关键分水岭在于<strong>引理2.1的结构简化是否成立</strong>：轨迹级混合满足优势函数还原；步/段级混合一般不满足，因为未来回报受索引转移核影响。</p>

<h4 id="采样陈旧性-inlmath204mathend-的差异">采样陈旧性 $S_k$ 的差异</h4>

<p><strong>轨迹级混合</strong>的陈旧性来源于：混合权重 $\alpha_i^{(k)}$ 在新策略发布后仍对旧策略保留一定的比例。</p>

<p><strong>步/段级混合</strong>具有<strong>指数压缩效应</strong>：考虑从旧到新以概率 $\sigma$ 切换的简化模型，折扣访问分布下旧索引的边缘质量为 $\frac{1-\gamma}{1-\gamma(1-\sigma)}$。只要 $\sigma \gg 1-\gamma$，旧策略的权重即可被显著压缩。</p>

<h4 id="代理目标估计的差异">代理目标估计的差异</h4>

<p><strong>轨迹级混合</strong>：优势函数还原为 $A^{\pi^{(i)}}(s,a)$，估计路径清晰。</p>

<p><strong>步/段级混合的优势替代偏差</strong>：若沿用单策略优势估计，将产生系统性偏差。原因是 $A^{\beta^{(k)}}((s,i),a)$ 需要对未来索引切换取期望，而 $A^{\pi^{(i)}}(s,a)$ 隐含了“未来始终沿用 $\pi^{(i)}$”的假设。</p>

<h4 id="bandit-设定下的统一">Bandit 设定下的统一</h4>

<p>在单步 episode 的 LLM 训练中，无后续状态转移，两类机制的估计问题统一，无上述偏差。</p>

<h3 id="62-风险与适用场景">6.2 风险与适用场景</h3>

<p>步/段级混合还有一个隐患：即便单步重要性比值被裁剪，长轨迹下多步噪声叠加仍会放大梯度估计方差。当每次更新的策略变化幅度较大时，轨迹内部的“行为突变”可能引发更重尾的比值分布。这也是表6.1中“策略变化幅度大”场景推荐轨迹级混合的原因。</p>

<h4 id="适用场景">适用场景</h4>

<h4 id="表-61两类混合机制的适用场景">表 6.1　两类混合机制的适用场景</h4>

<table>
  <thead>
    <tr>
      <th>场景特征</th>
      <th>推荐机制</th>
      <th>理由</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>长轨迹、高频更新、强异步</td>
      <td>步/段级</td>
      <td>可显著压缩 $S_k$</td>
    </tr>
    <tr>
      <td>短轨迹（非Bandit）</td>
      <td>轨迹级</td>
      <td>$S_k$ 自然较低</td>
    </tr>
    <tr>
      <td>每次更新策略变化幅度大</td>
      <td>轨迹级</td>
      <td>避免方差放大</td>
    </tr>
    <tr>
      <td>单步episode（Bandit）</td>
      <td>均可</td>
      <td>按实现便利选择</td>
    </tr>
    <tr>
      <td>需要折中方案</td>
      <td>段级</td>
      <td>在自然边界切换</td>
    </tr>
  </tbody>
</table>

<p><strong>核心权衡</strong>：步/段级混合在采样侧更强（快速去陈旧），轨迹级混合在估计侧更稳（代理目标易于估计）。</p>

<h2 id="第七部分训推不一致的处理">第七部分：训推不一致的处理</h2>

<h3 id="71-背景与有效陈旧性">7.1 背景与有效陈旧性</h3>

<p>在大规模分布式训练中，推理端和训练端的策略可能存在不一致：</p>

<ul>
  <li><strong>数值实现差异</strong>：softmax归一化、量化、核融合等</li>
  <li><strong>解码规则差异</strong>：温度缩放、top-p/top-k采样等</li>
</ul>

<p>设训练侧建模的行为策略为 $\pi^{(i)}$，而推理端实际采样的策略为 $\hat{\pi}^{(i)}$。</p>

<h4 id="有效陈旧性">有效陈旧性</h4>

<p>定义<strong>有效陈旧性</strong>：</p>

<p>$$
\hat{S}_k := \mathbb{E}_{(s,i) \sim d_{\hat{\beta}^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_k, \hat{\pi}^{(i)}; s) \big]
$$</p>

<p>该定义同时覆盖了版本陈旧性与训推实现差异。</p>

<h3 id="72-可操作控制">7.2 可操作控制</h3>

<p>由引理3.3，$\hat{S}_k$ 可表示为样本级可计算形式。给定阈值 $\epsilon_{\mathrm{stale}}$，若训练仅使用满足 $\lvert\pi_k(a\mid s)/\hat{\pi}^{(i)}(a\mid s) - 1\rvert \leq \epsilon_{\mathrm{stale}}$ 的样本，则 $\hat{S}_k \leq \epsilon_{\mathrm{stale}}/2$。</p>

<h4 id="关键实现要点">关键实现要点</h4>

<ol>
  <li><strong>行为分母对齐</strong>：损失中的行为概率应使用推理端记录的 $\hat{\pi}^{(i)}(a\mid s)$</li>
  <li><strong>概率平滑</strong>：若推理端有截断（如top-k），需确保比值合法</li>
</ol>

<h2 id="总结实践指南">总结：实践指南</h2>

<h4 id="核心理论框架">核心理论框架</h4>

<p>单调提升下界的结构为：</p>

<p>$$
J(\pi_{k+1}) - J(\pi_k) \geq \underbrace{L_{\beta^{(k)}}(\pi_{k+1})}_{\text{代理目标}} - \underbrace{C_1 \cdot U_k}_{\text{更新偏移惩罚}} - \underbrace{C_2 \cdot S_k}_{\text{采样陈旧性惩罚}}
$$</p>

<h4 id="职责分离原则">职责分离原则</h4>

<table>
  <thead>
    <tr>
      <th>控制项</th>
      <th>负责方</th>
      <th>控制手段</th>
      <th>具体操作</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$U_k$</td>
      <td>优化算法</td>
      <td>策略裁剪</td>
      <td>对更新增量进行裁剪（例如对 $\pi_{k+1}/\pi_k$ 裁剪）</td>
    </tr>
    <tr>
      <td>$S_k$</td>
      <td>采样系统</td>
      <td>数据过滤</td>
      <td>丢弃陈旧样本</td>
    </tr>
    <tr>
      <td>$S_k$</td>
      <td>采样系统</td>
      <td>版本窗口</td>
      <td>仅用最近 $W$ 个版本</td>
    </tr>
  </tbody>
</table>

<h4 id="裁剪方法选择">裁剪方法选择</h4>

<table>
  <thead>
    <tr>
      <th>场景</th>
      <th>推荐方法</th>
      <th>理由</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>陈旧性较高</td>
      <td>方法一（自适应）</td>
      <td>自动对陈旧样本收紧约束</td>
    </tr>
    <tr>
      <td>实现简洁优先</td>
      <td>方法二（增量）</td>
      <td>无需存储旧策略信息</td>
    </tr>
    <tr>
      <td>LLM大词表</td>
      <td>方法一</td>
      <td>避免低概率token更新过慢</td>
    </tr>
  </tbody>
</table>

<h4 id="训推不一致的处理">训推不一致的处理</h4>

<ul>
  <li>使用推理端记录的 $\hat{\pi}^{(i)}$ 作为行为分母</li>
  <li>通过样本过滤压缩有效陈旧性</li>
</ul>

<h2 id="附录关键符号速查表">附录：关键符号速查表</h2>

<table>
  <thead>
    <tr>
      <th>符号</th>
      <th>含义</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\pi_k$, $\pi^{(i)}$</td>
      <td>第 $k$ 轮最新策略，第 $i$ 个旧策略</td>
    </tr>
    <tr>
      <td>$d_\pi(s)$, $A^\pi(s,a)$</td>
      <td>折扣状态访问分布，优势函数</td>
    </tr>
    <tr>
      <td>$D_{\mathrm{TV}}(\pi, \pi'; s)$</td>
      <td>两策略在状态 $s$ 上的TV距离</td>
    </tr>
    <tr>
      <td>$\beta^{(k)}(a \mid s, i) := \pi^{(i)}(a \mid s)$</td>
      <td>第 $k$ 轮混合行为策略</td>
    </tr>
    <tr>
      <td>$q(i' \mid i)$, $\alpha_i^{(k)}$</td>
      <td>索引转移核，索引初始分布</td>
    </tr>
    <tr>
      <td>$U_k$, $S_k$</td>
      <td>更新增量偏移，采样陈旧性</td>
    </tr>
    <tr>
      <td>$\epsilon$, $\epsilon_{\mathrm{stale}}$, $W$</td>
      <td>裁剪半径，陈旧性阈值，版本窗口</td>
    </tr>
    <tr>
      <td>$C_{\pi,\pi_k}$</td>
      <td>期望优势上界常数</td>
    </tr>
  </tbody>
</table>

<h2 id="参考文献">参考文献</h2>

<ol>
  <li>
    <p>John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. “Trust Region Policy Optimization” (TRPO). arXiv:1502.05477. <a href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a></p>
  </li>
  <li>
    <p>Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel. “Constrained Policy Optimization” (CPO). arXiv:1705.10528. <a href="https://arxiv.org/abs/1705.10528">https://arxiv.org/abs/1705.10528</a></p>
  </li>
  <li>
    <p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. “Proximal Policy Optimization Algorithms” (PPO). arXiv:1707.06347. <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
  </li>
  <li>
    <p>James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras. “Generalized Proximal Policy Optimization with Sample Reuse” (GePPO). arXiv:2111.00072. <a href="https://arxiv.org/abs/2111.00072">https://arxiv.org/abs/2111.00072</a></p>
  </li>
  <li>
    <p>Yuzhen Zhou, Jiajun Li, Yusheng Su, et al. “APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation” (APRIL; partial rollout). arXiv:2509.18521. <a href="https://arxiv.org/abs/2509.18521">https://arxiv.org/abs/2509.18521</a></p>
  </li>
  <li>
    <p>Jacob Hilton, Karl Cobbe, John Schulman. “Batch size-invariance for policy optimization” (Decoupled PPO). arXiv:2110.00641. <a href="https://arxiv.org/abs/2110.00641">https://arxiv.org/abs/2110.00641</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025OffPolicyLLMRL</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Off-Policy Training in LLM Reinforcement Learning: From Theory to Practice}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html}</span><span class="p">,</span>
	<span class="na">urldate</span>      <span class="p">=</span> <span class="s">{2025-12-17}</span>
<span class="p">}</span>
</code></pre></div></div>

      </article>

      

      
        
      </div>

    <div class="col-lg-3 d-none d-lg-block toc-sidebar-col">
      <aside class="toc-sidebar" data-toc-sidebar id="toc-sidebar">
  <div class="toc-sidebar__header">
    <div class="toc-sidebar__title">目录</div>
    <button class="toc-toggle-btn" aria-label="Toggle table of contents" aria-expanded="false" aria-controls="toc-content" data-toc-toggle>
      <i class="fas fa-chevron-right"></i>
    </button>
  </div>
  <nav
    id="toc-content"
    class="toc js-page-toc"
    data-toc
    data-toc-content=".toc-content"
    data-toc-headings="h2,h3"
    data-toc-min-items="2"
    aria-label="目录"
  ></nav>
</aside>

<!-- Collapsed TOC toggle button (shown when TOC is collapsed) -->
<button class="toc-collapsed-toggle" aria-label="Show table of contents" aria-expanded="false" aria-controls="toc-content" data-toc-expand>
  <i class="fas fa-list"></i>
</button>


    </div>
  </div>
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2026 Xihuai Leo Wang. Last updated: February 18, 2026.
      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/blog_enhancements.js" type="text/javascript"></script>
  <script defer src="/assets/js/sidenotes.js" type="text/javascript"></script>
  <script defer src="/assets/js/footnote_preview.js" type="text/javascript"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>
  <script defer src="/assets/js/toc.js" type="text/javascript"></script>
  <script defer src="/assets/js/venue_filter.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax 3.x with comprehensive configuration -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        // Support all common math delimiters
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        processRefs: true,
        // Common macros for convenience
        macros: {
          RR: '\\mathbb{R}',
          NN: '\\mathbb{N}',
          ZZ: '\\mathbb{Z}',
          CC: '\\mathbb{C}',
          EE: '\\mathbb{E}',
          PP: '\\mathbb{P}',
          bm: ['\\boldsymbol{#1}', 1],
          argmax: '\\operatorname*{arg\\,max}',
          argmin: '\\operatorname*{arg\\,min}',
          sgn: '\\operatorname{sgn}',
          KL: '\\mathrm{KL}',
          Var: '\\operatorname{Var}',
          Cov: '\\operatorname{Cov}',
          tr: '\\operatorname{tr}',
          diag: '\\operatorname{diag}'
        },
        // AMS packages
        packages: {'[+]': ['ams', 'boldsymbol', 'newcommand']}
      },
      loader: {
        load: ['[tex]/ams', '[tex]/boldsymbol', '[tex]/newcommand']
      },
      options: {
        // Skip math rendering in these HTML elements
        skipHtmlTags: [
          'script', 'noscript', 'style', 'textarea', 'pre', 'code',
          'annotation', 'annotation-xml', 'kbd', 'samp', 'var'
        ],
        // Fix issues with underscores being converted to <em> by HTML
        processHtmlClass: 'mathjax-process',
        ignoreHtmlClass: 'tex2jax_ignore|no-mathjax',
        // Render math even with HTML entities
        renderActions: {
          findScript: [10, function (doc) {
            // Pre-process to fix HTML entity issues in math
            for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            }
          }, '']
        }
      },
      svg: {
        fontCache: 'global',
        scale: 1.0
      },
      chtml: {
        scale: 1.0,
        matchFontHeight: true
      },
      startup: {
        ready: function () {
          MathJax.startup.defaultReady();
          // Fix: restore underscores that might have been converted to <em>
          MathJax.startup.promise.then(() => {
            console.log('MathJax typesetting complete');
          });
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  
  <!-- Pre-processing script to protect math from Markdown/HTML interference -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Fix underscores in math that were converted to <em> by Markdown
      function fixMathUnderscores() {
        const mathContainers = document.querySelectorAll('.MathJax, .MathJax_Display, mjx-container');
        // This runs before MathJax, so we need to fix raw content
        const content = document.querySelector('.post-content, article');
        if (!content) return;
        
        // Find math delimiters and restore any <em> or <strong> inside them
        const html = content.innerHTML;
        
        // Pattern to find math blocks and restore underscore formatting
        // This is a fallback; the main protection is in the Jekyll plugin
      }
      
      // Fix HTML entities in display math blocks
      function fixHtmlEntities() {
        document.querySelectorAll('.language-plaintext.highlighter-rouge').forEach(el => {
          // Check if this looks like an HTML figure that wasn't rendered
          const text = el.textContent;
          if (text.includes('<img') || text.includes('<figure') || text.includes('<figcaption')) {
            // This is raw HTML that should be rendered - replace with actual HTML
            const temp = document.createElement('div');
            temp.innerHTML = text;
            el.replaceWith(...temp.childNodes);
          }
        });
      }
      
      fixHtmlEntities();
    });
  </script>

    <!-- Pseudocode -->
  <script defer src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js" integrity="sha256-aVkDxqyzrB+ExUsOY9PdyelkDhn/DfrjWu08aVpqNlo=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/pseudocode-init.js" type="text/javascript"></script>
    <!-- Mermaid -->
  <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.9.3/dist/mermaid.min.js"></script>
  <script defer src="/assets/js/mermaid-init.js" type="text/javascript"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2923RQZBXG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-2923RQZBXG');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    // Use the CSS-defined padding-top value to avoid layout shift
    // The navbar height is already handled by CSS: body.fixed-top-nav { padding-top: 50px; }
    let navbarHeight = $("#navbar").outerHeight(true) || 50;
    // Only set progressBar position, don't override body padding to avoid layout shift
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
