<!DOCTYPE html><html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation | Xihuai Wang's Page</title><meta name="author" content="Xihuai Leo Wang" />
    <meta name="description" content="How you approximate KL can make or break training stability. This post analyzes the classic estimators k1, k2, k3 in on-policy and off-policy settings, and gives practical guidance on using KL as a differentiable loss term versus as a detached reward penalty." />
    <meta name="keywords" content="Reinforcement Learning, Multi-agent System, Language Model" />

    <!-- 
    Unified Share Image URL Calculation
    Priority:
    1) page.disable_share_image / page.share_image: none  -> no image
    2) page.og_image                                     -> explicit per-page image
    3) dynamic generation (posts)                         -> horizontal OG via Tailgraph
    4) site.og_image == 'auto'                            -> horizontal OG via Tailgraph for non-post pages
    5) site.og_image                                      -> site default
    This keeps homepage free to use a portrait image while giving blog pages a better horizontal card.
    --><!-- Itemprop meta tags for Zhihu and other platforms using Schema.org microdata -->
    <meta itemprop="name" content="Xihuai&#39;s Blog | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    <meta itemprop="description" content="How you approximate KL can make or break training stability. This post analyzes the classic estimators k1, k2, k3 in on-policy and off-policy settings, and gives practical guidance on using KL as a differentiable loss term versus as a detached reward penalty." /><meta itemprop="image" content="https://xihuai18.github.io/assets/img/kl-estimators/kl-estimator-og.jpg" /><!-- WeChat/Weibo/QQ specific meta tags for Chinese social platforms --><meta name="weibo:article:title" content="Xihuai&#39;s Blog | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    <meta name="weibo:article:description" content="How you approximate KL can make or break training stability. This post analyzes the classic estimators k1, k2, k3 in on-policy and off-policy settings, and gives practical guidance on using KL as a differentiable loss term versus as a detached reward penalty." /><meta name="weibo:article:image" content="https://xihuai18.github.io/assets/img/kl-estimators/kl-estimator-og.jpg" />

    <!-- OpenGraph -->
    <meta property="og:site_name" content="Xihuai Wang's Page" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-12-01T00:00:00+00:00" /><meta property="article:author" content="Xihuai Leo Wang" /><meta property="og:title" content="Xihuai&#39;s Blog | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    <meta property="og:url" content="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html" />
    <meta property="og:description" content="How you approximate KL can make or break training stability. This post analyzes the classic estimators k1, k2, k3 in on-policy and off-policy settings, and gives practical guidance on using KL as a differentiable loss term versus as a detached reward penalty." /><meta property="og:image" content="https://xihuai18.github.io/assets/img/kl-estimators/kl-estimator-og.jpg" />
    <meta property="og:image:secure_url" content="https://xihuai18.github.io/assets/img/kl-estimators/kl-estimator-og.jpg" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:image:alt" content="Xihuai&#39;s Blog | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" /><meta property="og:locale" content="en_US" />

        <!-- Twitter card --><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="Xihuai&#39;s Blog | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    <meta name="twitter:description" content="How you approximate KL can make or break training stability. This post analyzes the classic estimators k1, k2, k3 in on-policy and off-policy settings, and gives practical guidance on using KL as a differentiable loss term versus as a detached reward penalty." /><meta name="twitter:image" content="https://xihuai18.github.io/assets/img/kl-estimators/kl-estimator-og.jpg" />
    <meta name="twitter:image:alt" content="Xihuai&#39;s Blog | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    

    <!-- Schema.org -->
    <script type="application/ld+json">
      {
        "@context": "https://schema.org","@type": "BlogPosting",
        "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html"
        },
        "headline": "Xihuai&#39;s Blog | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation",
        "description": "How you approximate KL can make or break training stability. This post analyzes the classic estimators k1, k2, k3 in on-policy and off-policy settings, and gives practical guidance on using KL as a differentiable loss term versus as a detached reward penalty.","image": "https://xihuai18.github.io/assets/img/kl-estimators/kl-estimator-og.jpg","author": {
          "@type": "Person",
          "name": "Xihuai Leo Wang"
        },
        "publisher": {
          "@type": "Person",
          "name": "Xihuai Leo Wang"
        },"datePublished": "2025-12-01T00:00:00+00:00","dateModified": "2025-12-01T00:00:00+00:00","url": "https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html",
        "sameAs": ["https://scholar.google.com/citations?user=hy6v3qUAAAAJ","https://github.com/xihuai18"]
      }
    </script>


    <!-- DNS Prefetch & Preconnect for faster external resource loading -->
    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://fonts.googleapis.com">
    <link rel="dns-prefetch" href="https://fonts.gstatic.com">
    <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" /><!-- Pseudocode -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"><!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

    <!-- Prefetch/Preload for faster navigation -->
    <link rel="prefetch" href="/" as="document">
    <link rel="prefetch" href="/blog/" as="document">
    <link rel="prefetch" href="/publications/" as="document">
    <link rel="prefetch" href="/cv/" as="document">
    
    <!-- Instant.page for instant page loads on hover -->
    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module" defer></script>

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">Xihuai Wang's Page</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">About</a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">Blog</a>
          </li>

          <!-- CV -->
          <!-- 
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/" target="_blank"
              rel="noopener noreferrer">cv</a>
          </li> -->
          <!-- Other pages -->
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">Publications</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/cv/">CV</a>
          </li>
          <!-- Toggle theme mode -->
          <li class="nav-item toggle-container">
            <button id="light-toggle" class="nav-link" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post toc-layout">
  <header class="post-header">
    <h1 class="post-title">Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation</h1>
    <div class="post-meta-container">
      <div class="post-meta-row">
        <span class="post-date">
          <i class="far fa-calendar-alt"></i>
          December 1, 2025
        </span></div>
      <div class="post-tags-row">
        <a href="/blog/?year=2025" data-filter-link data-filter-type="year" data-filter-value="2025">
          üìÖ 2025
        </a>
          &nbsp; &middot; &nbsp;
          
            <a href="/blog/?category=reinforcement-learning" data-filter-link data-filter-type="category" data-filter-value="reinforcement-learning">
              üè∑Ô∏è reinforcement-learning
            </a>
            
          
      </div>
    </div>
  </header>

  <div class="post-links">
  

  <!-- Bilingual Links -->
  

  

  <!-- External Platform Links -->
  

  

  <!-- External Source (from plugin) -->
  

  <!-- Output links -->
  
    <a href="/reinforcement-learning/2025/12/01/kl-estimators-zh.html">‰∏≠ÊñáÁâàÊú¨</a>
  
    <a href="https://zhuanlan.zhihu.com/p/1978993413425763764" target="_blank">Áü•‰πé <img src="/assets/img/icons/zhihu.ico" style="height: 1em; vertical-align: middle;"></a>
  
    <a href="https://mp.weixin.qq.com/s/VD_NBty5na4PfAa7wLoGAw" target="_blank">ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ <img src="/assets/img/icons/wechat.png" style="height: 1em; vertical-align: middle;"></a>
  
</div>


  <div class="row">
    
      <div class="col-lg toc-content">
    

      <article class="post-content">
        <p><img src="/assets/img/kl-estimators/kl-estimator.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>How you approximate KL divergence can make or break training stability. This post analyzes three estimators $k_1, k_2, k_3$ in both on-policy and off-policy settings, and offers practical guidance on choosing them when KL is used as a differentiable loss term versus as a detached reward penalty.</p>
</blockquote>

<h2 id="introduction-the-role-of-kl-divergence-in-reinforcement-learning">Introduction: The Role of KL Divergence in Reinforcement Learning</h2>

<p>In policy optimization (PPO, GRPO, etc.) and alignment training (RLHF/RLAIF), a <strong>KL penalty</strong> is the primary mechanism for keeping the updated policy from drifting too far from a reference policy, which helps prevent training instability or collapse. In practice, ‚Äúadding a KL penalty‚Äù hides several intertwined design choices: <strong>which estimator</strong> ($k_1$, $k_2$, $k_3$), <strong>which distribution you sample from</strong> (on-policy vs. off-policy), and <strong>how the KL term enters optimization</strong> (as a differentiable loss term vs. as a detached reward penalty). This post makes these choices explicit and clarifies how they relate.</p>

<h3 id="the-distinction-between-forward-kl-and-reverse-kl">The Distinction Between Forward KL and Reverse KL</h3>

<p>Let $q_\theta$ be the current actor policy, $p$ the reference policy. The two directions of KL divergence are:</p>

<p><strong>Reverse KL:</strong>
$$
D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{x \sim q_\theta}\left[\log \frac{q_\theta(x)}{p(x)}\right]
$$</p>

<figure style="text-align:center;">
 <img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:80%;max-width:100%;" />
 <figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Forward KL:</strong>
$$
D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q_\theta(x)}\right]
$$</p>

<figure style="text-align:center;">
 <img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:80%;max-width:100%;" />
 <figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Intuition:</strong></p>

<ul>
  <li><strong>Reverse KL</strong> is mode-seeking: the policy concentrates on high-probability regions of $p$, possibly sacrificing diversity.</li>
  <li><strong>Forward KL</strong> is mass-covering: the policy tries to cover the support of $p$.</li>
</ul>

<p>RLHF typically uses <strong>reverse KL</strong> because we want the actor not to stray too far from the reference, rather than requiring it to cover every mode.</p>

<h3 id="the-three-core-questions-who-to-sample-from-what-to-estimate-how-to-use">The Three Core Questions: Who to Sample From, What to Estimate, How to Use</h3>

<p>When implementing a KL penalty, it helps to separate three interrelated questions:</p>

<ol>
  <li><strong>Who to sample from?</strong> Do samples come from the current policy $q_\theta$ (on-policy), or from a behavior policy $\mu$ (off-policy)?</li>
  <li><strong>What to estimate?</strong> Are we trying to estimate reverse KL $D_{\mathrm{KL}}(q_\theta \| p)$ or forward KL $D_{\mathrm{KL}}(p \| q_\theta)$?</li>
  <li><strong>How to use it?</strong> Is the KL term used as a differentiable loss term, or as a detached reward penalty (stop-gradient)?</li>
</ol>

<p>Different combinations of these three questions determine which estimator should be used. The goal of this post is to systematically clarify these choices and their interrelationships.</p>

<h2 id="preliminaries-notation-and-basic-concepts">Preliminaries: Notation and Basic Concepts</h2>

<p>Before diving into the analysis, let‚Äôs unify our notation and derive two fundamental results that will be used repeatedly.</p>

<h3 id="notation-sampling-distribution-and-true-gradients">Notation, Sampling Distribution, and True Gradients</h3>

<p><strong>Notation:</strong></p>

<ul>
  <li>$q_\theta$: Current actor policy (parameterized by $\theta$)</li>
  <li>$q$: When unambiguous, we write $q := q_\theta$</li>
  <li>$p$: Reference policy (independent of $\theta$)</li>
  <li>$\mu$: Behavior policy for off-policy sampling (independent of $\theta$)</li>
  <li>$s_\theta(x) = \nabla_\theta \log q_\theta(x)$: Score function</li>
  <li>$\text{sg}(\cdot)$: Stop-gradient operation (<code class="language-plaintext highlighter-rouge">.detach()</code> in code)</li>
</ul>

<h4 id="a-unified-perspective-on-sampling-policies-introducing-the-inlmath91mathend-notation">A Unified Perspective on Sampling Policies: Introducing the $\rho$ Notation</h4>

<p>When analyzing the gradient properties of KL estimators, on-policy and off-policy scenarios may seem to require separate treatment, but we can actually describe them within a unified framework.</p>

<p>Introduce the <strong>sampling policy</strong> $\mu$, meaning data are drawn from $x \sim \mu$. Define the <strong>unified ratio</strong>:</p>

<p>$$
\rho(x) := \frac{q_\theta(x)}{\text{sg}(\mu(x))}
$$</p>

<p>The key insight is: <strong>in both on-policy and off-policy analyses, we treat the sampling policy $\mu$ as a gradient constant</strong> (i.e., apply stop-gradient to $\mu$).</p>

<ul>
  <li><strong>Off-policy</strong> ($\mu \neq q_\theta$): $\mu$ is inherently independent of $\theta$, so $\text{sg}(\mu) = \mu$, giving $\rho = \frac{q_\theta}{\mu}$</li>
  <li><strong>On-policy</strong> ($\mu = q_\theta$): Set $\mu = q_\theta$ but stop its gradient, so $\rho = \frac{q_\theta}{\text{sg}(q_\theta)} \equiv 1$ (numerically always 1), while still having $\nabla_\theta \rho = s_\theta \neq 0$</li>
</ul>

<p><strong>Implementation note</strong>: In the on-policy case, even though $\rho \equiv 1$ numerically, you must explicitly construct $\rho = \frac{q_\theta}{\text{sg}(q_\theta)}$ (or equivalently $\rho = \exp(\log q_\theta - \text{sg}(\log q_\theta))$) in the computation graph. If you replace it with the literal constant 1, you cut off the score-function path, causing the derivation to degenerate to the ‚Äúnaive on-policy implementation‚Äù described later.</p>

<p><strong>Intuition</strong>: The role of $\rho$ is to restore the gradient path for the sampling distribution‚Äôs dependence on $\theta$. In the on-policy case, this dependence is precisely why expect-then-differentiate and differentiate-then-expect can disagree, and why explicitly modeling $\rho$ resolves the mismatch.</p>

<p>With this unified notation, we can merge the on-policy and off-policy analyses into a single framework, greatly simplifying the derivations that follow.</p>

<h4 id="score-function-and-true-kl-gradients">Score Function and True KL Gradients</h4>

<p>The score function has an important property: $\mathbb{E}_{q_\theta}[s_\theta] = 0$ (since $\int \nabla_\theta q_\theta dx = \nabla_\theta \int q_\theta dx = \nabla_\theta 1 = 0$).</p>

<p>Using this property, we can derive the <strong>true gradients</strong> of forward and reverse KL divergences with respect to $\theta$.</p>

<p><strong>Reverse KL Gradient:</strong></p>

<p>$$
D_{\mathrm{KL}}(q_\theta \| p) = \int q_\theta(x) \log \frac{q_\theta(x)}{p(x)} dx
$$</p>

<p>Differentiating with respect to $\theta$ (using the product rule):</p>

<p>$$
\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \int \nabla_\theta q_\theta \cdot \log \frac{q_\theta}{p} dx + \int q_\theta \cdot \nabla_\theta \log \frac{q_\theta}{p} dx
$$</p>

<p>Using $\nabla_\theta q_\theta = q_\theta \cdot s_\theta$, $\nabla_\theta \log q_\theta = s_\theta$, and $\nabla_\theta \log p = 0$:</p>

<p>$$
= \mathbb{E}_{q_\theta}\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] + \mathbb{E}_{q_\theta}[s_\theta] = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \log \frac{q_\theta}{p}\right]
$$</p>

<p>Thus:</p>

<p>$$
\boxed{\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] = -\mathbb{E}_{q_\theta}\left[s_\theta \cdot \log \frac{p}{q_\theta}\right]}
$$</p>

<blockquote>
  <p><strong>Preview</strong>: We will later define $k_1 := -\log\frac{p}{q_\theta}$, so the above can be written concisely as $\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1]$ ‚Äî this form appears repeatedly in gradient analysis.</p>
</blockquote>

<p><strong>Forward KL Gradient:</strong></p>

<p>$$
D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \log \frac{p(x)}{q_\theta(x)} dx
$$</p>

<p>Since $p(x)$ is independent of $\theta$:</p>

<p>$$
\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \cdot \nabla_\theta \left(-\log q_\theta(x)\right) dx = -\mathbb{E}_p[s_\theta]
$$</p>

<p>To estimate this using samples from $q$, apply importance sampling:</p>

<p>$$
-\mathbb{E}_p[s_\theta] = -\mathbb{E}_{q_\theta}\left[\frac{p}{q_\theta} \cdot s_\theta\right]
$$</p>

<p>Using $\mathbb{E}_{q_\theta}[s_\theta] = 0$, this can be rewritten as:</p>

<p>$$
\boxed{\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_{q_\theta}\left[\left(1-\frac{p}{q_\theta}\right) \cdot s_\theta\right]}
$$</p>

<blockquote>
  <p><strong>Preview</strong>: We will later derive $\nabla_\theta k_3 = (1-\frac{p}{q_\theta}) s_\theta$, so $\mathbb{E}_{q_\theta}[\nabla_\theta k_3] = \nabla_\theta D_{\mathrm{KL}}(p \| q_\theta)$ (forward KL). This is why directly backpropagating through $k_3$ produces the ‚Äúwrong‚Äù gradient direction when you intend reverse KL.</p>
</blockquote>

<p>With these two results, we can later determine which KL‚Äôs true gradient each estimator‚Äôs gradient expectation corresponds to.</p>

<h2 id="three-estimators-definitions-and-design-principles">Three Estimators: Definitions and Design Principles</h2>

<p>Let $\frac{p(x)}{q_\theta(x)}$ denote the ratio. John Schulman proposed three single-sample estimators, defined as follows:</p>

<h3 id="the-three-estimators-definitions-and-intuition">The Three Estimators: Definitions and Intuition</h3>

<p><strong>$k_1$: The Naive Log-Ratio Estimator</strong></p>

<p>$$
k_1(x) = -\log \frac{p(x)}{q_\theta(x)} = \log q_\theta(x) - \log p(x)
$$</p>

<p>This is the most direct definition: the negative log-ratio. It is unbiased for reverse KL, but it has a major drawback: <strong>it can be negative</strong>, while KL divergence is always non-negative. This can lead to high variance because positive and negative samples cancel.</p>

<p><strong>$k_2$: The Squared Estimator Based on f-Divergence</strong></p>

<p>$$
k_2(x) = \frac{1}{2}\left(\log \frac{p(x)}{q_\theta(x)}\right)^2
$$</p>

<p><strong>Design motivation</strong>: $k_1$ can be either positive or negative; squaring yields an estimator where <strong>every sample is non-negative</strong>, and each sample measures the magnitude of mismatch between $p$ and $q$.</p>

<p><strong>Why is the bias often small?</strong> $k_2$ corresponds to an <strong>f-divergence</strong> with $f(x) = \frac{1}{2}(\log x)^2$. A key fact is that <strong>any twice-differentiable f-divergence admits a second-order expansion around $q \approx p$ of the form</strong></p>

<p>$$
D_f\big(p, q_{\theta_0+\Delta\theta}\big) = D_f\big(p, q_{\theta_0}\big) + \frac{f^{\prime\prime}(1)}{2}\, \Delta\theta^T F(\theta_0)\, \Delta\theta + O(\|\Delta\theta\|^3)
$$</p>

<p>where $F(\theta_0)$ is the Fisher information matrix at $\theta_0$. KL divergence corresponds to $f(x) = -\log x$, with $f^{\prime\prime}(1) = 1$, while $k_2$ corresponds to $f(x) = \frac{1}{2}(\log x)^2$, which also has $f^{\prime\prime}(1) = 1$. This means that <strong>when the two policies are close, $\mathbb{E}_{q_\theta}[k_2]$ and the true KL share the same local second-order curvature</strong>, with differences appearing only in higher-order terms.</p>

<p><strong>$k_3$: The Bregman Divergence Estimator via Control Variates</strong></p>

<p>$$
k_3(x) = \frac{p(x)}{q_\theta(x)} - 1 - \log \frac{p(x)}{q_\theta(x)}
$$</p>

<p><strong>Design motivation</strong>: We want an estimator that is <strong>both unbiased and low variance</strong>. A standard approach is to add a <strong>control variate</strong> to $k_1$‚Äîa term with zero expectation that (ideally) is negatively correlated with $k_1$.</p>

<p>Note that $\mathbb{E}_{q_\theta}\left[\frac{p}{q_\theta} - 1\right] = \mathbb{E}_{q_\theta}\left[\frac{p}{q_\theta}\right] - 1 = 1 - 1 = 0$, so for any $\lambda$,</p>

<p>$$
k_1 + \lambda\left(\frac{p}{q_\theta} - 1\right) = -\log \frac{p}{q_\theta} + \lambda\left(\frac{p}{q_\theta} - 1\right)
$$</p>

<p>remains an unbiased estimator.</p>

<p><strong>Why choose $\lambda = 1$?</strong> Since $\log$ is concave, we have $\log x \leq x - 1$, therefore</p>

<p>$$
k_3 = \left(\frac{p}{q_\theta} - 1\right) - \log \frac{p}{q_\theta} \geq 0
$$</p>

<p>It is <strong>always non-negative</strong>. This ensures every sample contributes ‚Äúpositively‚Äù to the estimate, eliminating the cancellation problem of $k_1$.</p>

<p><strong>Geometric intuition</strong>: $k_3$ is actually a <strong>Bregman divergence</strong>. Consider the convex function $\phi(x) = -\log x$, whose tangent at $x=1$ is $y = 1 - x$. The Bregman divergence is defined as the difference between the function value and the tangent value:</p>

<p>$$
\begin{aligned}
D_\phi\left(\frac{p}{q_\theta}, 1\right) &= \phi\left(\frac{p}{q_\theta}\right) - \phi(1) - \phi'(1)\left(\frac{p}{q_\theta} - 1\right) \\
&= -\log \frac{p}{q_\theta} - 0 - (-1)\left(\frac{p}{q_\theta} - 1\right) \\
&= \frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta} \\
&= k_3.
\end{aligned}
$$</p>

<p>Since a convex function always lies above its tangent, this gap is <strong>naturally non-negative</strong>. More importantly, as $\frac{p}{q_\theta} \to 1$, the gap shrinks at a second-order rate \left(\frac{p}{q_\theta} - 1\right)^2, which is the fundamental reason why $k_3$ tends to have lower variance when the policies are close.</p>

<p><strong>Summary: Design Logic Comparison</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Definition</th>
      <th style="text-align: center">Design Principle</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">$-\log \frac{p}{q_\theta}$</td>
      <td style="text-align: center">Naive definition</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">$\frac{1}{2}\left(\log \frac{p}{q_\theta}\right)^2$</td>
      <td style="text-align: center">f-divergence, matches KL to second order</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">$\frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta}$</td>
      <td style="text-align: center">Control variate + Bregman divergence</td>
    </tr>
  </tbody>
</table>

<p>With the definitions and design principles in place, we first analyze their behavior as <strong>value estimators</strong> of KL‚Äîspecifically, bias and variance.</p>

<h2 id="value-estimation-bias-and-variance">Value Estimation: Bias and Variance</h2>

<p>This section analyzes the properties of the three estimators when <strong>estimating KL values</strong>. These properties are fundamental in any usage scenario.</p>

<p>Assume we sample from $q_\theta$ to estimate reverse KL $D_{\mathrm{KL}}(q_\theta \| p)$:</p>

<h3 id="unbiasedness-analysis">Unbiasedness Analysis</h3>

<p>$$
\begin{aligned}
\mathbb{E}_{q_\theta}[k_1] &= \mathbb{E}_{q_\theta}\left[\log \tfrac{q_\theta}{p}\right] = D_{\mathrm{KL}}(q_\theta \| p) && \textbf{(unbiased)} \\[8pt]
\mathbb{E}_{q_\theta}[k_3] &= \mathbb{E}_{q_\theta}\left[\frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta}\right] && \\
&= 1 - 1 + D_{\mathrm{KL}}(q_\theta \| p) && \\
&= D_{\mathrm{KL}}(q_\theta \| p) && \textbf{(unbiased)} \\[8pt]
\mathbb{E}_{q_\theta}[k_2] &= \frac{1}{2}\mathbb{E}_{q_\theta}\left[\left(\log \frac{p}{q_\theta}\right)^2\right] \neq D_{\mathrm{KL}}(q_\theta \| p) && \textbf{(biased)}
\end{aligned}
$$</p>

<p><strong>Conclusion</strong>: For reverse KL <strong>values</strong>, $k_1$ and $k_3$ are unbiased estimators, while $k_2$ is biased.</p>

<h3 id="variance-characteristics">Variance Characteristics</h3>

<p>John Schulman‚Äôs experiments ($q = \mathcal{N}(0,1)$, $p = \mathcal{N}(0.1,1)$, true KL = 0.005) show:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">20</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">0.002</td>
      <td style="text-align: center">1.42</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.42</td>
    </tr>
  </tbody>
</table>

<p>When KL is large ($p = \mathcal{N}(1,1)$, true KL = 0.5):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">0.25</td>
      <td style="text-align: center">1.73</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.7</td>
    </tr>
  </tbody>
</table>

<p><strong>Intuition</strong>:</p>

<ul>
  <li>$k_1 = -\log \frac{p}{q}$ has a first-order term; when $\frac{p}{q}$ is close to 1 it can fluctuate substantially and can be negative.</li>
  <li>$k_3 = \frac{p}{q} - 1 - \log \frac{p}{q}$ is second-order around $\frac{p}{q}=1$ and is always non-negative, which typically yields lower variance when the policies are close.</li>
  <li>In extreme mismatch regimes where $\frac{p}{q}$ can blow up, $k_3$ can inherit large variance from the ratio; in such cases $k_1$ may be more numerically stable.</li>
</ul>

<p><strong>Summary of Value Estimation</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Bias for value</th>
      <th style="text-align: center">Variance characteristics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">Unbiased</td>
      <td style="text-align: center">High (can be +/-)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">Biased (minimal)</td>
      <td style="text-align: center">Low (always positive)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">Unbiased</td>
      <td style="text-align: center">Low (always positive)</td>
    </tr>
  </tbody>
</table>

<p>From a pure value-estimation perspective, $k_3$ is often the best choice among unbiased estimators due to its lower variance.</p>

<blockquote>
  <p><strong>Note</strong>: To estimate the <strong>forward KL value</strong> $D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_p\left[\log \frac{p}{q_\theta}\right]$, but only sample from $q_\theta$, use importance sampling $\mathbb{E}_{q_\theta}\left[\frac{p}{q_\theta} \log \frac{p}{q_\theta}\right]$.</p>
</blockquote>

<h2 id="two-ways-to-use-a-kl-penalty">Two Ways to Use a KL Penalty</h2>

<p>Having understood the value properties of these estimators, we need to further clarify: <strong>How exactly is the KL penalty applied in reinforcement learning?</strong> This choice determines whether we only care about the estimator‚Äôs value properties, or must also consider its gradient properties.</p>

<p>Recall the objective for KL-regularized reinforcement learning (where $\tau \sim q_\theta$ denotes the trajectory distribution induced by policy $q_\theta$):</p>

<p>$$
J(\theta) = \mathbb{E}_{\tau \sim q_\theta} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right] - \beta \cdot D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p>This mathematical form looks unified, but in actor-critic algorithms (e.g., PPO) it gives rise to two fundamentally different implementation paradigms. They often differ by only a few lines of code, yet correspond to different optimization semantics.</p>

<blockquote>
  <p><strong>Notation</strong>: In this section, we use $\text{KL}_t$ or $\text{KL}(s)$ to generically refer to a token/state-level KL estimator (such as $k_1, k_2, k_3$), with specific definitions from the earlier section ‚ÄúThree Estimators: Definitions and Design Principles‚Äù.</p>
</blockquote>

<h3 id="as-a-loss-term-kl-participates-in-backpropagation">As a Loss Term: KL Participates in Backpropagation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantage</span> <span class="o">*</span> <span class="n">log_prob</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>  <span class="c1"># kl participates in gradient
</span></code></pre></div></div>

<p>The critic learns only the environment value function; the KL term acts as an explicit regularizer for the actor and participates directly in backpropagation.</p>

<h3 id="as-a-reward-penalty-kl-enters-reward-shaping">As a Reward Penalty: KL Enters Reward Shaping</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kl</span> <span class="o">=</span> <span class="nf">compute_kl</span><span class="p">(</span><span class="n">log_prob_q</span><span class="p">,</span> <span class="n">log_prob_p</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">shaped_reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>
</code></pre></div></div>

<p>KL is treated as part of the reward via reward shaping, and the actor-critic update is performed on the shaped reward. The KL term itself is detached and does not backpropagate.</p>

<p>These two approaches may look like they differ only by a <code class="language-plaintext highlighter-rouge">.detach()</code>, but they correspond to different optimization semantics. A detailed comparison appears later in ‚Äú$k_1$ in Reward vs. Low-Variance KL in Loss: Equivalence and Differences‚Äù. Here we first summarize the core distinction:</p>

<ul>
  <li><strong>KL as a loss term</strong>: Requires correct gradients for the KL component, including which objective those gradients correspond to.</li>
  <li><strong>KL as a reward penalty</strong>: Requires accurate KL values, and also requires that the induced policy-gradient update matches the intended objective.</li>
</ul>

<p>Below we analyze estimator gradients under the two usage modes: as a differentiable loss term and as a detached reward penalty.</p>

<h2 id="gradient-analysis-when-used-as-a-loss-term">Gradient Analysis When Used as a Loss Term</h2>

<p>When KL serves as a differentiable loss term, the key question is which objective each estimator actually optimizes through its gradient. This is subtle but central in practice.</p>

<p>Leveraging the unified framework introduced earlier, we can merge the on-policy and off-policy analyses into a single derivation. Recall the unified ratio definition:</p>

<p>$$
\rho(x) := \frac{q_\theta(x)}{\text{sg}(\mu(x))}
$$</p>

<p>where $\mu$ is the sampling policy. Within this framework:</p>

<ul>
  <li><strong>On-policy</strong> ($\mu = q_\theta$): $\rho \equiv 1$, but $\nabla_\theta \rho = s_\theta$</li>
  <li><strong>Off-policy</strong> ($\mu \neq q_\theta$): $\rho = \frac{q_\theta}{\mu}$, and $\nabla_\theta \rho = \rho \cdot s_\theta$</li>
</ul>

<h3 id="basic-gradients-of-the-three-estimators">Basic Gradients of the Three Estimators</h3>

<p>First, we compute the gradients of the three estimators themselves (without $\rho$). These results will be used repeatedly in subsequent analysis.</p>

<p><strong>Deriving $\nabla_\theta k_1$</strong>:</p>

<p>$$
k_1 = -\log \frac{p(x)}{q_\theta(x)} = \log q_\theta(x) - \log p(x)
$$</p>

<p>$$
\nabla_\theta k_1 = \nabla_\theta \log q_\theta(x) - \nabla_\theta \log p(x) = s_\theta - 0 = s_\theta
$$</p>

<p><strong>Deriving $\nabla_\theta k_2$</strong>:</p>

<p>$$
k_2 = \frac{1}{2}\left(\log \frac{p}{q_\theta}\right)^2
$$</p>

<p>By the chain rule:</p>

<p>$$
\begin{aligned}
\nabla_\theta k_2
&= \left(\log \frac{p}{q_\theta}\right) \cdot \nabla_\theta\left(\log \frac{p}{q_\theta}\right) \\
&= \left(\log \frac{p}{q_\theta}\right) \cdot \nabla_\theta(\log p(x) - \log q_\theta(x)) \\
&= \left(\log \frac{p}{q_\theta}\right)(-s_\theta) \\
&= - \left(\log \frac{p}{q_\theta}\right) s_\theta.
\end{aligned}
$$</p>

<p><strong>Deriving $\nabla_\theta k_3$</strong>:</p>

<p>$$
k_3 = \frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta}
$$</p>

<p>First, compute $\nabla_\theta \frac{p}{q_\theta}$. Since $\frac{p}{q_\theta} = p(x) \cdot q_\theta(x)^{-1}$:</p>

<p>$$
\nabla_\theta \frac{p}{q_\theta} = p(x) \cdot (-1) \cdot q_\theta(x)^{-2} \cdot \nabla_\theta q_\theta(x) = -\frac{p(x)}{q_\theta(x)} \cdot \frac{\nabla_\theta q_\theta(x)}{q_\theta(x)} = -\frac{p}{q_\theta} \cdot s_\theta
$$</p>

<p>Then compute $\nabla_\theta \log \frac{p}{q_\theta}$:</p>

<p>$$
\nabla_\theta \log \frac{p}{q_\theta} = \frac{q_\theta}{p} \nabla_\theta \frac{p}{q_\theta} = \frac{q_\theta}{p} \cdot \left(-\frac{p}{q_\theta} \cdot s_\theta\right) = -s_\theta
$$</p>

<p>Therefore:</p>

<p>$$
\nabla_\theta k_3 = \nabla_\theta \frac{p}{q_\theta} - 0 - \nabla_\theta \log \frac{p}{q_\theta} = -\frac{p}{q_\theta} \cdot s_\theta - (-s_\theta) = \left(1 - \frac{p}{q_\theta}\right) \cdot s_\theta
$$</p>

<p><strong>Summary</strong>: The gradients of the three estimators are:</p>

<ul>
  <li>$\nabla_\theta k_1 = s_\theta$</li>
  <li>$\nabla_\theta k_2 = -\left(\log \frac{p}{q_\theta}\right) s_\theta = k_1 \cdot s_\theta$</li>
  <li>$\nabla_\theta k_3 = \left(1 - \frac{p}{q_\theta}\right) s_\theta$</li>
</ul>

<p>These basic gradients will be used repeatedly in the unified framework analysis that follows.</p>

<h4 id="expect-then-differentiate-vs-differentiate-then-expect-a-key-pitfall">‚ÄúExpect-then-Differentiate‚Äù vs. ‚ÄúDifferentiate-then-Expect‚Äù: A Key Pitfall</h4>

<p>When analyzing estimator gradients, there is a common pitfall: <strong>‚Äúexpect-then-differentiate‚Äù and ‚Äúdifferentiate-then-expect‚Äù need not agree</strong>.</p>

<p>If we treat $\mathbb{E}_{q_\theta}[k_i]$ as a function of $\theta$ and differentiate analytically (i.e., ‚Äúexpect-then-differentiate‚Äù), then because $\mathbb{E}_{q_\theta}[k_1] = \mathbb{E}_{q_\theta}[k_3] = D_{\mathrm{KL}}(q_\theta \| p)$, we have:</p>

<p>$$
\nabla_\theta \mathbb{E}_{q_\theta}[k_1] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p>$$
\nabla_\theta \mathbb{E}_{q_\theta}[k_3] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p>Both yield the reverse-KL gradient. However, when you backpropagate through the sample mean of $k_i$ in code, autograd effectively computes ‚Äúdifferentiate-then-expect‚Äù, i.e., $\mathbb{E}_{q_\theta}[\nabla_\theta k_i]$, which <strong>can differ</strong>.</p>

<p>The root cause is that the sampling distribution $q_\theta$ depends on $\theta$, so expectation and differentiation cannot be exchanged naively. This is exactly the subtlety in the on-policy case, and why we introduce the unified $\rho$ framework.</p>

<h3 id="gradient-analysis-under-the-unified-framework">Gradient Analysis Under the Unified Framework</h3>

<p>Now, we use the $\rho$ framework to uniformly handle on-policy and off-policy scenarios. Consider the loss function form $L = \rho \cdot k$, where $\rho = \frac{q_\theta}{\text{sg}(\mu)}$.</p>

<p><strong>Key observation</strong>: Because $\text{sg}(\mu)$ does not depend on $\theta$, for any differentiable $f_\theta(x)$ we have</p>

<p>$$
\nabla_\theta \mathbb{E}_{\mu}[f_\theta(x)] = \mathbb{E}_{\mu}[\nabla_\theta f_\theta(x)]
$$</p>

<p>This means that under the $\rho$ framework, ‚Äúexpect-then-differentiate‚Äù and ‚Äúdifferentiate-then-expect‚Äù <strong>are always equivalent</strong>, whether on-policy or off-policy.</p>

<blockquote>
  <p><strong>Note</strong>: The expectation here is $\mathbb{E}_\mu[\cdot]$ over a <strong>fixed</strong> sampling distribution $\mu$. We route ‚Äúdistribution dependence on $\theta$‚Äù through $\rho = \frac{q_\theta}{\text{sg}(\mu)}$. This does not mean you can always exchange expectation and differentiation under $\mathbb{E}_{q_\theta}[\cdot]$.</p>
</blockquote>

<h4 id="gradient-derivations-for-the-three-estimators-under-the-unified-framework">Gradient Derivations for the Three Estimators Under the Unified Framework</h4>

<p>Using $\nabla_\theta \rho = \rho \cdot s_\theta$ (since $\rho = q_\theta / \text{sg}(\mu)$), combined with the previously derived $\nabla_\theta k_i$, applying the product rule:</p>

<p><strong>$\nabla_\theta(\rho k_1)$</strong>:</p>

<p>$$
\nabla_\theta(\rho k_1) = (\nabla_\theta \rho) k_1 + \rho (\nabla_\theta k_1) = \rho s_\theta k_1 + \rho s_\theta = \rho s_\theta (k_1 + 1)
$$</p>

<p><strong>$\nabla_\theta(\rho k_2)$</strong>:</p>

<p>$$
\nabla_\theta(\rho k_2) = \rho s_\theta k_2 + \rho \left(-\log \frac{p}{q_\theta}\right) s_\theta = \rho s_\theta \left(k_2 - \log \frac{p}{q_\theta}\right) = \rho s_\theta (k_2 + k_1)
$$</p>

<p><strong>$\nabla_\theta(\text{sg}(\rho) k_2)$</strong> (applying stop-gradient to $\rho$):</p>

<p>$$
\nabla_\theta(\text{sg}(\rho) k_2) = \text{sg}(\rho) \cdot \nabla_\theta k_2 = \rho \cdot \left(-\log \frac{p}{q_\theta}\right) s_\theta = \rho s_\theta k_1
$$</p>

<p><strong>$\nabla_\theta(\rho k_3)$</strong>:</p>

<p>$$
\nabla_\theta(\rho k_3) = \rho s_\theta k_3 + \rho \left(1-\frac{p}{q_\theta}\right) s_\theta = \rho s_\theta \left(k_3 + 1 - \frac{p}{q_\theta}\right)
$$</p>

<p>Substituting $k_3 = \frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta}$:</p>

<p>$$
k_3 + 1 - \frac{p}{q_\theta} = \left(\frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta}\right) + 1 - \frac{p}{q_\theta} = -\log \frac{p}{q_\theta} = k_1
$$</p>

<p>Thus we obtain a key simplification:</p>

<p>$$
\boxed{\nabla_\theta(\rho k_3) = \rho s_\theta k_1}
$$</p>

<h4 id="gradient-expectations-and-optimization-objectives">Gradient Expectations and Optimization Objectives</h4>

<p>Using $\mathbb{E}_\mu[\rho \cdot f] = \mathbb{E}_{q_\theta}[f]$ and $\mathbb{E}_{q_\theta}[s_\theta] = 0$:</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(\rho k_1)]$</strong>:</p>

<p>$$
\mathbb{E}_\mu[\rho s_\theta (k_1 + 1)] = \mathbb{E}_{q_\theta}[s_\theta k_1] + \underbrace{\mathbb{E}_{q_\theta}[s_\theta]}_{=0} = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark
$$</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(\rho k_2)]$</strong>:</p>

<p>$$
\begin{aligned}
\mathbb{E}_\mu[\rho s_\theta (k_2 + k_1)]
&= \mathbb{E}_{q_\theta}[s_\theta k_2] + \mathbb{E}_{q_\theta}[s_\theta k_1] \\
&= \mathbb{E}_{q_\theta}[s_\theta k_2] + \mathbb{E}_{q_\theta}[\nabla_\theta k_2] && \text{(since } \nabla_\theta k_2 = k_1 s_\theta \text{)} \\
&= \nabla_\theta \mathbb{E}_{q_\theta}[k_2] && \text{(Leibniz rule)}
\end{aligned}
$$</p>

<p>In other words, the gradient expectation of $\rho k_2$ corresponds to ‚Äúminimizing $\mathbb{E}_{q_\theta}[k_2]$‚Äù (an f-divergence with second-order behavior matching KL), <strong>not</strong> the true gradient of reverse KL $D_{\mathrm{KL}}(q_\theta\|p)$; therefore, when the goal is reverse KL, avoid using $\rho k_2$.</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(\text{sg}(\rho) k_2)]$</strong>:</p>

<p>$$
\mathbb{E}_\mu[\rho s_\theta k_1] = \mathbb{E}_{q_\theta}[s_\theta k_1] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark
$$</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(\rho k_3)]$</strong>:</p>

<p>$$
\mathbb{E}_\mu[\rho s_\theta k_1] = \mathbb{E}_{q_\theta}[s_\theta k_1] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark
$$</p>

<h4 id="gradient-equivalence-which-methods-produce-identical-gradient-random-variables">Gradient Equivalence: Which Methods Produce Identical Gradient Random Variables</h4>

<p>From the above derivations, we discover a key fact:</p>

<blockquote>
  <p><strong>$\text{sg}(\rho) k_2$ and $\rho k_3$ have identical gradients</strong>: $\nabla_\theta(\text{sg}(\rho) k_2) = \nabla_\theta(\rho k_3) = \rho s_\theta k_1$</p>
</blockquote>

<p>This means they are equal not only in expectation, but <strong>as random variables</strong>: same mean, variance, and higher moments.</p>

<p><strong>Summary Table</strong>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Loss Form</th>
      <th style="text-align: center">Gradient Random Variable</th>
      <th style="text-align: center">Expected Gradient</th>
      <th style="text-align: center">Optimization Objective</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$\rho k_1$</td>
      <td style="text-align: center">$\rho s_\theta (k_1+1)$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL ‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$\rho k_2$</td>
      <td style="text-align: center">$\rho s_\theta (k_2 + k_1)$</td>
      <td style="text-align: center">$\nabla_\theta \mathbb{E}_{q_\theta}[k_2]$</td>
      <td style="text-align: center">f-divergence (not reverse KL) ‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\text{sg}(\rho) k_2$</td>
      <td style="text-align: center">$\rho s_\theta k_1$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL ‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$\rho k_3$</td>
      <td style="text-align: center">$\rho s_\theta k_1$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL ‚úì</td>
    </tr>
  </tbody>
</table>

<h3 id="a-unified-view-of-on-policy-and-off-policy">A Unified View of On-Policy and Off-Policy</h3>

<p>We can now revisit the relationship between on-policy and off-policy settings through the unified framework.</p>

<p><strong>On-policy</strong> ($\mu = q_\theta$):</p>

<ul>
  <li>$\rho = \frac{q_\theta}{\text{sg}(q_\theta)} \equiv 1$ (numerically always 1)</li>
  <li>$\rho k_1 = k_1$, $\rho k_2 = k_2$, $\rho k_3 = k_3$</li>
  <li>But the gradients differ, because $\nabla_\theta \rho = s_\theta \neq 0$.</li>
</ul>

<p>This explains why <strong>naive direct backpropagation</strong> (i.e., without explicitly constructing $\rho$) fails when using $k_1$ or $k_3$ as the KL loss term in the on-policy case:</p>

<ul>
  <li>Directly using $k_1$ (without $\rho$): $\mathbb{E}_{q_\theta}[\nabla k_1] = \mathbb{E}_{q_\theta}[s_\theta] = 0$, so the KL term is ineffective.</li>
  <li>Directly using $k_3$ (without $\rho$): $\mathbb{E}_{q_\theta}[\nabla k_3] = \nabla D_{\mathrm{KL}}(p \| q_\theta)$ (forward KL), i.e., the wrong direction for reverse-KL regularization.</li>
  <li>Directly using $k_2$: $\mathbb{E}_{q_\theta}[\nabla k_2] = \nabla D_{\mathrm{KL}}(q_\theta \| p)$ (reverse KL), which makes it the only correct choice under the naive implementation.</li>
</ul>

<p>If you <strong>explicitly construct</strong> $\rho = \frac{q_\theta}{\text{sg}(q_\theta)}$, then:</p>

<ul>
  <li><strong>Usable</strong>: $\rho k_1$ (higher variance), $\text{sg}(\rho) k_2$ (recommended), and $\rho k_3$ (recommended) all yield reverse-KL gradients.</li>
  <li><strong>Not usable</strong>: $\rho k_2$ (where $\rho$ participates in the gradient) optimizes an f-divergence rather than the reverse KL.</li>
</ul>

<p><strong>Off-policy</strong> ($\mu \neq q_\theta$):</p>

<ul>
  <li>$\rho = \frac{q_\theta}{\mu}$ (standard importance weight)</li>
  <li><strong>Usable</strong>: $\rho k_1$ (higher variance), $\text{sg}(\rho) k_2$ (recommended), and $\rho k_3$ (recommended) all yield reverse-KL gradients.</li>
  <li><strong>Not usable</strong>: $\rho k_2$ (where $\rho$ participates in the gradient) optimizes an f-divergence rather than the reverse KL.</li>
</ul>

<p><strong>Key insight</strong>: The reason $k_2$ works directly in the on-policy case is that its gradient $-\log\frac{p}{q_\theta} \cdot s_\theta = k_1 \cdot s_\theta$ happens to match $\rho s_\theta k_1$ (when $\rho \equiv 1$). This is a special case, not a general rule.</p>

<p>For an in-depth analysis of off-policy scenarios in large language models, refer to my previous blog post: <a href="/reinforcement-learning/2025/11/15/three-policy-en.html">From Two-Policy to Three-Policy: TRPO Extension Under Behavior-Reference Mismatch in LLM RL</a>.</p>

<h3 id="variance-analysis">Variance Analysis</h3>

<p>Earlier we saw that three choices give unbiased gradients for reverse KL: $\rho k_1$, $\text{sg}(\rho) k_2$, $\rho k_3$. Their gradient random variables are (note that $s_\theta$ is a vector, so the gradient is also a vector):</p>

<p>$$
g_1(x) = \rho(x) s_\theta(x) (k_1(x) + 1), \quad g_\star(x) = \rho(x) s_\theta(x) k_1(x)
$$</p>

<p>where $g_\star$ corresponds to both $\text{sg}(\rho) k_2$ and $\rho k_3$ (they are identical).</p>

<p>To avoid ambiguity in ‚Äúvariance of a vector gradient‚Äù, we compare the projection variance in any direction: take any unit vector $u$, and define scalar random variables</p>

<p>$$
g_1^{(u)} := u^\top g_1, \quad g_\star^{(u)} := u^\top g_\star.
$$</p>

<p>Let $A_u(x) := \rho(x)\, u^\top s_\theta(x)$, $B(x) := k_1(x)$, then</p>

<p>$$
g_1^{(u)} = A_u(B+1), \quad g_\star^{(u)} = A_u B.
$$</p>

<p>Both have the same expectation, and the variance difference in any direction is</p>

<p>$$
\boxed{
\mathrm{Var}_\mu\big(g_1^{(u)}\big) - \mathrm{Var}_\mu\big(g_\star^{(u)}\big)
= \mathbb{E}_\mu\big[A_u(x)^2 \big(2B(x)+1\big)\big]
= \mathbb{E}_\mu\Big[\rho(x)^2\,\big(u^\top s_\theta(x)\big)^2\,\big(2k_1(x)+1\big)\Big].
}
$$</p>

<p>(You can also understand this as comparing variance for each coordinate component separately; the conclusion is consistent with intuitive magnitude estimates.)</p>

<p><strong>In the typical KL penalty regime</strong> ($q_\theta \approx p \approx \mu$), setting $\frac{p(x)}{q_\theta(x)} = 1 + \varepsilon(x)$, $|\varepsilon| \ll 1$:</p>

<ul>
  <li>$k_1 = -\log \frac{p}{q_\theta} \approx -\varepsilon$</li>
  <li>$2k_1 + 1 \approx 1 - 2\varepsilon$, with the leading term being a positive $O(1)$ constant</li>
</ul>

<p>Therefore $\mathrm{Var}_\mu(g_1) > \mathrm{Var}_\mu(g_\star)$.</p>

<p><strong>Core intuitive understanding</strong>:</p>

<ul>
  <li>$g_1 = \rho s_\theta (k_1 + 1)$ contains a zero-mean noise term of magnitude $O(1)$: $\rho s_\theta$</li>
  <li>$g_\star = \rho s_\theta k_1$ has eliminated this constant noise term, leaving only first-order terms proportional to $\varepsilon$</li>
</ul>

<p><strong>Variance Comparison Table</strong>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Gradient Random Variable</th>
      <th style="text-align: center">Coefficient Magnitude ($\frac{p}{q_\theta}\approx1$)</th>
      <th style="text-align: center">Variance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$\rho k_1$</td>
      <td style="text-align: center">$\rho s_\theta (k_1+1)$</td>
      <td style="text-align: center">$O(1)$</td>
      <td style="text-align: center">High</td>
    </tr>
    <tr>
      <td style="text-align: center">$\text{sg}(\rho) k_2$</td>
      <td style="text-align: center">$\rho s_\theta k_1$</td>
      <td style="text-align: center">$O(\varepsilon)$</td>
      <td style="text-align: center">Low</td>
    </tr>
    <tr>
      <td style="text-align: center">$\rho k_3$</td>
      <td style="text-align: center">$\rho s_\theta k_1$</td>
      <td style="text-align: center">$O(\varepsilon)$</td>
      <td style="text-align: center">Low</td>
    </tr>
  </tbody>
</table>

<p><strong>Conclusion</strong>: $\text{sg}(\rho) k_2$ and $\rho k_3$ are equivalent at the gradient level (the same random variable). In contrast, $\rho k_1$ contains an additional zero-mean constant term, which leads to substantially higher variance in the typical small-KL regime.</p>

<blockquote>
  <p><strong>Practical recommendation</strong>: For optimizing reverse KL, prefer $\rho k_3$ or $\text{sg}(\rho) k_2$ (both have equivalent gradients and low variance); $\rho k_1$ is unbiased but has higher variance, and can serve as a fallback with clipping/regularization.</p>
</blockquote>

<p><strong>Warning (extreme off-policy mismatch)</strong>:</p>

<p>When $\mu$ differs greatly from $q_\theta$ ‚Äî for example, when $\mu$ has almost no samples in high-density regions of $q_\theta$, or when $\rho = q_\theta / \mu$ explodes in the tails ‚Äî any $\rho$-based method will suffer from severe variance issues. In such cases, the advantage of $\rho k_3$ (or $\text{sg}(\rho) k_2$) over $\rho k_1$ is no longer theoretically guaranteed, and strategies like clipping and regularization must be combined.</p>

<p>However, in RL practice we typically control KL constraints and limit the degree of off-policy sampling (e.g., using a nearby policy $\mu = q_{\theta_\text{old}}$). In this common regime, we can say with confidence:</p>

<blockquote>
  <p><strong>If you‚Äôve decided to use importance sampling to optimize reverse KL, we recommend using $\rho k_3$ or $\text{sg}(\rho) k_2$ (both have equivalent gradients and low variance); in comparison, $\rho k_1$ has higher variance.</strong></p>
</blockquote>

<p>This is why the DeepSeek v3.2 technical report uses $\frac{q_\theta}{\mu} k_3$ as an off-policy KL penalty estimator.</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/dpsk-3d2-k3.png" style="width:95%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Source: <a href="https://arxiv.org/pdf/2512.02556v1">DeepSeek v3.2 Technical Report Section 3.1</a></figcaption>
</figure>

<h4 id="comprehensive-gradient-analysis-summary">Comprehensive Gradient Analysis Summary</h4>

<p>Combining the above analysis, the following table summarizes the gradient expectations and corresponding optimization objectives for each estimator under the unified framework:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Sampling Type</th>
      <th style="text-align: center">Loss</th>
      <th style="text-align: center">Expected $\nabla_\theta$ Loss</th>
      <th style="text-align: center">Optimization Objective</th>
      <th style="text-align: center">Usable for Reverse KL?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">on/off-policy</td>
      <td style="text-align: center">$\rho k_1$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì (higher variance)</td>
    </tr>
    <tr>
      <td style="text-align: center">on/off-policy</td>
      <td style="text-align: center">$\rho k_2$</td>
      <td style="text-align: center">$\nabla_\theta \mathbb{E}_{q_\theta}[k_2]$</td>
      <td style="text-align: center">f-divergence (not reverse KL)</td>
      <td style="text-align: center">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">on/off-policy</td>
      <td style="text-align: center">$\text{sg}(\rho) k_2$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì (recommended, low variance)</td>
    </tr>
    <tr>
      <td style="text-align: center">on/off-policy</td>
      <td style="text-align: center">$\rho k_3$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì (recommended, low variance)</td>
    </tr>
  </tbody>
</table>

<p>where $\rho = \frac{q_\theta}{\text{sg}(\mu)}$. When on-policy ($\mu = q_\theta$), $\rho \equiv 1$.</p>

<p>It must be emphasized: <strong>the conclusions in the table above apply to the unified framework where ‚Äúloss is written as $L=\rho\,k$ and $\rho$ retains its gradient path in the computation graph‚Äù</strong>. In the on-policy case, although $\rho \equiv 1$ numerically, since $\rho=\frac{q_\theta}{\text{sg}(q_\theta)}$, we still have $\nabla_\theta\rho=s_\theta\neq 0$, so $\rho k$ and ‚Äúdirectly backpropagating through the sample mean of $k$‚Äù are not equivalent in terms of gradients.</p>

<p>If you use the <strong>naive on-policy implementation</strong> (i.e., after sampling from $q_\theta$, treat $\{k_i(x)\}$ as ordinary scalars and directly backpropagate through their sample mean; without explicitly constructing $\rho=\frac{q_\theta}{\text{sg}(q_\theta)}$ to restore the score-function path), then it degenerates to:</p>

<ul>
  <li>Directly using $k_1$: $\mathbb{E}_{q_\theta}[\nabla k_1]=0$ (ineffective)</li>
  <li>Directly using $k_2$: $\mathbb{E}_{q_\theta}[\nabla k_2]=\nabla D_{\mathrm{KL}}(q_\theta\|p)$ (reverse KL) ‚úì</li>
  <li>Directly using $k_3$: $\mathbb{E}_{q_\theta}[\nabla k_3]=\nabla D_{\mathrm{KL}}(p\|q_\theta)$ (forward KL) ‚úó</li>
</ul>

<p><strong>Key Conclusions</strong>:</p>

<ol>
  <li><strong>On-policy optimization of reverse KL (naive direct backprop implementation)</strong>: The only correct choice is $k_2$</li>
  <li><strong>Off-policy optimization of reverse KL</strong>: Three correct options:</li>
</ol>

<ul>
  <li>$\rho k_1$: Unbiased but higher variance</li>
  <li>$\text{sg}(\rho) k_2$: Unbiased, <strong>gradient identical</strong> to $\rho k_3$</li>
  <li>$\rho k_3$: Unbiased and lower variance (equivalent to the above, both recommended)</li>
</ul>

<ol>
  <li><strong>$\rho k_2$ (weight participates in gradient) fails</strong>: This is an easily overlooked pitfall</li>
</ol>

<h2 id="gradient-analysis-when-used-as-a-reward-penalty">Gradient Analysis When Used as a Reward Penalty</h2>

<p>Having analyzed the gradient properties of the three estimators when used as loss, one might naturally think: since both $k_1$ and $k_3$ are unbiased for reverse KL value (see the ‚ÄúValue Estimation‚Äù section), using either of them (with stop-gradient) as a reward penalty should work fine.</p>

<p><strong>But this conclusion is incomplete.</strong></p>

<p>The issue is that when KL is used as a reward penalty, the KL term is detached, but it still influences the policy update through the advantage. Therefore, to decide whether an estimator is appropriate ‚Äúin reward‚Äù, you must consider not only value bias, but whether <strong>the induced policy gradient is correct</strong>.</p>

<h3 id="the-true-kl-regularized-policy-gradient">The True KL-Regularized Policy Gradient</h3>

<p>Consider the KL-regularized reinforcement learning objective:</p>

<p>$$
J(\theta) = \mathbb{E}_{q_\theta}[R] - \beta \cdot D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p>Its true gradient is:</p>

<p>$$
\nabla_\theta J = \mathbb{E}_{q_\theta}[s_\theta \cdot R] - \beta \cdot \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p>Using the result from the ‚ÄúPreliminaries‚Äù section, the reverse KL gradient is:</p>

<p>$$
\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(-\log \frac{p}{q_\theta}\right)\right] = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1]
$$</p>

<p>Therefore, the true KL-regularized policy gradient is:</p>

<p>$$
\nabla_\theta J = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(R - \beta \cdot k_1\right)\right]
$$</p>

<h4 id="gradient-form-when-using-estimator-inlmath392mathend">Gradient Form When Using Estimator $\hat{k}$</h4>

<p>When we use some estimator $\hat{k}$ (with stop-gradient) as a reward penalty, the shaped reward is $\tilde{R} = R - \beta \cdot \text{sg}(\hat{k})$, and the policy gradient becomes:</p>

<p>$$
\nabla_\theta \tilde{J} = \mathbb{E}_{q_\theta}\left[s_\theta \cdot (R - \beta \cdot \hat{k})\right]
$$</p>

<p><strong>Unbiasedness condition</strong>: $\nabla_\theta \tilde{J} = \nabla_\theta J$ if and only if</p>

<p>$$
\mathbb{E}_{q_\theta}[s_\theta \cdot \hat{k}] = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1]
$$</p>

<h4 id="using-inlmath396mathend-as-penalty-gradient-unbiased">Using $k_1$ as Penalty: Gradient Unbiased</h4>

<p>When $\hat{k} = k_1$, the condition is automatically satisfied:</p>

<p>$$
\mathbb{E}_{q_\theta}[s_\theta \cdot k_1] = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1] \quad \checkmark
$$</p>

<p>Therefore, <strong>when $k_1$ is used as a reward penalty, the induced policy gradient is unbiased</strong>.</p>

<h4 id="using-inlmath399mathend-as-penalty-gradient-biased">Using $k_3$ as Penalty: Gradient Biased</h4>

<p>When $\hat{k} = k_3 = \frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta}$:</p>

<p>$$
\mathbb{E}_{q_\theta}[s_\theta \cdot k_3] = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(\frac{p}{q_\theta} - 1\right)\right] + \mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(-\log \frac{p}{q_\theta}\right)\right]
$$</p>

<p>The second term is exactly $\mathbb{E}_{q_\theta}[s_\theta \cdot k_1]$. The problem lies in the first term:</p>

<p>$$
\mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(\frac{p}{q_\theta} - 1\right)\right] = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \frac{p}{q_\theta}\right] - \underbrace{\mathbb{E}_{q_\theta}[s_\theta]}_{=0} = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \frac{p}{q_\theta}\right]
$$</p>

<p>This can be rewritten as:</p>

<p>$$
\mathbb{E}_{q_\theta}\left[s_\theta \cdot \frac{p}{q_\theta}\right] = \int q_\theta(x) \cdot \nabla_\theta \log q_\theta(x) \cdot \frac{p(x)}{q_\theta(x)} dx = \int p(x) \cdot \nabla_\theta \log q_\theta(x) dx = \mathbb{E}_p[s_\theta]
$$</p>

<p>Using the forward KL gradient formula $\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = -\mathbb{E}_p[s_\theta]$, we have:</p>

<p>$$
\mathbb{E}_{q_\theta}\left[s_\theta \cdot \frac{p}{q_\theta}\right] = -\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta)
$$</p>

<p>Therefore:</p>

<p>$$
\mathbb{E}_{q_\theta}[s_\theta \cdot k_3] = \underbrace{-\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta)}_{\text{bias term}} + \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p><strong>When $k_3$ is used as a reward penalty, the gradient is biased</strong>, with the bias term equal to the negative of the forward KL gradient.</p>

<p><strong>Interpretation of the bias</strong>: Using $k_3$ as a reward penalty is equivalent to optimizing a mixed objective that you likely do not intend:</p>

<ul>
  <li>Penalizing reverse KL (hoping policy doesn‚Äôt deviate from reference)</li>
  <li>But also <strong>wrongly encouraging forward KL to increase</strong> (hoping reference doesn‚Äôt cover policy)</li>
</ul>

<p>These two directions can conflict and destabilize optimization.</p>

<p><strong>Empirical evidence</strong>: Shah et al. (2025) report that in on-policy RL fine-tuning of LLMs:</p>

<ul>
  <li><strong>$k_1$ in reward</strong>: Training is stable</li>
  <li><strong>$k_3$ in reward</strong>: <strong>Training collapses</strong></li>
</ul>

<p>This is consistent with the theoretical analysis above.</p>

<h4 id="off-policy-scenario-conclusions">Off-Policy Scenario Conclusions</h4>

<p>The above analysis assumes on-policy sampling. Does the conclusion change in off-policy scenarios?</p>

<p>Let samples come from behavior policy $\mu$, using importance-weighted policy gradient:</p>

<p>$$
\nabla_\theta \tilde{J} = \mathbb{E}_\mu\left[\frac{q_\theta}{\mu} \cdot s_\theta \cdot (R - \beta \cdot k)\right]
$$</p>

<p>Using $\mathbb{E}_\mu[\frac{q_\theta}{\mu} \cdot f] = \mathbb{E}_{q_\theta}[f]$, this equals:</p>

<p>$$
= \mathbb{E}_{q_\theta}[s_\theta \cdot R] - \beta \cdot \mathbb{E}_{q_\theta}[s_\theta \cdot k]
$$</p>

<p><strong>The unbiasedness condition</strong> remains $\mathbb{E}_{q_\theta}[s_\theta \cdot k] = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1]$, exactly the same as on-policy.</p>

<p><strong>Key insight</strong>: In an off-policy policy-gradient estimator, the importance weight $\frac{q_\theta}{\mu}$ multiplies the entire policy-gradient term. There is <strong>no need to additionally importance-weight the KL scalar inside the shaped reward</strong>. Therefore:</p>

<ul>
  <li>Shaped reward keeps its original form: $\tilde{R} = R - \beta \cdot k_1$ (not $R - \beta \cdot \frac{q_\theta}{\mu} k_1$)</li>
  <li>Under the <strong>stop-gradient reward shaping</strong> ($\tilde{R}=R-\beta\,\text{sg}(k)$) with the <strong>reverse-KL regularization</strong> setting discussed in this post, the conclusion is the same as in the on-policy case: <strong>use $k_1$, not $k_3$</strong>.</li>
</ul>

<h3 id="key-finding-only-inlmath416mathend-is-suitable-as-a-reward-penalty">Key Finding: Only $k_1$ Is Suitable as a Reward Penalty</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Value unbiased?</th>
      <th style="text-align: center">Gradient unbiased when used as reward penalty?</th>
      <th style="text-align: center">Actual performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">‚úì</td>
      <td style="text-align: center">‚úì</td>
      <td style="text-align: center">Stable</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">‚úì</td>
      <td style="text-align: center">‚úó</td>
      <td style="text-align: center">Collapses</td>
    </tr>
  </tbody>
</table>

<p><strong>Core lesson</strong>: ‚ÄúValue unbiasedness‚Äù and ‚Äúgradient correctness‚Äù are independent axes. For the reward-penalty setup discussed here (stop-gradient reward shaping with reverse-KL regularization, on-policy or off-policy), <strong>only $k_1$ yields the correct induced policy gradient</strong>. Even though $k_3$ is value-unbiased and often lower variance, using it as a reward penalty introduces a biased update and may trigger collapse.</p>

<p>At this point, an apparent tension may arise:</p>

<ul>
  <li>In <strong>reward penalty</strong> we emphasize ‚Äúonly use $k_1$‚Äù;</li>
  <li>But in the earlier <strong>loss-term backpropagation</strong> discussion (especially off-policy), we recommend using $\rho k_3$ or $\text{sg}(\rho)k_2$ for lower-variance reverse-KL gradients.</li>
</ul>

<p>The next section explains why these are not contradictory: for the KL regularization term‚Äôs contribution to the policy-gradient update, the two implementations can be <strong>sample-wise equivalent</strong>. The practical differences arise mainly from whether the KL term enters the advantage/baseline and from the resulting credit-assignment pathway.</p>

<h2 id="inlmath424mathend-in-reward-vs-low-variance-kl-in-loss-equivalence-and-differences">$k_1$ in Reward vs. Low-Variance KL in Loss: Equivalence and Differences</h2>

<p>Having separately analyzed KL as Loss and as Reward, a natural question arises: <strong>In what sense are these two approaches equivalent, and how do they differ?</strong> This section explores this question in depth, with particular focus on LLM RL practice.</p>

<h3 id="sample-level-equivalence-of-the-kl-gradient-term">Sample-Level Equivalence of the KL Gradient Term</h3>

<p>This section compares only the <strong>policy-gradient contribution from KL regularization</strong>, written as the ascent direction $\nabla_\theta J$ (minimizing a loss is just a global sign flip). We also keep the unified notation: samples come from $x \sim \mu$, and the importance weight $\rho = \frac{q_\theta}{\text{sg}(\mu)}$ multiplies the policy-gradient estimator.</p>

<p>Recall the key conclusions from earlier:</p>

<p><strong>KL as Loss (low-variance choice)</strong>: We proved earlier that when using $\text{sg}(\rho) k_2$ or $\rho k_3$ as the regularization term, the gradient random variable simplifies to</p>

<p>$$
\nabla_\theta(\text{sg}(\rho) k_2) = \nabla_\theta(\rho k_3) = \rho s_\theta k_1
$$</p>

<p><strong>KL as Reward ($k_1$ in reward)</strong>: The shaped reward is $\tilde{R} = R - \beta \cdot k_1$ (applying stop-gradient to $k_1$ just avoids ‚ÄúKL directly backpropagating‚Äù in implementation, without changing its numerical value as a penalty). In the ‚Äúpolicy gradient term‚Äù, the KL penalty contributes</p>

<p>$$
\mathbb{E}_\mu[\rho s_\theta \cdot (-\beta k_1)] = -\beta \cdot \mathbb{E}_\mu[\rho s_\theta k_1]
$$</p>

<p><strong>Key finding</strong>: The KL gradient terms from both approaches are <strong>identical at the sample level</strong>.</p>

<p>In other words, ignoring the specific construction details of baseline/advantage:</p>

<ul>
  <li>‚ÄúWriting KL into loss with low-variance implementation ($\text{sg}(\rho)k_2$ or $\rho k_3$)‚Äù</li>
  <li>and ‚ÄúWriting KL into reward with $k_1$ (stop-gradient shaped reward)‚Äù</li>
</ul>

<p>can exert exactly the same KL regularization ‚Äúforce‚Äù on policy updates.</p>

<p>Specifically, if we only look at the gradient term contributed by KL penalty when ‚Äúmaximizing $J$‚Äù (the penalty term carries a negative sign in $J$, so the ascent direction naturally carries $-\beta$):</p>

<ul>
  <li><strong>KL in Loss (low-variance implementation)</strong>: $-\beta \cdot \rho s_\theta k_1$</li>
  <li><strong>KL in Reward ($k_1$ in reward)</strong>: $\rho s_\theta \cdot (-\beta k_1) = -\beta \cdot \rho s_\theta k_1$</li>
</ul>

<p>They are <strong>the same random variable</strong>, not just equal in expectation.</p>

<h4 id="where-the-two-implementations-still-differ">Where the Two Implementations Still Differ</h4>

<p>Although the KL gradient terms are sample-level equivalent, <strong>the overall update semantics of the two approaches still differ</strong>. The differences mainly manifest in the following aspects:</p>

<h4 id="1-whether-kl-enters-advantagebaseline">1. Whether KL Enters Advantage/Baseline</h4>

<p><strong>KL as Loss</strong> (equivalent to maximizing $J(\theta) = \mathbb{E}[R] - \beta\,\mathrm{KL}$, but implementing the KL term as an independent, controllable ‚Äúexplicit force‚Äù):</p>

<p>$$
\nabla_\theta J_{\text{loss-impl}} = \underbrace{\mathbb{E}_\mu[\rho s_\theta A_{\text{env}}]}_{\text{RL ascent direction}} + \underbrace{(-\beta) \cdot \mathbb{E}_\mu[\rho s_\theta k_1]}_{\text{independent KL penalty ascent direction}}
$$</p>

<p>KL is an <strong>independent regularization term</strong>, completely decoupled from advantage. The magnitude of the KL gradient depends only on $k_1$ itself, unaffected by critic quality or baseline choice.</p>

<p><strong>KL as Reward</strong>:</p>

<p>$$
\nabla_\theta J_{\text{reward-impl}} = \mathbb{E}_\mu[\rho s_\theta \tilde{A}], \quad \tilde{A} \text{ based on } (R - \beta \cdot k_1)
$$</p>

<p>KL enters advantage computation through shaped reward and gets processed by the baseline. This means:</p>

<ul>
  <li>KL‚Äôs influence is modulated by how advantage is constructed</li>
  <li>If using a value function baseline, KL‚Äôs influence is partially absorbed</li>
</ul>

<p>From an implementation perspective, the difference can be understood as: the Loss approach estimates ‚Äúenvironment return‚Äù and ‚ÄúKL regularization‚Äù separately; the Reward approach treats KL as part of the return, so it follows all the processing you do to returns (baseline, normalization, clipping, etc.).</p>

<h4 id="2-credit-assignment-explicit-regularization-vs-shaped-reward-coupling">2. Credit Assignment: Explicit Regularization vs. Shaped-Reward Coupling</h4>

<p><strong>KL as Loss</strong>: Each token/state‚Äôs KL gradient is local, directly affecting the update at that position.</p>

<p><strong>KL as Reward</strong>: The KL penalty is folded into the return/advantage computation and can influence earlier decisions depending on how returns are propagated.</p>

<h4 id="3-reward-centered-kl-impact-on-gradient-unbiasedness">3. Reward-Centered KL: Impact on Gradient Unbiasedness</h4>

<p>In LLM RL (such as GRPO, PPO for LLM), a common advantage computation is $A = r - \text{mean}(r)$. When KL is used as a reward penalty, whether to include KL in the mean affects gradient unbiasedness.</p>

<p>Let samples be $x_1, \dots, x_n \overset{iid}{\sim} q_\theta$, denote $g_i = \nabla_\theta \log q_\theta(x_i)$, and use $\mathrm{kl}_i$ for the KL penalty scalar of the $i$-th sample, $\bar{\mathrm{kl}} = \frac{1}{n}\sum_j \mathrm{kl}_j$.</p>

<p><strong>No centering ($-\beta\,\mathrm{kl}_i$)</strong>: The expected KL gradient term is</p>

<p>$$
-\beta \mathbb{E}[g_i\,\mathrm{kl}_i] = -\beta \nabla_\theta \mathbb{E}[\mathrm{KL}]
$$</p>

<p>This is an <strong>unbiased gradient</strong> of $-\beta \mathbb{E}[\mathrm{KL}]$.</p>

<p><strong>Same-batch mean centering ($-\beta(\mathrm{kl}_i - \bar{\mathrm{kl}})$, including self)</strong>: Since $\bar{\mathrm{kl}}$ depends on all samples (including $x_i$ itself), the expected gradient becomes</p>

<p>$$
-\beta \left(1 - \frac{1}{n}\right) \nabla_\theta \mathbb{E}[\mathrm{KL}]
$$</p>

<p>The KL regularization gradient is <strong>shrunk</strong> by $\frac{1}{n}$, equivalent to a smaller effective $\beta$. This is not strictly unbiased.</p>

<p><strong>Leave-one-out centering ($-\beta(\mathrm{kl}_i - \bar{\mathrm{kl}}_{-i})$)</strong>: If we use $\bar{\mathrm{kl}}_{-i} = \frac{1}{n-1}\sum_{j \neq i} \mathrm{kl}_j$ instead, then $\bar{\mathrm{kl}}_{-i}$ is independent of $g_i$, giving $\mathbb{E}[g_i \bar{\mathrm{kl}}_{-i}] = 0$, therefore</p>

<p>$$
-\beta \mathbb{E}[g_i (\mathrm{kl}_i - \bar{\mathrm{kl}}_{-i})] = -\beta \nabla_\theta \mathbb{E}[\mathrm{KL}]
$$</p>

<p>This remains an <strong>unbiased gradient</strong>, while enjoying variance reduction from centering.</p>

<p><strong>Conclusion</strong>: Same-batch mean centering induces an $O(1/n)$ shrinkage of the KL gradient term (equivalently, a slight reduction in the effective $\beta$). This is typically negligible for large group sizes (e.g., GRPO); for strict unbiasedness while retaining variance reduction, use a leave-one-out mean.</p>

<h3 id="when-to-choose-which-approach">When to Choose Which Approach?</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Dimension</th>
      <th style="text-align: center">KL as Loss</th>
      <th style="text-align: center">KL as Reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">KL gradient form</td>
      <td style="text-align: center">$\rho s_\theta k_1$ (low-var choice)</td>
      <td style="text-align: center">$\rho s_\theta k_1$</td>
    </tr>
    <tr>
      <td style="text-align: center">Coupling w/ Advantage</td>
      <td style="text-align: center">Fully decoupled</td>
      <td style="text-align: center">Coupled through shaped reward</td>
    </tr>
    <tr>
      <td style="text-align: center">KL centering</td>
      <td style="text-align: center">None (absolute penalty)</td>
      <td style="text-align: center">Yes ($\text{KL} - \text{mean}(\text{KL})$)</td>
    </tr>
    <tr>
      <td style="text-align: center">Credit assignment</td>
      <td style="text-align: center">Local, per-token</td>
      <td style="text-align: center">May have temporal backprop (impl-dependent)</td>
    </tr>
    <tr>
      <td style="text-align: center">Suitable for</td>
      <td style="text-align: center">More controllable KL, less critic-dependent</td>
      <td style="text-align: center">More global KL constraint with planning capability</td>
    </tr>
  </tbody>
</table>

<p><strong>Practical recommendations</strong>:</p>

<ol>
  <li>
    <p><strong>If you want KL constraint to be ‚Äúcorrective‚Äù</strong> ‚Äî allowing the agent to explore but locally correcting its behavior, while keeping KL pressure more controllable and less dependent on critic quality ‚Äî choose <strong>KL as Loss</strong>, using $\text{sg}(\rho) k_2$ or $\rho k_3$. For on-policy scenarios, if you prefer not to explicitly construct $\rho = \frac{q_\theta}{\text{sg}(q_\theta)}$, directly using $k_2$ is simpler and less error-prone.</p>
  </li>
  <li>
    <p><strong>If you want KL constraint to be ‚Äúpreventive‚Äù</strong> ‚Äî having the agent avoid high-KL regions from the outset, accepting that KL is modulated by the baseline ‚Äî choose <strong>KL as Reward</strong>, using $k_1$.</p>
  </li>
</ol>

<p>Based on the above conclusions about ‚Äúvalue unbiasedness vs. gradient correctness‚Äù and ‚Äúdifferences between Loss and Reward implementations‚Äù, we now proceed to the quick reference guide and common pitfalls that can be directly applied to code.</p>

<h2 id="practical-guide-and-common-pitfalls">Practical Guide and Common Pitfalls</h2>

<h3 id="quick-reference-for-the-three-estimator-definitions">Quick Reference for the Three Estimator Definitions</h3>

<p>$$
k_1 = \log \frac{q_\theta}{p}, \quad k_2 = \frac{1}{2}\left(\log \frac{p}{q_\theta}\right)^2, \quad k_3 = \frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta}
$$</p>

<h3 id="value-estimation-properties">Value Estimation Properties</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Unbiased for reverse KL $D_{\mathrm{KL}}(q_\theta \| p)$ value?</th>
      <th style="text-align: center">Variance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">‚úì</td>
      <td style="text-align: center">High (can be negative)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">‚úó (but bias is minimal)</td>
      <td style="text-align: center">Low</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">‚úì</td>
      <td style="text-align: center">Low</td>
    </tr>
  </tbody>
</table>

<h3 id="quick-reference-tables">Quick Reference Tables</h3>

<h4 id="on-policy-optimization-of-reverse-kl-loss">On-Policy Optimization of Reverse KL (Loss)</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Loss Form</th>
      <th style="text-align: center">Pros</th>
      <th style="text-align: center">Cons</th>
      <th style="text-align: center">Rec.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">‚Äî</td>
      <td style="text-align: center">Gradient expectation is zero, <strong>completely ineffective</strong></td>
      <td style="text-align: center">‚úó‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">Correct gradient (reverse KL), low variance, <strong>simplest implementation</strong></td>
      <td style="text-align: center">Value biased (but bias is minimal)</td>
      <td style="text-align: center">‚úì‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">‚Äî</td>
      <td style="text-align: center">Gradient corresponds to <strong>forward KL</strong>, wrong direction</td>
      <td style="text-align: center">‚úó‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\frac{q_\theta}{\text{sg}(q_\theta)} k_3$</td>
      <td style="text-align: center">Correct gradient (reverse KL), low variance, value unbiased</td>
      <td style="text-align: center">Requires explicit $\rho$ construction, slightly complex</td>
      <td style="text-align: center">‚úì</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Note</strong>: $k_2$ and $\frac{q_\theta}{\text{sg}(q_\theta)} k_3$ have identical gradients (sample-level equivalent). For on-policy, directly using $k_2$ is recommended as the simplest approach.</p>
</blockquote>

<h4 id="off-policy-optimization-of-reverse-kl-loss">Off-Policy Optimization of Reverse KL (Loss)</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Loss Form</th>
      <th style="text-align: center">Pros</th>
      <th style="text-align: center">Cons</th>
      <th style="text-align: center">Rec.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$\frac{q_\theta}{\mu} k_1$</td>
      <td style="text-align: center">Correct gradient (reverse KL), value unbiased</td>
      <td style="text-align: center"><strong>Higher variance</strong></td>
      <td style="text-align: center">‚ñ≥</td>
    </tr>
    <tr>
      <td style="text-align: center">$\frac{q_\theta}{\mu} k_2$</td>
      <td style="text-align: center">‚Äî</td>
      <td style="text-align: center">Gradient corresponds to <strong>f-divergence</strong> (not reverse KL)</td>
      <td style="text-align: center">‚úó‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$</td>
      <td style="text-align: center">Correct gradient (reverse KL), <strong>low variance</strong></td>
      <td style="text-align: center">Value biased (but bias is minimal)</td>
      <td style="text-align: center">‚úì‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$\frac{q_\theta}{\mu} k_3$</td>
      <td style="text-align: center">Correct gradient (reverse KL), <strong>low variance</strong>, value unbiased</td>
      <td style="text-align: center">‚Äî</td>
      <td style="text-align: center">‚úì‚úì</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Note</strong>: $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$ and $\frac{q_\theta}{\mu} k_3$ have identical gradients (sample-level equivalent). Both are recommended choices.</p>
</blockquote>

<h4 id="kl-as-reward-penalty-stop-gradient-shaped-reward">KL as Reward Penalty (stop-gradient shaped reward)</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Pros</th>
      <th style="text-align: center">Cons</th>
      <th style="text-align: center">Rec.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">Value unbiased, <strong>induced policy gradient unbiased</strong></td>
      <td style="text-align: center">Higher variance</td>
      <td style="text-align: center">‚úì‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">Value biased</td>
      <td style="text-align: center">Induced policy gradient biased</td>
      <td style="text-align: center">‚úó‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">Value unbiased, low variance</td>
      <td style="text-align: center"><strong>Induced policy gradient biased</strong>, bias term is $-\nabla D_{\mathrm{KL}}(p\|q)$, may cause <strong>training collapse</strong></td>
      <td style="text-align: center">‚úó‚úó</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Note</strong>: For reward penalty scenarios, <strong>only $k_1$ is the correct choice</strong>. Although $k_3$ is value-unbiased with lower variance, it causes biased policy gradients, with training collapse observed in experiments.</p>
</blockquote>

<h4 id="legend">Legend</h4>

<ul>
  <li>‚úì‚úì: <strong>Strongly recommended</strong>, theoretically correct with good practical performance</li>
  <li>‚úì: Recommended, theoretically correct but slightly more complex or has minor drawbacks</li>
  <li>‚ñ≥: Usable but with caution, has issues like high variance</li>
  <li>‚úó‚úó: <strong>Do not use</strong>, theoretically incorrect or causes training failure</li>
</ul>

<h3 id="common-pitfalls">Common Pitfalls</h3>

<ol>
  <li><strong>Using $k_1 = \log \frac{q_\theta}{p}$ directly as Loss (on-policy)</strong>: Gradient expectation is zero, completely ineffective.</li>
  <li><strong>Using $k_3 = \frac{p}{q_\theta} - 1 - \log \frac{p}{q_\theta}$ as Loss to optimize reverse KL (on-policy)</strong>: Its gradient corresponds to forward KL $D_{\mathrm{KL}}(p \| q_\theta)$, wrong direction.</li>
  <li><strong>Using $\frac{q_\theta}{\mu} k_2$ (importance weight not detached) off-policy</strong>: Gradient corresponds to f-divergence, not reverse KL.</li>
  <li><strong>Using $k_3$ in reward penalty</strong>: Although value-unbiased, it induces a biased policy-gradient update and may lead to training collapse.</li>
  <li><strong>Simply setting $\rho$ to constant 1 in on-policy</strong>: Must explicitly construct $\rho = \frac{q_\theta}{\text{sg}(q_\theta)}$ (or equivalently $\exp(\log q_\theta - \text{sg}(\log q_\theta))$), otherwise the score-function gradient path is lost, causing $\rho k_1$ and $\rho k_3$ to degenerate to naive forms and fail.</li>
  <li><strong>Confusing ‚Äúvalue unbiasedness‚Äù with ‚Äúgradient correctness‚Äù</strong>: $k_3$ is value-unbiased for reverse KL, but when used as a reward penalty, the induced policy gradient is biased; both dimensions must be considered when choosing an estimator.</li>
</ol>

<h2 id="summary">Summary</h2>

<p>This post systematically analyzes the three KL estimators $k_1, k_2, k_3$ around three core questions: <strong>who to sample from</strong>, <strong>how to use it</strong>, and <strong>what to estimate</strong>.</p>

<blockquote>
  <p><strong>Core takeaway</strong>: <strong>Value unbiasedness ‚â† Gradient correctness</strong>. When choosing an estimator, you must consider both ‚Äúwhose value is being estimated‚Äù and ‚Äúwhich optimization objective the gradient corresponds to‚Äù.</p>
</blockquote>

<p><strong>Key content</strong>:</p>

<ol>
  <li><strong>Value estimation</strong>: $k_1$ and $k_3$ are unbiased for reverse KL value, and $k_3$ also has low variance.</li>
  <li><strong>Gradients when used as Loss</strong>: Use $k_2$ or $\frac{q_\theta}{\text{sg}(q_\theta)} k_3$ for on-policy; use $\frac{q_\theta}{\mu} k_3$ or $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$ for off-policy.</li>
  <li><strong>As a reward penalty</strong>: Only use $k_1$, because $k_3$ causes biased policy gradients.</li>
  <li><strong>Relationship between Loss and Reward implementations</strong>:
    <ul>
      <li><strong>Sample-level equivalence</strong>: When Loss uses low-variance implementation ($\text{sg}(\rho) k_2$ or $\rho k_3$) and Reward uses $k_1$, their KL gradient terms are <strong>the same random variable</strong> $\rho s_\theta k_1$ ‚Äî not only equal in expectation, but also identical in variance.</li>
      <li><strong>Overall semantic differences</strong>: In the Loss approach, KL is an independent regularization term, completely decoupled from advantage, unaffected by critic quality; in the Reward approach, KL enters advantage computation through shaped reward and is processed and modulated by the baseline.</li>
      <li><strong>Credit assignment differences</strong>: The Loss approach‚Äôs KL gradient is local (per-token); the Reward approach‚Äôs KL penalty may propagate through returns to affect earlier decisions.</li>
    </ul>
  </li>
  <li><strong>Unified $\rho$ framework</strong>: This post introduces $\rho = \frac{q_\theta}{\text{sg}(\mu)}$ to treat on-policy and off-policy settings within a single framework. The key insight is that routing the sampling distribution‚Äôs $\theta$-dependence through the $\rho$ gradient path makes expect-then-differentiate and differentiate-then-expect coincide under $\mathbb{E}_\mu[\cdot]$. In on-policy settings, $\rho \equiv 1$ but $\nabla_\theta \rho = s_\theta \neq 0$, which explains why directly backpropagating through $k_1$ or $k_3$ fails, while $k_2$ works as a special case.</li>
</ol>

<h2 id="references">References</h2>

<ol>
  <li>
    <p>Dibya Ghosh. ‚ÄúKL Divergence for Machine Learning‚Äù. <a href="https://dibyaghosh.com/blog/probability/kldivergence">https://dibyaghosh.com/blog/probability/kldivergence</a></p>
  </li>
  <li>
    <p>John Schulman. ‚ÄúApproximating KL Divergence‚Äù. <a href="https://joschu.net/blog/kl-approx.html">https://joschu.net/blog/kl-approx.html</a></p>
  </li>
  <li>
    <p>Verl Documentation. ‚ÄúProximal Policy Optimization (PPO)‚Äù. <a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">https://verl.readthedocs.io/en/latest/algo/ppo.html</a></p>
  </li>
  <li>
    <p>Âàù‰∏É123334. RLHF/RLVR ËÆ≠ÁªÉ‰∏≠ÁöÑ KL Ëøë‰ººÊñπÊ≥ïÊµÖÊûêÔºàk1 / k2 / k3Ôºâ. <a href="https://zhuanlan.zhihu.com/p/1966872846212010437">https://zhuanlan.zhihu.com/p/1966872846212010437</a></p>
  </li>
  <li>
    <p>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. ‚ÄúRethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization‚Äù. <a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></p>
  </li>
  <li>
    <p>Yifan Zhang, Yiping Ji, Gavin Brown, et al. ‚ÄúOn the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning‚Äù. <a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></p>
  </li>
  <li>
    <p>Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro. ‚ÄúA Comedy of Estimators: On KL Regularization in RL Training of LLMs‚Äù. <a href="https://arxiv.org/abs/2512.21852">https://arxiv.org/abs/2512.21852</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>

      </article>

      

      
        
      </div>

    <div class="col-lg-3 d-none d-lg-block toc-sidebar-col">
      <aside class="toc-sidebar" data-toc-sidebar id="toc-sidebar">
  <div class="toc-sidebar__header">
    <div class="toc-sidebar__title">Contents</div>
    <button class="toc-toggle-btn" aria-label="Toggle table of contents" aria-expanded="false" aria-controls="toc-content" data-toc-toggle>
      <i class="fas fa-chevron-right"></i>
    </button>
  </div>
  <nav
    id="toc-content"
    class="toc js-page-toc"
    data-toc
    data-toc-content=".toc-content"
    data-toc-headings="h2,h3"
    data-toc-min-items="2"
    aria-label="Contents"
  ></nav>
</aside>

<!-- Collapsed TOC toggle button (shown when TOC is collapsed) -->
<button class="toc-collapsed-toggle" aria-label="Show table of contents" aria-expanded="false" aria-controls="toc-content" data-toc-expand>
  <i class="fas fa-list"></i>
</button>


    </div>
  </div>
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2026 Xihuai Leo Wang. Last updated: February 10, 2026.
      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/blog_enhancements.js" type="text/javascript"></script>
  <script defer src="/assets/js/sidenotes.js" type="text/javascript"></script>
  <script defer src="/assets/js/footnote_preview.js" type="text/javascript"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>
  <script defer src="/assets/js/toc.js" type="text/javascript"></script>
  <script defer src="/assets/js/venue_filter.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax 3.x with comprehensive configuration -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        // Support all common math delimiters
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        processRefs: true,
        // Common macros for convenience
        macros: {
          RR: '\\mathbb{R}',
          NN: '\\mathbb{N}',
          ZZ: '\\mathbb{Z}',
          CC: '\\mathbb{C}',
          EE: '\\mathbb{E}',
          PP: '\\mathbb{P}',
          bm: ['\\boldsymbol{#1}', 1],
          argmax: '\\operatorname*{arg\\,max}',
          argmin: '\\operatorname*{arg\\,min}',
          sgn: '\\operatorname{sgn}',
          KL: '\\mathrm{KL}',
          Var: '\\operatorname{Var}',
          Cov: '\\operatorname{Cov}',
          tr: '\\operatorname{tr}',
          diag: '\\operatorname{diag}'
        },
        // AMS packages
        packages: {'[+]': ['ams', 'boldsymbol', 'newcommand']}
      },
      loader: {
        load: ['[tex]/ams', '[tex]/boldsymbol', '[tex]/newcommand']
      },
      options: {
        // Skip math rendering in these HTML elements
        skipHtmlTags: [
          'script', 'noscript', 'style', 'textarea', 'pre', 'code',
          'annotation', 'annotation-xml', 'kbd', 'samp', 'var'
        ],
        // Fix issues with underscores being converted to <em> by HTML
        processHtmlClass: 'mathjax-process',
        ignoreHtmlClass: 'tex2jax_ignore|no-mathjax',
        // Render math even with HTML entities
        renderActions: {
          findScript: [10, function (doc) {
            // Pre-process to fix HTML entity issues in math
            for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            }
          }, '']
        }
      },
      svg: {
        fontCache: 'global',
        scale: 1.0
      },
      chtml: {
        scale: 1.0,
        matchFontHeight: true
      },
      startup: {
        ready: function () {
          MathJax.startup.defaultReady();
          // Fix: restore underscores that might have been converted to <em>
          MathJax.startup.promise.then(() => {
            console.log('MathJax typesetting complete');
          });
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  
  <!-- Pre-processing script to protect math from Markdown/HTML interference -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Fix underscores in math that were converted to <em> by Markdown
      function fixMathUnderscores() {
        const mathContainers = document.querySelectorAll('.MathJax, .MathJax_Display, mjx-container');
        // This runs before MathJax, so we need to fix raw content
        const content = document.querySelector('.post-content, article');
        if (!content) return;
        
        // Find math delimiters and restore any <em> or <strong> inside them
        const html = content.innerHTML;
        
        // Pattern to find math blocks and restore underscore formatting
        // This is a fallback; the main protection is in the Jekyll plugin
      }
      
      // Fix HTML entities in display math blocks
      function fixHtmlEntities() {
        document.querySelectorAll('.language-plaintext.highlighter-rouge').forEach(el => {
          // Check if this looks like an HTML figure that wasn't rendered
          const text = el.textContent;
          if (text.includes('<img') || text.includes('<figure') || text.includes('<figcaption')) {
            // This is raw HTML that should be rendered - replace with actual HTML
            const temp = document.createElement('div');
            temp.innerHTML = text;
            el.replaceWith(...temp.childNodes);
          }
        });
      }
      
      fixHtmlEntities();
    });
  </script>

    <!-- Pseudocode -->
  <script defer src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js" integrity="sha256-aVkDxqyzrB+ExUsOY9PdyelkDhn/DfrjWu08aVpqNlo=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/pseudocode-init.js" type="text/javascript"></script>
    <!-- Mermaid -->
  <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.9.3/dist/mermaid.min.js"></script>
  <script defer src="/assets/js/mermaid-init.js" type="text/javascript"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2923RQZBXG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-2923RQZBXG');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    // Use the CSS-defined padding-top value to avoid layout shift
    // The navbar height is already handled by CSS: body.fixed-top-nav { padding-top: 50px; }
    let navbarHeight = $("#navbar").outerHeight(true) || 50;
    // Only set progressBar position, don't override body padding to avoid layout shift
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
