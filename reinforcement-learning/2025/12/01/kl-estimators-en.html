<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation | Xihuai Wang's Page</title>
    <meta name="author" content="Xihuai Leo Wang" />
    <meta name="description" content="In reinforcement learning, how we approximate KL divergence directly affects training stability. This post systematically dissects three classic estimators k1, k2, and k3, covering both on-policy and off-policy scenarios, and gives practical guidelines for choosing them for reward penalties vs. gradient-based losses." />
    <meta name="keywords" content="Reinforcement Learning, Multi-agent System" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">Xihuai Wang's Page</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about</a>
          </li>
          

          <!-- CV -->
          <!-- 
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/" target="_blank"
              rel="noopener noreferrer">cv</a>
          </li> -->
          <!-- Other pages -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">Xihuai's Blog</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">publications</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/cv/">cv</a>
          </li>

          <!-- Toogle theme mode -->
          <li class="toggle-container">
            <button id="light-toggle" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation</h1>
    <p class="post-meta">December 1, 2025</p>
    <p class="post-tags">
  <a href="/blog/2025"> üìÖ 2025 </a>
      &nbsp; &middot; &nbsp;
        <a href="/blog/category/reinforcement-learning">
          üè∑Ô∏è reinforcement-learning</a> &nbsp;
          

    </p>
  </header>

  <article class="post-content">
    <ul id="markdown-toc">
  <li><a href="#introduction-the-role-of-kl-divergence-in-reinforcement-learning" id="markdown-toc-introduction-the-role-of-kl-divergence-in-reinforcement-learning">Introduction: The Role of KL Divergence in Reinforcement Learning</a>    <ul>
      <li><a href="#forward-vs-reverse-kl" id="markdown-toc-forward-vs-reverse-kl">Forward vs. Reverse KL</a></li>
    </ul>
  </li>
  <li><a href="#three-estimators-definitions-and-design-principles" id="markdown-toc-three-estimators-definitions-and-design-principles">Three Estimators: Definitions and Design Principles</a>    <ul>
      <li><a href="#k_1-the-most-naive-estimator" id="markdown-toc-k_1-the-most-naive-estimator">$k_1$: The Most Naive Estimator</a></li>
      <li><a href="#k_2-a-low-variance-estimator-from-f-divergences" id="markdown-toc-k_2-a-low-variance-estimator-from-f-divergences">$k_2$: A Low-Variance Estimator from f-Divergences</a></li>
      <li><a href="#k_3-a-best-of-both-worlds-estimator-via-control-variates" id="markdown-toc-k_3-a-best-of-both-worlds-estimator-via-control-variates">$k_3$: A ‚ÄúBest of Both Worlds‚Äù Estimator via Control Variates</a></li>
      <li><a href="#summary-comparing-the-three-estimators" id="markdown-toc-summary-comparing-the-three-estimators">Summary: Comparing the Three Estimators</a></li>
    </ul>
  </li>
  <li><a href="#core-analysis" id="markdown-toc-core-analysis">Core Analysis</a>    <ul>
      <li><a href="#bias-and-variance-for-estimating-the-kl-value" id="markdown-toc-bias-and-variance-for-estimating-the-kl-value">Bias and Variance for Estimating the KL Value</a></li>
      <li><a href="#the-crucial-distinction-when-estimating-kl-gradients" id="markdown-toc-the-crucial-distinction-when-estimating-kl-gradients">The Crucial Distinction When Estimating KL Gradients</a>        <ul>
          <li><a href="#true-gradients-of-forward-and-reverse-kl" id="markdown-toc-true-gradients-of-forward-and-reverse-kl">True Gradients of Forward and Reverse KL</a></li>
          <li><a href="#two-orders-of-operations-for-gradients" id="markdown-toc-two-orders-of-operations-for-gradients">Two Orders of Operations for Gradients</a></li>
          <li><a href="#gradients-of-the-three-estimators" id="markdown-toc-gradients-of-the-three-estimators">Gradients of the Three Estimators</a></li>
          <li><a href="#expectation-then-gradient-vs-gradient-then-expectation" id="markdown-toc-expectation-then-gradient-vs-gradient-then-expectation">‚ÄúExpectation-then-gradient‚Äù vs. ‚ÄúGradient-then-expectation‚Äù</a></li>
        </ul>
      </li>
      <li><a href="#extension-kl-gradient-estimation-with-off-policy-sampling" id="markdown-toc-extension-kl-gradient-estimation-with-off-policy-sampling">Extension: KL Gradient Estimation with Off-Policy Sampling</a>        <ul>
          <li><a href="#setup-and-notation" id="markdown-toc-setup-and-notation">Setup and Notation</a></li>
          <li><a href="#key-observation-equivalence-of-the-two-orders" id="markdown-toc-key-observation-equivalence-of-the-two-orders">Key Observation: Equivalence of the Two Orders</a></li>
          <li><a href="#numerical-level-unbiasedness-holds" id="markdown-toc-numerical-level-unbiasedness-holds">Numerical Level: Unbiasedness Holds</a></li>
          <li><a href="#gradient-derivation" id="markdown-toc-gradient-derivation">Gradient Derivation</a></li>
          <li><a href="#which-ones-give-the-unbiased-reverse-kl-gradient" id="markdown-toc-which-ones-give-the-unbiased-reverse-kl-gradient">Which Ones Give the Unbiased Reverse KL Gradient?</a></li>
          <li><a href="#do-the-three-unbiased-gradient-estimators-differ-in-variance" id="markdown-toc-do-the-three-unbiased-gradient-estimators-differ-in-variance">Do the Three Unbiased Gradient Estimators Differ in Variance?</a></li>
          <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
        </ul>
      </li>
      <li><a href="#gradient-estimation-overview" id="markdown-toc-gradient-estimation-overview">Gradient Estimation Overview</a></li>
    </ul>
  </li>
  <li><a href="#practical-guidelines-for-rl" id="markdown-toc-practical-guidelines-for-rl">Practical Guidelines for RL</a>    <ul>
      <li><a href="#kl-as-a-reward-penalty-no-gradient-needed" id="markdown-toc-kl-as-a-reward-penalty-no-gradient-needed">KL as a Reward Penalty (No Gradient Needed)</a></li>
      <li><a href="#kl-as-a-loss-gradient-required" id="markdown-toc-kl-as-a-loss-gradient-required">KL as a Loss (Gradient Required)</a>        <ul>
          <li><a href="#on-policy-optimizing-reverse-kl-most-common-case" id="markdown-toc-on-policy-optimizing-reverse-kl-most-common-case">On-policy: Optimizing Reverse KL (Most Common Case)</a></li>
          <li><a href="#on-policy-optimizing-forward-kl-coverage-oriented-settings" id="markdown-toc-on-policy-optimizing-forward-kl-coverage-oriented-settings">On-policy: Optimizing Forward KL (Coverage-Oriented Settings)</a></li>
          <li><a href="#off-policy-optimizing-reverse-kl" id="markdown-toc-off-policy-optimizing-reverse-kl">Off-policy: Optimizing Reverse KL</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#a-ready-to-use-cheat-sheet" id="markdown-toc-a-ready-to-use-cheat-sheet">A Ready-to-Use Cheat Sheet</a></li>
  <li><a href="#common-implementation-pitfalls" id="markdown-toc-common-implementation-pitfalls">Common Implementation Pitfalls</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<p><img src="/assets/img/kl-estimators/kl-estimator-en.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>In reinforcement learning, how we approximate KL divergence directly affects training stability. This post systematically analyzes the differences between three classic estimators $k_1, k_2, k_3$ in both on-policy and off-policy scenarios, and provides practical guidelines for choosing them when KL is used as a reward penalty versus when it is used as a loss for backpropagation.</p>
</blockquote>

<p><a href="/reinforcement-learning/2025/12/01/kl-estimators-cn.html">‰∏≠ÊñáÁâà</a> | <a href="https://zhuanlan.zhihu.com/p/1978993413425763764">Áü•‰πéÁâàÊú¨ <img src="https://static.zhihu.com/heifetz/favicon.ico" alt="Zhihu" /></a></p>

<h2 id="introduction-the-role-of-kl-divergence-in-reinforcement-learning">Introduction: The Role of KL Divergence in Reinforcement Learning</h2>

<p>In policy optimization (PPO, GRPO, etc.) or alignment training (RLHF/RLAIF), <strong>KL regularization</strong> is the core mechanism that constrains the new policy from drifting too far away from a reference policy, in order to prevent unstable training or policy collapse.</p>

<h3 id="forward-vs-reverse-kl">Forward vs. Reverse KL</h3>

<p>Let $q_\theta$ be the current actor policy and $p$ be the reference policy. The two directions of KL divergence are</p>

<p><strong>Reverse KL</strong>:
\(D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{x \sim q_\theta}\left[\log \frac{q_\theta(x)}{p(x)}\right]\)</p>

<figure style="text-align:center;">
	<img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:95%;max-width:100%;" />
	<figcaption style="font-size:0.9em;color:gray;">Image credit: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Forward KL</strong>:
\(D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q_\theta(x)}\right]\)</p>

<figure style="text-align:center;">
	<img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:95%;max-width:100%;" />
	<figcaption style="font-size:0.9em;color:gray;">Image credit: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Intuition</strong>:</p>
<ul>
  <li><strong>Reverse KL</strong> is typically <strong>mode-seeking</strong> ‚Äì the policy concentrates on high-density regions of the reference distribution and may sacrifice diversity.</li>
  <li><strong>Forward KL</strong> is typically <strong>mass-covering</strong> ‚Äì the policy tries to cover the full support of the reference distribution.</li>
</ul>

<p>In mainstream RLHF implementations, <strong>reverse KL</strong> is more common, because we want the actor not to drift too far away from the reference policy, rather than forcing it to cover all modes.</p>

<h2 id="three-estimators-definitions-and-design-principles">Three Estimators: Definitions and Design Principles</h2>

<p>Let the ratio be $r(x) = \frac{p(x)}{q_\theta(x)}$. John Schulman introduced the following three single-sample estimators:</p>

<h3 id="k_1-the-most-naive-estimator">$k_1$: The Most Naive Estimator</h3>

\[k_1(x) = -\log r = \log q_\theta(x) - \log p(x)\]

<p>This is the most direct definition ‚Äì simply taking the negative log-ratio. It is an <strong>unbiased</strong> estimator of reverse KL, but it has a fatal issue: it <strong>can be negative</strong>, whereas KL divergence is always non-negative. This leads to extremely high variance because positive and negative samples can cancel each other out.</p>

<h3 id="k_2-a-low-variance-estimator-from-f-divergences">$k_2$: A Low-Variance Estimator from f-Divergences</h3>

\[k_2(x) = \frac{1}{2}(\log r)^2\]

<p><strong>Design motivation</strong>: The problem with $k_1$ is that it can be both positive and negative. $k_2$ squares the log-ratio, ensuring that <strong>every sample is positive</strong>. Intuitively, each sample tells you ‚Äúhow far apart‚Äù $p$ and $q$ are.</p>

<p><strong>Why is the bias small?</strong> $k_2$ is essentially an <strong>f-divergence</strong> with $f(x) = \frac{1}{2}(\log x)^2$. f-divergences have a nice property: <strong>for any differentiable $f$-divergence, when $q \approx p$, the second-order expansion has the form</strong></p>

\[D_f(p, q_\theta) = \frac{f^{\prime\prime}(1)}{2} \theta^T F \theta + O(\theta^3)\]

<p>where $F$ is the Fisher information matrix. KL divergence corresponds to $f(x) = -\log x$, with $f^{\prime\prime}(1) = 1$; $k_2$ corresponds to $f(x) = \frac{1}{2}(\log x)^2$, which also satisfies $f^{\prime\prime}(1) = 1$. This means that <strong>when the policies are close, $k_2$ behaves almost identically to the true KL</strong>, and the bias only appears in higher-order terms.</p>

<h3 id="k_3-a-best-of-both-worlds-estimator-via-control-variates">$k_3$: A ‚ÄúBest of Both Worlds‚Äù Estimator via Control Variates</h3>

\[k_3(x) = r - 1 - \log r\]

<p><strong>Design motivation</strong>: We would like an estimator that is <strong>both unbiased and low variance</strong>. A standard trick is to add a <strong>control variate</strong> ‚Äì a zero-mean term that is negatively correlated with the original estimator.</p>

<p>Note that $\mathbb{E}_q[r - 1] = \mathbb{E}_q\left[\frac{p}{q}\right] - 1 = 1 - 1 = 0$. Therefore, for any $\lambda$,</p>

\[k_1 + \lambda(r - 1) = -\log r + \lambda(r - 1)\]

<p>remains an unbiased estimator.</p>

<p><strong>Why choose $\lambda = 1$?</strong> Since $\log$ is concave, we have $\log x \leq x - 1$, so</p>

\[k_3 = (r - 1) - \log r \geq 0\]

<p>which is <strong>always non-negative</strong>. This guarantees that each sample contributes information in the ‚Äúsame direction‚Äù and eliminates the cancellation problem of $k_1$.</p>

<p><strong>Geometric intuition</strong>: $k_3$ is in fact a <strong>Bregman divergence</strong>. Consider the convex function $\phi(x) = -\log x$. The tangent at $x = 1$ is $y = 1 - x$. The Bregman divergence between $r$ and 1 is</p>

\[\begin{aligned}
D_\phi(r, 1) &amp;= \phi(r) - \phi(1) - \phi'(1)(r - 1) \\
&amp;= -\log r - 0 - (-1)(r - 1) \\
&amp;= r - 1 - \log r \\
&amp;= k_3.
\end{aligned}\]

<p>Since a convex function always lies above its tangents, this difference is <strong>naturally non-negative</strong>. More importantly, as $r \to 1$, the function and its tangent ‚Äústick together‚Äù more tightly, and the gap shrinks at the rate of $(r-1)^2$. This is exactly why $k_3$ has small variance when the policies are close.</p>

<h3 id="summary-comparing-the-three-estimators">Summary: Comparing the Three Estimators</h3>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Estimator</th>
      <th style="text-align: center;">Definition</th>
      <th style="text-align: center;">Design Principle</th>
      <th style="text-align: center;">Value Bias</th>
      <th style="text-align: center;">Variance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k\_1$</td>
      <td style="text-align: center;">$-\log r$</td>
      <td style="text-align: center;">Naive definition</td>
      <td style="text-align: center;">Unbiased</td>
      <td style="text-align: center;">High (can be neg.)</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k\_2$</td>
      <td style="text-align: center;">$\frac{1}{2}(\log r)^2$</td>
      <td style="text-align: center;">f-divergence, 2nd-order matches KL</td>
      <td style="text-align: center;">Biased (small)</td>
      <td style="text-align: center;">Low (always pos.)</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k\_3$</td>
      <td style="text-align: center;">$r - 1 - \log r$</td>
      <td style="text-align: center;">Control variate + Bregman divergence</td>
      <td style="text-align: center;">Unbiased</td>
      <td style="text-align: center;">Low (always pos.)</td>
    </tr>
  </tbody>
</table>
</div>

<p>From a pure <strong>value estimation</strong> perspective, $k_3$ looks like the ‚Äúbest of both worlds‚Äù: <strong>unbiased + low variance</strong>. However, as we will see, the <strong>story is completely different at the gradient level</strong>.</p>

<h2 id="core-analysis">Core Analysis</h2>

<h3 id="bias-and-variance-for-estimating-the-kl-value">Bias and Variance for Estimating the KL Value</h3>

<p>Assume we sample from $q_\theta$ to estimate the reverse KL $D_{\mathrm{KL}}(q_\theta | p)$.</p>

<p><strong>Unbiasedness</strong>:</p>

\[\begin{aligned}
\mathbb{E}_{q}[k_1] &amp;= \mathbb{E}_{q}\left[\log \frac{q}{p}\right] = D_{\mathrm{KL}}(q \| p) \quad \textbf{(Unbiased)}\\
\mathbb{E}_{q}[k_3] &amp;= \mathbb{E}_{q}[r - 1 - \log r] \\
&amp;= 1 - 1 + D_{\mathrm{KL}}(q \| p) \\
&amp;= D_{\mathrm{KL}}(q \| p) \quad \textbf{(Unbiased)}\\
\mathbb{E}_{q}[k_2] &amp;= \frac{1}{2}\mathbb{E}_{q}[(\log r)^2] \neq D_{\mathrm{KL}}(q \| p) \quad \textbf{(Biased)}
\end{aligned}\]

<p><strong>Conclusion</strong>: For estimating the <strong>value</strong> of the reverse KL, $k_1$ and $k_3$ are unbiased, whereas $k_2$ is biased.</p>

<p><strong>Bias‚Äìvariance trade-off</strong>:</p>

<p>In John Schulman‚Äôs experiment with $q = \mathcal{N}(0,1)$, $p = \mathcal{N}(0.1,1)$ and true KL = 0.005, the statistics are</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Estimator</th>
      <th style="text-align: center;">bias/true</th>
      <th style="text-align: center;">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k\_1$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">20</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k\_2$</td>
      <td style="text-align: center;">0.002</td>
      <td style="text-align: center;">1.42</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k\_3$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">1.42</td>
    </tr>
  </tbody>
</table>
</div>

<p>When KL is larger ($p = \mathcal{N}(1,1)$, true KL = 0.5):</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Estimator</th>
      <th style="text-align: center;">bias/true</th>
      <th style="text-align: center;">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k\_1$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">2</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k\_2$</td>
      <td style="text-align: center;">0.25</td>
      <td style="text-align: center;">1.73</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k\_3$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">1.7</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Intuition</strong>:</p>
<ul>
  <li>$k_1 = -\log r$ starts with a first-order term. When $r$ is close to 1, its fluctuations are large and it can be negative.</li>
  <li>$k_3 = r - 1 - \log r$ is second-order around $r = 1$ and always non-negative, so it has smaller variance when policies are close.</li>
  <li>When coverage is very poor (i.e., $r$ can explode), the variance of $k_3$ can blow up due to the heavy tails of $r$; in that regime, $k_1$ can be more stable.</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: To estimate the <strong>forward KL value</strong> $D_{\mathrm{KL}}(p | q) = \mathbb{E}_p[\log r]$ using samples from $q$, you can use importance sampling $\mathbb{E}_q[r \log r]$.</p>
</blockquote>

<h3 id="the-crucial-distinction-when-estimating-kl-gradients">The Crucial Distinction When Estimating KL Gradients</h3>

<p><strong>This is the most confusing yet practically important part.</strong></p>

<h4 id="true-gradients-of-forward-and-reverse-kl">True Gradients of Forward and Reverse KL</h4>

<p>Before analyzing the estimators, let us derive the <strong>true gradients</strong> of forward and reverse KL with respect to $\theta$.</p>

<p>Denote the score function $s_\theta(x) = \nabla_\theta \log q_\theta(x)$. A key property is $\mathbb{E}_{q_\theta}[s_\theta] = 0$ (since $\int \nabla_\theta q_\theta dx = \nabla_\theta 1 = 0$).</p>

<p><strong>Gradient of reverse KL</strong>:</p>

\[D_{\mathrm{KL}}(q_\theta \| p) = \int q_\theta(x) \log \frac{q_\theta(x)}{p(x)} dx.\]

<p>Taking the gradient with respect to $\theta$ (using the product rule):</p>

\[\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \int \nabla_\theta q_\theta \cdot \log \frac{q_\theta}{p} dx + \int q_\theta \cdot \nabla_\theta \log \frac{q_\theta}{p} dx.\]

<p>Using $\nabla_\theta q_\theta = q_\theta s_\theta$, $\nabla_\theta \log q_\theta = s_\theta$, and $\nabla_\theta \log p = 0$ gives</p>

\[= \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] + \mathbb{E}_q[s_\theta] = \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right].\]

<p>Thus</p>

\[\boxed{\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] = -\mathbb{E}_q[s_\theta \cdot \log r]}\]

<p><strong>Gradient of forward KL</strong>:</p>

\[D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \log \frac{p(x)}{q_\theta(x)} dx.\]

<p>Since $p(x)$ is independent of $\theta$,</p>

\[\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \cdot \nabla_\theta(-\log q_\theta(x)) dx = -\mathbb{E}_p[s_\theta].\]

<p>To estimate this using samples from $q$, we use importance sampling:</p>

\[-\mathbb{E}_p[s_\theta] = -\mathbb{E}_q\left[\frac{p}{q_\theta} s_\theta\right] = -\mathbb{E}_q[r s_\theta].\]

<p>Using $\mathbb{E}_q[s_\theta] = 0$, we can rewrite this as</p>

\[\boxed{\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_q[(1 - r) s_\theta]}\]

<p>These two ground-truth gradients will be our reference when judging what each estimator‚Äôs gradient actually corresponds to.</p>

<h4 id="two-orders-of-operations-for-gradients">Two Orders of Operations for Gradients</h4>

<p>In implementation, there are two conceptual orders of operations:</p>

<ol>
  <li><strong>Gradient-then-expectation</strong>: compute $\nabla_\theta k_i(x)$ for each sample and then average (Monte Carlo estimator).</li>
  <li><strong>Expectation-then-gradient</strong>: treat $\mathbb{E}_q[k_i]$ as a function of $\theta$ and differentiate analytically.</li>
</ol>

<p><strong>In typical deep learning code, we do (1)</strong>: autodiff computes the gradient per sample and then the batch average.</p>

<h4 id="gradients-of-the-three-estimators">Gradients of the Three Estimators</h4>

<p>Now we compute the gradients of the three estimators and see which KL gradient each one matches in expectation.</p>

<p><strong>Gradient of $k_1$</strong>:</p>

\[k_1 = -\log r = -\log \frac{p(x)}{q_\theta(x)} = \log q_\theta(x) - \log p(x),\]

<p>so</p>

\[\nabla_\theta k_1 = \nabla_\theta \log q_\theta(x) - \nabla_\theta \log p(x) = s_\theta - 0 = s_\theta.\]

<p><strong>Gradient of $k_2$</strong>:</p>

\[k_2 = \frac{1}{2}(\log r)^2.\]

<p>By the chain rule</p>

\[\begin{aligned}
\nabla_\theta k_2 
&amp;= (\log r) \cdot \nabla_\theta(\log r) \\
&amp;= (\log r) \cdot \nabla_\theta(\log p(x) - \log q_\theta(x)) \\
&amp;= (\log r)(-s_\theta) \\
&amp;= - (\log r) s_\theta.
\end{aligned}\]

<p><strong>Gradient of $k_3$</strong>:</p>

\[k_3 = r - 1 - \log r.\]

<p>First compute $\nabla_\theta r$. Since $r = p(x) q_\theta(x)^{-1}$,</p>

\[\nabla_\theta r = p(x)(-1) q_\theta(x)^{-2} \nabla_\theta q_\theta(x) = -\frac{p(x)}{q_\theta(x)} \cdot \frac{\nabla_\theta q_\theta(x)}{q_\theta(x)} = -r s_\theta.\]

<p>Then</p>

\[\nabla_\theta \log r = \frac{1}{r} \nabla_\theta r = \frac{1}{r}(-r s_\theta) = -s_\theta,\]

<p>so</p>

\[\nabla_\theta k_3 = \nabla_\theta r - 0 - \nabla_\theta \log r = -r s_\theta - (-s_\theta) = (1 - r) s_\theta.\]

<p>Taking expectations under $q_\theta$:</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Estimator</th>
      <th style="text-align: center;">$\mathbb{E}\_{q}[\nabla\_\theta k\_i]$</th>
      <th style="text-align: center;">Equals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k\_1$</td>
      <td style="text-align: center;">$\mathbb{E}\_{q}[s\_\theta] = 0$</td>
      <td style="text-align: center;"><strong>Zero (useless as loss)</strong></td>
    </tr>
    <tr>
      <td style="text-align: center;">$k\_2$</td>
      <td style="text-align: center;">$-\mathbb{E}\_{q}[(\log r) \cdot s\_\theta] = \nabla\_\theta D\_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>Gradient of reverse KL</strong></td>
    </tr>
    <tr>
      <td style="text-align: center;">$k\_3$</td>
      <td style="text-align: center;">$\mathbb{E}\_{q}[(1-r) \cdot s\_\theta] = \nabla\_\theta D\_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;"><strong>Gradient of forward KL</strong></td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Key takeaways</strong>:</p>
<ul>
  <li>The <strong>gradient of $k_2$</strong> matches the true gradient of <strong>reverse KL</strong> ‚Äì this is the correct choice if your goal is to constrain the policy not to drift from the reference.</li>
  <li>The <strong>gradient of $k_3$</strong> matches the true gradient of <strong>forward KL</strong> ‚Äì this corresponds to a coverage-style objective.</li>
  <li>The <strong>expected gradient of $k_1$ is always zero</strong> ‚Äì using $k_1$ directly as a loss is pointless.</li>
</ul>

<h4 id="expectation-then-gradient-vs-gradient-then-expectation">‚ÄúExpectation-then-gradient‚Äù vs. ‚ÄúGradient-then-expectation‚Äù</h4>

<p>If, analytically, you first compute $\mathbb{E}_q[k_i]$ and then differentiate (i.e., <strong>expectation-then-gradient</strong>), you obtain</p>

\[\nabla_\theta \mathbb{E}_q[k_1] = \nabla_\theta D_{\mathrm{KL}}(q \| p)\]

<p>and</p>

\[\nabla_\theta \mathbb{E}_q[k_3] = \nabla_\theta D_{\mathrm{KL}}(q \| p).\]

<p>Both give the gradient of reverse KL. But when you implement $k_3$ as a <strong>sample-wise loss in code</strong> and call <code class="language-plaintext highlighter-rouge">backward</code> on the batch mean, autodiff is effectively computing $\mathbb{E}_q[\nabla_\theta k_3]$, which, as shown above, is actually the gradient of <strong>forward KL</strong>.</p>

<p>This subtle difference is crucial: <strong>for the same estimator, changing the order of expectation and gradient can lead to completely different optimization objectives</strong>.</p>

<h3 id="extension-kl-gradient-estimation-with-off-policy-sampling">Extension: KL Gradient Estimation with Off-Policy Sampling</h3>

<p>The previous analysis assumed <strong>samples come from the current policy $q_\theta$</strong> (on-policy). However, in practical RL training, we often encounter off-policy scenarios:</p>

<ul>
  <li>Using old policies or mixed policies to generate data, then updating the current actor $q_\theta$.</li>
  <li>In offline RL / experience replay, the sample distribution is fixed to $\mu$, not the current $q_\theta$.</li>
</ul>

<p>In this case, if we still want to optimize the <strong>reverse KL</strong> $D_{\mathrm{KL}}(q_\theta | p)$, we must introduce <strong>importance weights</strong>.</p>

<p>For a deeper analysis of off-policy scenarios in large models, you can refer to my previous blog post: <a href="/reinforcement-learning/2025/11/15/three-policy-en.html">From Two Policies to Three: Extending TRPO under Behavior‚ÄìReference Policy Mismatch in LLM RL</a>.</p>

<h4 id="setup-and-notation">Setup and Notation</h4>

<p>Continuing with the previous notation, we now add the sampling distribution $\mu(x)$ and define the <strong>importance weight</strong>:</p>

\[w(x) = \frac{q_\theta(x)}{\mu(x)}\]

<p>When sampling from $x \sim \mu$, we use the batch mean of $w(x) k_i(x)$ as the loss and then call autodiff. What gradients do the three estimators provide?</p>

<p>A key difference is:</p>

<blockquote>
  <p><strong>Previously</strong>, the expectation was $\mathbb{E}_{q_{\theta}}[\cdot]$, where the distribution itself depended on $\theta$.
<strong>Now</strong>, the expectation is $\mathbb{E}_{\mu}[\cdot]$, and $\mu$ is independent of $\theta$.</p>
</blockquote>

<p>This fundamentally changes the relationship between ‚Äúexpectation-then-gradient‚Äù and ‚Äúgradient-then-expectation‚Äù.</p>

<h4 id="key-observation-equivalence-of-the-two-orders">Key Observation: Equivalence of the Two Orders</h4>

<p>Since $\mu$ is independent of $\theta$, for any function $f_\theta(x)$ differentiable with respect to $\theta$, we have</p>

\[\nabla_\theta \mathbb{E}_{\mu}[f_\theta(x)] = \mathbb{E}_{\mu}[\nabla_\theta f_\theta(x)]\]

<p>In other words, <strong>backpropagating through the sample mean in code (gradient-then-expectation) is equivalent to differentiating the analytical form (expectation-then-gradient)</strong>. It no longer splits into two different results as in the on-policy case.</p>

<p><strong>Therefore, in the off-policy + importance weighting case, for estimators $k_1$ and $k_3$ that are unbiased for the reverse KL value, their expected gradients will both correspond to the true gradient of the reverse KL.</strong></p>

<p>This is a fundamental difference from the on-policy case.</p>

<h4 id="numerical-level-unbiasedness-holds">Numerical Level: Unbiasedness Holds</h4>

<p>From the standard importance sampling relation $\mathbb{E}_\mu[w \cdot f] = \mathbb{E}_{q_\theta}[f]$, we have</p>

\[\mathbb{E}_\mu[w k_1] = D_{\mathrm{KL}}(q_\theta \| p), \quad
\mathbb{E}_\mu[w k_3] = D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{(Unbiased)}\]

\[\mathbb{E}_\mu[w k_2] = \mathbb{E}_{q_\theta}[k_2] \neq D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{(Biased)}\]

<p>This is exactly consistent with the on-policy case.</p>

<h4 id="gradient-derivation">Gradient Derivation</h4>

<p>First, compute the gradient of the importance weight. Since $w = q_\theta / \mu$ and $\mu$ does not depend on $\theta$:</p>

\[\nabla_\theta w(x) = w(x) s_\theta(x)\]

<p>Combining with the previously derived $\nabla_\theta k_i$ and using the product rule:</p>

<p><strong>$\nabla_\theta(w k_1)$</strong>:</p>

\[\nabla_\theta(w k_1) = (\nabla_\theta w) k_1 + w (\nabla_\theta k_1) = w s_\theta k_1 + w s_\theta = w s_\theta (k_1 + 1)\]

<p><strong>$\nabla_\theta(w k_2)$</strong>:</p>

\[\nabla_\theta(w k_2) = w s_\theta k_2 + w (-\log r) s_\theta = w s_\theta (k_2 - \log r)\]

<p><strong>$\nabla_\theta(w k_3)$</strong>:</p>

\[\nabla_\theta(w k_3) = w s_\theta k_3 + w (1-r) s_\theta = w s_\theta (k_3 + 1 - r)\]

<p>Substituting $k_3 = r - 1 - \log r$:</p>

\[k_3 + 1 - r = (r - 1 - \log r) + 1 - r = -\log r = k_1\]

<p>Thus, we have a beautiful simplification:</p>

\[\boxed{\nabla_\theta(w k_3) = w s_\theta k_1 = -w s_\theta \log r}\]

<h4 id="which-ones-give-the-unbiased-reverse-kl-gradient">Which Ones Give the Unbiased Reverse KL Gradient?</h4>

<p>Using $\mathbb{E}_\mu[w \cdot f] = \mathbb{E}_{q_\theta}[f]$ and $\mathbb{E}_{q_\theta}[s_\theta] = 0$:</p>

<p><strong>$\mathbb{E}<em>\mu[\nabla</em>\theta(w k_1)]$</strong>:</p>

\[\mathbb{E}_\mu[w s_\theta (k_1 + 1)] = \mathbb{E}_{q}[s_\theta k_1] + \underbrace{\mathbb{E}_{q}[s_\theta]}_{=0} = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark\]

<p><strong>$\mathbb{E}<em>\mu[\nabla</em>\theta(w k_2)]$</strong>:</p>

\[\mathbb{E}_\mu[w s_\theta (k_2 - \log r)] = \mathbb{E}_{q}[s_\theta (k_2 - \log r)] = \nabla_\theta \mathbb{E}_{q}[k_2]\]

<p>This is the true gradient of the f-divergence $\mathbb{E}_q[k_2]$, <strong>not</strong> the gradient of reverse KL.</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(\bar{w} k_2)]$</strong> (where $\bar{w} = \text{sg}(w)$ denotes detached weights):</p>

<p>If we treat the importance weight as a constant (detach it in code), then:</p>

\[\nabla_\theta(\bar{w} k_2) = \bar{w} \cdot \nabla_\theta k_2 = \bar{w} \cdot (-\log r) s_\theta\]

<p>Taking expectation:</p>

\[\mathbb{E}_\mu[\bar{w} \cdot (-\log r) s_\theta] = \mathbb{E}_{q}[(-\log r) s_\theta] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark\]

<p>This is exactly the true gradient of reverse KL!</p>

<p><strong>$\mathbb{E}<em>\mu[\nabla</em>\theta(w k_3)]$</strong>:</p>

\[\mathbb{E}_\mu[w s_\theta k_1] = \mathbb{E}_{q}[s_\theta k_1] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark\]

<p><strong>Summary Table</strong>:</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Weighted Estimator</th>
      <th style="text-align: center;">Expectation Target</th>
      <th style="text-align: center;">Expected Gradient Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$\frac{q\_\theta}{\mu} k\_1$</td>
      <td style="text-align: center;">$D\_{\mathrm{KL}}(q\_\theta \| p)$</td>
      <td style="text-align: center;">$\nabla\_\theta D\_{\mathrm{KL}}(q\_\theta \| p)$ (Reverse KL) ‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\frac{q\_\theta}{\mu} k\_2$</td>
      <td style="text-align: center;">$\mathbb{E}\_q[k\_2]$ (f-divergence)</td>
      <td style="text-align: center;">$\nabla\_\theta \mathbb{E}\_q[k\_2]$, <strong>NOT</strong> Reverse KL ‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\text{sg}\left(\frac{q\_\theta}{\mu}\right) k\_2$</td>
      <td style="text-align: center;">$\mathbb{E}\_q[k\_2]$ (f-divergence)</td>
      <td style="text-align: center;">$\nabla\_\theta D\_{\mathrm{KL}}(q\_\theta \| p)$ (Reverse KL) ‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\frac{q\_\theta}{\mu} k\_3$</td>
      <td style="text-align: center;">$D\_{\mathrm{KL}}(q\_\theta \| p)$</td>
      <td style="text-align: center;">$\nabla\_\theta D\_{\mathrm{KL}}(q\_\theta \| p)$ (Reverse KL) ‚úì</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Comparison with On-Policy Case ‚Äî An Interesting Reversal</strong>:</p>

<ul>
  <li>In on-policy, the gradient of $k_2$ as a loss is the reverse KL, while the expected gradient of $k_1$ is zero.</li>
  <li>In off-policy + importance weighting, $\frac{q_\theta}{\mu} k_1$ and $\frac{q_\theta}{\mu} k_3$ give the true gradient of reverse KL, while $\frac{q_\theta}{\mu} k_2$ (with weights in the gradient) <strong>is no longer applicable</strong>.</li>
  <li>However, if we <strong>detach</strong> the importance weights, $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$ also gives the true gradient of reverse KL.</li>
</ul>

<h4 id="do-the-three-unbiased-gradient-estimators-differ-in-variance">Do the Three Unbiased Gradient Estimators Differ in Variance?</h4>

<p>In the off-policy + importance sampling setting, <strong>three losses give an unbiased gradient of reverse KL</strong>:</p>

\[L_1(x) = w(x) k_1(x), \qquad
L_2(x) = \bar{w}(x) k_2(x), \qquad
L_3(x) = w(x) k_3(x),\]

<p>where $w = \dfrac{q_\theta}{\mu}$ and $\bar{w} = \mathrm{sg}(w)$ denotes a detached weight. Their gradient random variables are</p>

\[g_1(x) := \nabla_\theta L_1(x), \qquad
g_2(x) := \nabla_\theta L_2(x), \qquad
g_3(x) := \nabla_\theta L_3(x).\]

<p>Using previously derived results ($\nabla_\theta w = w s_\theta$, $\nabla_\theta k_1 = s_\theta$, $\nabla_\theta k_2 = - (\log r) s_\theta = k_1 s_\theta$, $\nabla_\theta k_3 = (1 - r) s_\theta$), we get</p>

\[g_1(x) = w(x) s_\theta(x)\big(k_1(x)+1\big),\]

\[g_2(x) = \bar{w}(x)\, k_1(x) s_\theta(x) = w(x) s_\theta(x) k_1(x),\]

\[g_3(x) = w(x) s_\theta(x)\big(k_3(x) + 1 - r(x)\big) = w(x) s_\theta(x) k_1(x).\]

<p>So in gradient space <strong>$g_2(x) \equiv g_3(x)$ exactly</strong> (same mean, same variance, same higher moments). Both share the correct expectation $\nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$. Compared with $g_1$, the only difference is a zero-mean extra term $w s_\theta$:</p>

\[g\_1(x) - g\_3(x) = w(x) s\_\theta(x), \qquad \mathbb{E}\_\mu[w s\_\theta] = \mathbb{E}\_{q\_\theta}[s\_\theta] = 0.\]

<p>Define $A(x) := w(x) s_\theta(x)$ and $B(x) := k_1(x)$. Then $g_1 = A(B+1)$ and $g_\star := g_2 = g_3 = A B$. Their variance difference is</p>

\[\mathrm{Var}_\mu(g_1) - \mathrm{Var}_\mu(g_\star) = \mathbb{E}_\mu\big[A^2 \big((B+1)^2 - B^2\big)\big] = \mathbb{E}_\mu\big[A^2 (2B + 1)\big]\]

<p>or explicitly</p>

\[\mathrm{Var}\_\mu(g\_1) - \mathrm{Var}\_\mu(g\_\star) = \mathbb{E}\_\mu\Big[w(x)^2 s\_\theta(x)^2 \big(2 k\_1(x) + 1\big)\Big].\]

<p>In the typical KL-penalty regime $q_\theta \approx p \approx \mu$, let $r(x) = 1 + \varepsilon(x)$ with $\lvert\varepsilon\rvert \ll 1$. Then $k_1 = -\log r \approx -\varepsilon$, so $2k_1 + 1 \approx 1 - 2\varepsilon$, with the leading term being a positive $O(1)$ constant. This means the right-hand side is approximately $\mathbb{E}_\mu[w^2 s_\theta^2] &gt; 0$, and therefore $\mathrm{Var}_\mu(g_1) &gt; \mathrm{Var}_\mu(g_\star)$.</p>

<p>More specifically, with first-order approximation $k_1 \approx -\varepsilon$ and $k_1 + 1 \approx 1 - \varepsilon$, we have</p>

\[g_1(x) \approx w(x) s_\theta(x)\big(1 - \varepsilon(x)\big), \qquad
g_\star(x) \approx w(x) s_\theta(x)\big(-\varepsilon(x)\big).\]

<p>Intuition: $g_1$ keeps an $O(1)$ zero-mean noise term $w s_\theta$, while $g_\star$ is an $O(\varepsilon)$ term. When policies are close, $\lvert\varepsilon\rvert$ is small, so $g_\star$ (i.e., $\bar{w} k_2$ or $w k_3$) has much lower variance than $g_1$ ($w k_1$).</p>

<p>Summary table:</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center; white-space: nowrap;">Estimator</th>
      <th style="text-align: center; white-space: nowrap;">Gradient RV</th>
      <th style="text-align: center; white-space: nowrap;">Scale ($r\approx1$)</th>
      <th style="text-align: center;">Variance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$w k\_1$</td>
      <td style="text-align: center;">$w s\_\theta (k\_1+1)$</td>
      <td style="text-align: center;">$O(1)$</td>
      <td style="text-align: center;">High</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mathrm{sg}(w) k\_2$</td>
      <td style="text-align: center;">$w s\_\theta k\_1$</td>
      <td style="text-align: center;">$O(\varepsilon)$</td>
      <td style="text-align: center;">Low</td>
    </tr>
    <tr>
      <td style="text-align: center;">$w k\_3$</td>
      <td style="text-align: center;">$w s\_\theta k\_1$</td>
      <td style="text-align: center;">$O(\varepsilon)$</td>
      <td style="text-align: center;">Low</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Warning for Extreme Off-Policy</strong>:</p>

<p>When $\mu$ differs greatly from $q_\theta$ ‚Äî for example, $\mu$ has almost no samples in the high-density regions of $q_\theta$, or $w = q_\theta / \mu$ explodes in the tails ‚Äî any method based on $\frac{q_\theta}{\mu}$ will suffer from severe variance problems. In this case, the advantage of $w k_3$ (or $\mathrm{sg}(w) k_2$) over $w k_1$ is no longer theoretically guaranteed, and strategies like clipping or regularization are needed.</p>

<p>However, in RL practice, we usually control the KL constraint and limit the degree of off-policy (e.g., using a proximal policy $\mu = q_{\theta_\text{old}}$). In this common regime, we can say with considerable confidence:</p>

<blockquote>
  <p><strong>If you have decided to use off-policy + importance sampling to optimize reverse KL, prefer $w k_3$ or $\mathrm{sg}(w) k_2$; $w k_1$ is unbiased but noisier.</strong></p>
</blockquote>

<p>This is why the DeepSeek v3.2 technical report uses $\frac{q_\theta}{\mu} k_3$ as the estimator for off-policy KL penalty.</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/dpsk-3d2-k3.png" style="width:95%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://arxiv.org/pdf/2512.02556v1">DeepSeek v3.2 Technical Report Section 3.1</a></figcaption>
</figure>

<h4 id="summary">Summary</h4>

<ul>
  <li>When sampling from a behavior policy $\mu$, the natural off-policy KL estimator is $\frac{q_\theta}{\mu} k_i$.</li>
  <li><strong>Numerically</strong>, $\frac{q_\theta}{\mu} k_1$ and $\frac{q_\theta}{\mu} k_3$ remain unbiased for reverse KL; $\frac{q_\theta}{\mu} k_2$ is biased.</li>
  <li><strong>Gradient-wise</strong>, because $\mu$ is independent of $\theta$:
    <ul>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\frac{q_\theta}{\mu} k_1)] = \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\mathrm{sg}(\frac{q_\theta}{\mu}) k_2)] = \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\frac{q_\theta}{\mu} k_3)] = \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
    </ul>
  </li>
  <li><strong>Variance-wise</strong>, $\mathrm{sg}(w) k_2$ and $w k_3$ have identical gradients and lower variance; $w k_1$ is unbiased but noisier unless clipped.</li>
</ul>

<h3 id="gradient-estimation-overview">Gradient Estimation Overview</h3>

<p>The following table summarizes the expected gradients and corresponding optimization objectives for each estimator in both on-policy and off-policy scenarios:</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center; white-space: nowrap;">Sampling Source</th>
      <th style="text-align: center;">Loss</th>
      <th style="text-align: center;">Expected $\nabla\_\theta$ Loss</th>
      <th style="text-align: center;">Corresponding Objective</th>
      <th style="text-align: center; white-space: nowrap;">Can Optimize Reverse KL?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k\_1$</td>
      <td style="text-align: center;">$\mathbb{E}\_q[s\_\theta] = 0$</td>
      <td style="text-align: center;">None (Gradient is zero)</td>
      <td style="text-align: center;">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k\_2$</td>
      <td style="text-align: center;">$\nabla\_\theta D\_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>Reverse KL</strong></td>
      <td style="text-align: center;">‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k\_3$</td>
      <td style="text-align: center;">$\nabla\_\theta D\_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;">Forward KL</td>
      <td style="text-align: center;">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k\_1$</td>
      <td style="text-align: center;">$\nabla\_\theta D\_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>Reverse KL</strong></td>
      <td style="text-align: center;">‚úì (High Variance)</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k\_2$</td>
      <td style="text-align: center;">$\nabla\_\theta \mathbb{E}\_q[k\_2]$</td>
      <td style="text-align: center;">f-divergence (Not KL)</td>
      <td style="text-align: center;">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\text{sg}\left(\frac{q}{\mu}\right) k\_2$</td>
      <td style="text-align: center;">$\nabla\_\theta D\_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>Reverse KL</strong></td>
      <td style="text-align: center;">‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k\_3$</td>
      <td style="text-align: center;">$\nabla\_\theta D\_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>Reverse KL</strong></td>
      <td style="text-align: center;">‚úì (Recommended, Low Var)</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Key Conclusions</strong>:</p>

<ol>
  <li><strong>On-policy optimizing Reverse KL</strong>: The only correct choice is $k_2$.</li>
  <li><strong>Off-policy optimizing Reverse KL</strong>: Three correct options:
    <ul>
      <li>$\frac{q}{\mu} k_1$: Unbiased but high variance</li>
      <li>$\text{sg}\left(\frac{q}{\mu}\right) k_2$: Unbiased, similar behavior to on-policy $k_2$</li>
      <li>$\frac{q}{\mu} k_3$: Unbiased and lower variance (recommended)</li>
    </ul>
  </li>
  <li><strong>$\frac{q}{\mu} k_2$ (weights in gradient) fails in off-policy</strong>: This is a trap that is easily overlooked.</li>
</ol>

<h2 id="practical-guidelines-for-rl">Practical Guidelines for RL</h2>

<h3 id="kl-as-a-reward-penalty-no-gradient-needed">KL as a Reward Penalty (No Gradient Needed)</h3>

<p>When KL is only used as a scalar penalty in reward shaping, we only care about an accurate <strong>value estimate</strong>, and we do not backpropagate through it.</p>

<p><strong>Recommendations</strong>:</p>
<ul>
  <li>Use <strong>$k_1$</strong> or <strong>$k_3$</strong> (both are unbiased for the reverse KL value).</li>
  <li>When the policy is already close to the reference, $k_3$ often has lower variance.</li>
  <li>When coverage is poor or there is severe tail mismatch, $k_1$ can be more robust.</li>
  <li>In off-policy settings, simply add the importance weight $\frac{q_\theta}{\mu}$.</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: If you want a <strong>forward KL penalty</strong> (to encourage coverage of the behavior distribution), you can use $\mathbb{E}_q[r \log r]$ or, if you can sample from $p$, directly use $\mathbb{E}_p[\log r]$.</p>
</blockquote>

<h3 id="kl-as-a-loss-gradient-required">KL as a Loss (Gradient Required)</h3>

<p>When KL is part of the loss that you differentiate, you must ensure that the gradient matches your intended objective.</p>

<h4 id="on-policy-optimizing-reverse-kl-most-common-case">On-policy: Optimizing Reverse KL (Most Common Case)</h4>

<p>Goal: constrain the actor not to drift far from the reference policy.</p>

<p><strong>Correct choice</strong>: use <strong>$k_2$</strong> as the loss.</p>

\[\mathcal{L}\_{k\_2} = \frac{1}{2}(\log r)^2.\]

<p>Its gradient expectation $\mathbb{E}_q[\nabla k_2] = \nabla_\theta D_{\mathrm{KL}}(q | p)$ is exactly the true gradient of reverse KL.</p>

<h4 id="on-policy-optimizing-forward-kl-coverage-oriented-settings">On-policy: Optimizing Forward KL (Coverage-Oriented Settings)</h4>

<p>Goal: make the policy cover the support of the reference distribution (e.g., in offline RL or imitation learning).</p>

<p><strong>Correct choice</strong>: use <strong>$k_3$</strong> as the loss.</p>

\[\mathbb{E}\_q[\nabla k\_3] = \mathbb{E}\_q[(1 - r) s\_\theta] = \nabla\_\theta D\_{\mathrm{KL}}(p \| q).\]

<p>If you backpropagate through the batch mean of $k_3$, autodiff computes exactly this forward-KL gradient ‚Äì no extra tricks needed.</p>

<h4 id="off-policy-optimizing-reverse-kl">Off-policy: Optimizing Reverse KL</h4>

<p>Goal: Data comes from behavior policy $\mu$, but we still want to optimize reverse KL.</p>

<p><strong>Recommended</strong>: use <strong>$\frac{q_\theta}{\mu} k_3$</strong> as the loss.</p>

\[\mathcal{L} = \frac{q\_\theta(x)}{\mu(x)} \cdot \left(\frac{p(x)}{q\_\theta(x)} - 1 - \log \frac{p(x)}{q\_\theta(x)}\right)\]

<ul>
  <li>Unbiased gradient.</li>
  <li>Significantly lower variance when $q_\theta \approx p$.</li>
</ul>

<p><strong>Alternative 1</strong>: Use $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$ (detach the importance weights).</p>

\[\mathcal{L} = \text{sg}\left(\frac{q\_\theta(x)}{\mu(x)}\right) \cdot \frac{1}{2}\left(\log \frac{p(x)}{q\_\theta(x)}\right)^2\]

<p>This way, the gradient becomes $\bar{w} \cdot (-\log r) s_\theta$, whose expectation is still the true gradient of reverse KL. This approach is similar in form to on-policy $k_2$, just with an additional importance weight that does not participate in the gradient.</p>

<p><strong>Alternative 2</strong>: Use $\frac{q_\theta}{\mu} k_1$ (gradient is also unbiased, but variance is higher).</p>

<p><strong>Avoid</strong>: Using $\frac{q_\theta}{\mu} k_2$ (with weights in gradient) ‚Äî the gradient is biased, not the correct direction for reverse KL.</p>

<h2 id="a-ready-to-use-cheat-sheet">A Ready-to-Use Cheat Sheet</h2>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Objective</th>
      <th style="text-align: center;">Sampling Source</th>
      <th style="text-align: center;">For <strong>Value Estimate</strong></th>
      <th style="text-align: center;">For <strong>Gradient (Loss)</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">Reverse KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;">$q$ (on-policy)</td>
      <td style="text-align: center;">$k\_1$ or $k\_3$ (unbiased)</td>
      <td style="text-align: center;">$k\_2$</td>
    </tr>
    <tr>
      <td style="text-align: center;">Reverse KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;">$\mu$ (off-policy)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k\_1$ or $\frac{q}{\mu} k\_3$ (unbiased)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k\_3$ (recommended) or $\text{sg}\left(\frac{q}{\mu}\right) k\_2$</td>
    </tr>
    <tr>
      <td style="text-align: center;">Forward KL $D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;">$q$</td>
      <td style="text-align: center;">$\mathbb{E}\_q[r\log r]$</td>
      <td style="text-align: center;">$k\_3$</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="common-implementation-pitfalls">Common Implementation Pitfalls</h2>

<p><strong>Pitfall 1: Using $k_1$ Directly as a Loss (On-Policy)</strong></p>

<p>The expected gradient of $k_1$ is zero ($\mathbb{E}_q[\nabla k_1] = \mathbb{E}_q[s_\theta] = 0$), so as a loss it is ineffective.</p>

<blockquote>
  <p><strong>Fix</strong>: Use $k_1$ or $k_3$ only when you need a scalar KL penalty in rewards (no gradient), and use $k_2$ or $k_3$ when you actually want a loss with a meaningful gradient.</p>
</blockquote>

<p><strong>Pitfall 2: Confusing $k_3$‚Äôs Unbiased Value with Its Gradient Objective</strong></p>

<p>$k_3$ is an <strong>unbiased value estimator of the reverse KL</strong>, but its <strong>gradient</strong> corresponds to the <strong>forward KL</strong>. If your goal is to optimize reverse KL but you use $k_3$ as a loss, you are in fact optimizing forward KL.</p>

<blockquote>
  <p><strong>Fix</strong>: Be explicit about your objective. Use $k_2$ when optimizing reverse KL; use $k_3$ only when you intentionally optimize forward KL.</p>
</blockquote>

<p><strong>Pitfall 3: Heavy-Tailed $r$ Causing Variance Explosion</strong></p>

<p>When the policy and reference distribution are very different, $r = p/q$ can have extreme values, causing the variance of $k_3$ (and importance-sampling-based estimators) to blow up.</p>

<blockquote>
  <p><strong>Fix</strong>: Control the KL constraint or clip $r$.</p>
</blockquote>

<p><strong>Pitfall 4: Using $k_2$ or $\frac{q_\theta}{\mu} k_2$ (with weights in gradient) in Off-Policy Settings</strong></p>

<p>In on-policy settings, $k_2$ is the correct choice for optimizing reverse KL. However, if data comes from $\mu \neq q_\theta$:</p>
<ul>
  <li>Using $k_2$ directly (unweighted): The expectation is not over $q_\theta$, so the estimator fails.</li>
  <li>Using $\frac{q_\theta}{\mu} k_2$ (with weights in gradient): The gradient is biased and does not point to the reverse KL direction.</li>
</ul>

<blockquote>
  <p><strong>Fix</strong>: In off-policy scenarios, switch to $\frac{q_\theta}{\mu} k_3$ (recommended), $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$ (detach weights), or $\frac{q_\theta}{\mu} k_1$.</p>
</blockquote>

<p><strong>Pitfall 5: Improper Handling of Importance Weight Detachment</strong></p>

<p>In implementation, $w = q_\theta / \mu$ is usually computed via <code class="language-plaintext highlighter-rouge">exp(log_prob_q - log_prob_mu)</code>. Whether to detach $w$ leads to completely different results:</p>

<ul>
  <li><strong>When using $k_1$ or $k_3$</strong>: $w$ <strong>should participate in gradient computation</strong> (do not detach), otherwise you lose the $\nabla_\theta w = w s_\theta$ term, leading to incorrect gradients.</li>
  <li><strong>When using $k_2$</strong>: $w$ <strong>should be detached</strong>, so that you get the true gradient of reverse KL. If $w$ participates in gradient computation, you get the gradient of an f-divergence, not reverse KL.</li>
</ul>

<blockquote>
  <p><strong>Summary</strong>: When choosing different estimators, make sure to match the correct detach strategy.</p>
</blockquote>

<h2 id="conclusion">Conclusion</h2>

<p><strong>One-line summary</strong>:</p>

<ul>
  <li><strong>KL for value only (reward penalty)</strong>: use $k_1$ or $k_3$ (both are unbiased for reverse KL); add importance weights if off-policy.</li>
  <li><strong>KL as a differentiable loss (needs gradients)</strong>:
    <ul>
      <li><strong>On-policy</strong>: To optimize <strong>reverse KL</strong>, use $k_2$; to optimize <strong>forward KL</strong>, use $k_3$.</li>
      <li><strong>Off-policy</strong>: To optimize <strong>reverse KL</strong>, use $\frac{q_\theta}{\mu} k_3$ (recommended, unbiased + low variance) or $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$ (detach weights).</li>
    </ul>
  </li>
</ul>

<p>Once you keep clear <strong>who you sample from</strong>, <strong>which KL you estimate</strong>, and <strong>with respect to which quantity you differentiate</strong>, the three estimators become much less confusing. Especially note: <strong>the correct choice for optimizing reverse KL differs between on-policy ($k_2$) and off-policy ($\frac{q_\theta}{\mu} k_3$ or $\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$)</strong>.</p>

<h2 id="references">References</h2>

<ol>
  <li>Dibya Ghosh. ‚ÄúKL Divergence for Machine Learning‚Äù. https://dibyaghosh.com/blog/probability/kldivergence</li>
  <li>John Schulman. ‚ÄúApproximating KL Divergence‚Äù. https://joschu.net/blog/kl-approx.html</li>
  <li>Verl Documentation. ‚ÄúProximal Policy Optimization (PPO)‚Äù. https://verl.readthedocs.io/en/latest/algo/ppo.html</li>
  <li>Âàù‰∏É123334. ‚ÄúApproximate KL in RLHF/RLVR Training: A Brief Analysis of k1 / k2 / k3‚Äù (in Chinese). https://zhuanlan.zhihu.com/p/1966872846212010437</li>
  <li>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. ‚ÄúRethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization‚Äù. https://arxiv.org/abs/2510.01555</li>
  <li>Yifan Zhang, Yiping Ji, Gavin Brown, et al. ‚ÄúOn the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning‚Äù. https://arxiv.org/abs/2505.17508</li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding {KL} Divergence Estimators in {RL}: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>


  </article></div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2025 Xihuai Leo Wang. Last updated: December 05, 2025.
      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    <!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2923RQZBXG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-2923RQZBXG');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
