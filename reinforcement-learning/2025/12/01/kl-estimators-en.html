<!DOCTYPE html><html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation | Xihuai Wang's Page</title>
    <meta name="author" content="Xihuai Leo Wang" />
    <meta name="description" content="How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate." />
    <meta name="keywords" content="Reinforcement Learning, Multi-agent System, Language Model" />

    <!-- OpenGraph -->
    <meta property="og:site_name" content="Xihuai Wang's Page" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Xihuai Wang's Page | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    <meta property="og:url" content="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html" />
    <meta property="og:description" content="How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate." />
    <meta property="og:locale" content="en_US" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    <meta name="twitter:description" content="How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate." />
    
    

    <!-- Schema.org -->
    <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Xihuai Leo Wang"
        },
        "url": "https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html",
        "@type": "WebSite",
        "description": "How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate.",
        "headline": "Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation",
        "sameAs": ["https://scholar.google.com/citations?user=hy6v3qUAAAAJ", "https://github.com/xihuai18"],
        "name": "Xihuai Leo Wang",
        "@context": "https://schema.org"
      }
    </script>


    <!-- DNS Prefetch & Preconnect for faster external resource loading -->
    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://fonts.googleapis.com">
    <link rel="dns-prefetch" href="https://fonts.gstatic.com">
    <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" /><!-- Pseudocode -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"><!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

    <!-- Prefetch/Preload for faster navigation -->
    <link rel="prefetch" href="/" as="document">
    <link rel="prefetch" href="/blog/" as="document">
    <link rel="prefetch" href="/publications/" as="document">
    <link rel="prefetch" href="/cv/" as="document">
    
    <!-- Instant.page for instant page loads on hover -->
    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module" defer></script>

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">Xihuai Wang's Page</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">About</a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">Blog</a>
          </li>

          <!-- CV -->
          <!-- 
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/" target="_blank"
              rel="noopener noreferrer">cv</a>
          </li> -->
          <!-- Other pages -->
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">Publications</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/cv/">CV</a>
          </li>
          <!-- Toggle theme mode -->
          <li class="nav-item toggle-container">
            <button id="light-toggle" class="nav-link" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post toc-layout">
  <header class="post-header">
    <h1 class="post-title">Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation</h1>
    <div class="post-meta-container">
      <div class="post-meta-row">
        <span class="post-date">
          <i class="far fa-calendar-alt"></i>
          December 1, 2025
        </span></div>
      <div class="post-tags-row">
        <a href="/blog/?year=2025" data-filter-link data-filter-type="year" data-filter-value="2025">
          üìÖ 2025
        </a>
          &nbsp; &middot; &nbsp;
          
            <a href="/blog/?category=reinforcement-learning" data-filter-link data-filter-type="category" data-filter-value="reinforcement-learning">
              üè∑Ô∏è reinforcement-learning
            </a>
            
          
      </div>
    </div>
  </header>

  <div class="post-links">
  

  <!-- Bilingual Links -->
  

  

  <!-- External Platform Links -->
  

  

  <!-- External Source (from plugin) -->
  

  <!-- Output links -->
  
    <a href="/reinforcement-learning/2025/12/01/kl-estimators-zh.html">‰∏≠ÊñáÁâàÊú¨</a>
  
    <a href="https://zhuanlan.zhihu.com/p/1978993413425763764" target="_blank">Áü•‰πé <img src="/assets/img/icons/zhihu.ico" style="height: 1em; vertical-align: middle;"></a>
  
</div>


  <div class="row">
    
      <div class="col-lg toc-content">
    

      <article class="post-content">
        <p><img src="/assets/img/kl-estimators/kl-estimator.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>How we approximate KL divergence directly affects training stability. This post systematically analyzes three estimators $k_1, k_2, k_3$ in both on-policy and off-policy scenarios, and gives practical guidelines for choosing them when KL is used as a reward penalty versus when it is used as a loss for backpropagation.</p>
</blockquote>

<h2 id="introduction-the-role-of-kl-divergence-in-reinforcement-learning">Introduction: The Role of KL Divergence in Reinforcement Learning</h2>

<p>In policy optimization (PPO, GRPO, etc.) and alignment training (RLHF/RLAIF), <strong>KL penalty</strong> is the core mechanism to constrain the new policy from deviating too far from the reference policy, preventing training instability or policy collapse. However, implementing KL penalty involves multiple layers of choices: <strong>which estimator</strong> ($k_1$, $k_2$, $k_3$), <strong>who to sample from</strong> (on-policy vs off-policy), and <strong>how to use it</strong> (as a loss for gradient backpropagation or as a reward penalty). This post systematically dissects these choices and their interrelationships, helping readers clarify the relevant concepts.</p>

<h3 id="the-distinction-between-forward-kl-and-reverse-kl">The Distinction Between Forward KL and Reverse KL</h3>

<p>Let $q_\theta$ be the current actor, $p$ the reference policy. The two directions are:</p>

<p><strong>Reverse KL:</strong>
$$
D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{x \sim q_\theta}\left[\log \frac{q_\theta(x)}{p(x)}\right]
$$</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:80%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Forward KL:</strong>
$$
D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q_\theta(x)}\right]
$$</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:80%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Intuition:</strong></p>
<ul>
  <li><strong>Reverse KL</strong> is mode-seeking: policy concentrates on high-probability regions of $p$, possibly sacrificing diversity.</li>
  <li><strong>Forward KL</strong> is mass-covering: policy tries to cover the support of $p$.</li>
</ul>

<p>RLHF typically uses <strong>reverse KL</strong> because we want the actor not to move too far from the reference, not necessarily to cover every mode.</p>

<h3 id="the-three-core-questions-who-to-sample-from-what-to-estimate-how-to-use">The Three Core Questions: Who to Sample From, What to Estimate, How to Use</h3>

<p>When implementing KL penalty in practice, we need to answer three interrelated questions:</p>

<ol>
  <li><strong>Who to sample from?</strong> Do samples come from the current policy $q_\theta$ (on-policy), or from a behavior policy $\mu$ (off-policy)?</li>
  <li><strong>What to estimate?</strong> Are we trying to estimate reverse KL $D_{\mathrm{KL}}(q_\theta \| p)$ or forward KL $D_{\mathrm{KL}}(p \| q_\theta)$?</li>
  <li><strong>How to use it?</strong> Is the KL term used as a loss for gradient backpropagation, or as a reward penalty (stop-gradient)?</li>
</ol>

<p>These three questions‚Äô different combinations determine which estimator should be used. The goal of this post is to systematically clarify these choices and their interrelationships.</p>

<h2 id="preliminaries-notation-and-basic-concepts">Preliminaries: Notation and Basic Concepts</h2>

<p>Before diving into the analysis, let‚Äôs unify our notation and derive two fundamental results that will be used repeatedly.</p>

<h3 id="notation">Notation</h3>

<ul>
  <li>$q_\theta$: Current actor policy (parameterized by $\theta$)</li>
  <li>$p$: Reference policy (independent of $\theta$)</li>
  <li>$\mu$: Behavior policy for off-policy sampling (independent of $\theta$)</li>
  <li>$s_\theta(x) = \nabla_\theta \log q_\theta(x)$: Score function</li>
  <li>$w(x) = \frac{q_\theta(x)}{\mu(x)}$: Importance weight</li>
  <li>$\text{sg}(\cdot)$: Stop-gradient operation (<code class="language-plaintext highlighter-rouge">.detach()</code> in code)</li>
</ul>

<h3 id="score-function-and-true-kl-gradients">Score Function and True KL Gradients</h3>

<p>The score function has an important property: $\mathbb{E}_{q_\theta}[s_\theta] = 0$ (since $\int \nabla_\theta q_\theta dx = \nabla_\theta \int q_\theta dx = \nabla_\theta 1 = 0$).</p>

<p>Using this property, we can derive the <strong>true gradients</strong> of forward and reverse KL divergences.</p>

<p><strong>Reverse KL Gradient:</strong></p>

<p>$$
D_{\mathrm{KL}}(q_\theta \| p) = \int q_\theta(x) \log \frac{q_\theta(x)}{p(x)} dx
$$</p>

<p>Differentiating with respect to $\theta$ (using product rule):</p>

<p>$$
\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \int \nabla_\theta q_\theta \cdot \log \frac{q_\theta}{p} dx + \int q_\theta \cdot \nabla_\theta \log \frac{q_\theta}{p} dx
$$</p>

<p>Using $\nabla_\theta q_\theta = q_\theta \cdot s_\theta$, $\nabla_\theta \log q_\theta = s_\theta$, and $\nabla_\theta \log p = 0$:</p>

<p>$$
= \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] + \mathbb{E}_q[s_\theta] = \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right]
$$</p>

<p>Thus:</p>

<p>$$
\boxed{\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] = -\mathbb{E}_q\left[s_\theta \cdot \log \frac{p}{q}\right]}
$$</p>

<p><strong>Forward KL Gradient:</strong></p>

<p>$$
D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \log \frac{p(x)}{q_\theta(x)} dx
$$</p>

<p>Since $p(x)$ is independent of $\theta$:</p>

<p>$$
\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \cdot \nabla_\theta \left(-\log q_\theta(x)\right) dx = -\mathbb{E}_p[s_\theta]
$$</p>

<p>To estimate this using samples from $q$, apply importance sampling:</p>

<p>$$
-\mathbb{E}_p[s_\theta] = -\mathbb{E}_q\left[\frac{p}{q_\theta} \cdot s_\theta\right] = -\mathbb{E}_q\left[\frac{p}{q} \cdot s_\theta\right]
$$</p>

<p>Using $\mathbb{E}_q[s_\theta] = 0$, this can be rewritten as:</p>

<p>$$
\boxed{\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_q\left[\left(1-\frac{p}{q}\right) \cdot s_\theta\right]}
$$</p>

<p>With these two results, we can later determine which KL‚Äôs true gradient each estimator‚Äôs gradient expectation corresponds to.</p>

<h2 id="three-estimators-definitions-and-design-principles">Three Estimators: Definitions and Design Principles</h2>

<p>Let $r(x) = \dfrac{p(x)}{q_\theta(x)}$. John Schulman defined three single-sample estimators:</p>

<h3 id="inlmath87mathend-the-naive-estimator">$k_1$: the naive estimator</h3>

<p>$$
k_1(x) = -\log r = \log q_\theta(x) - \log p(x)
$$</p>

<p>Direct log-ratio. It is unbiased for reverse KL, but <strong>can be negative</strong> while KL is always nonnegative, giving huge variance because positive and negative samples cancel.</p>

<h3 id="inlmath88mathend-an-f-divergence-lower-variance">$k_2$: an f-divergence, lower variance</h3>

<p>$$
k_2(x) = \frac{1}{2}(\log r)^2
$$</p>

<p><strong>Motivation:</strong> $k_1$ can be positive or negative; $k_2$ squares it so <strong>every sample is positive</strong>, each telling you how far $p$ and $q$ differ.</p>

<p><strong>Why tiny bias?</strong> $k_2$ is an <strong>f-divergence</strong> with $f(x) = \tfrac{1}{2}(\log x)^2$. All smooth f-divergences have the same second-order expansion near $q \approx p$:</p>

<p>$$
D_f(p, q_\theta) = \frac{f^{\prime\prime}(1)}{2} \theta^T F \theta + O(\theta^3)
$$</p>

<p>KL corresponds to $f(x) = -\log x$, so $f^{\prime\prime}(1) = 1$. For $k_2$, $f^{\prime\prime}(1) = 1$ as well. <strong>When policies are close, $k_2$ tracks true KL almost identically</strong>, bias only appears in higher-order terms.</p>

<h3 id="inlmath101mathend-control-variate-optimal-shape">$k_3$: control variate, ‚Äúoptimal‚Äù shape</h3>

<p>$$
k_3(x) = r - 1 - \log r
$$</p>

<p><strong>Motivation:</strong> we want <strong>unbiased and low variance</strong>. Add a <strong>control variate</strong> to $k_1$: something zero-mean and negatively correlated.</p>

<p>Because $\mathbb{E}_q[r - 1] = 1 - 1 = 0$, for any $\lambda$:</p>

<p>$$
k_1 + \lambda(r - 1) = -\log r + \lambda(r - 1)
$$</p>

<p>is still unbiased.</p>

<p><strong>Why $\lambda = 1$?</strong> By concavity of $\log$, $\log x \le x - 1$, so</p>

<p>$$
k_3 = (r - 1) - \log r \ge 0
$$</p>

<p>It is <strong>always nonnegative</strong>, avoiding the cancelation problem.</p>

<p><strong>Geometric view:</strong> $k_3$ is a <strong>Bregman divergence</strong> for $\phi(x) = -\log x$. Its tangent at $x=1$ is $y = 1 - x$, so</p>

<p>$$
\begin{aligned}
D_\phi(r, 1) &= \phi(r) - \phi(1) - \phi'(1)(r - 1) \\
&= -\log r - 0 - (-1)(r - 1) \\
&= r - 1 - \log r = k_3.
\end{aligned}
$$</p>

<p>Convexity keeps $\phi$ above its tangent, so this gap is <strong>nonnegative</strong>. As $r \to 1$, the gap shrinks quadratically $(r-1)^2$, explaining the low variance when policies are close.</p>

<h3 id="quick-comparison">Quick Comparison</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Definition</th>
      <th style="text-align: center">Design idea</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">$-\log r$</td>
      <td style="text-align: center">Naive log-ratio</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">$\tfrac{1}{2}(\log r)^2$</td>
      <td style="text-align: center">f-divergence, KL-matching 2nd order</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">$r - 1 - \log r$</td>
      <td style="text-align: center">Control variate + Bregman</td>
    </tr>
  </tbody>
</table>

<p>These three estimators arise from different design philosophies. Next, we analyze their properties when <strong>estimating KL values</strong> ‚Äî specifically, their bias and variance characteristics.</p>

<p>After understanding the definitions and design principles of the three estimators, we first analyze their properties in <strong>estimating KL values</strong> ‚Äî that is, bias and variance.</p>

<h2 id="value-estimation-bias-and-variance">Value Estimation: Bias and Variance</h2>

<p>This section analyzes the properties of the three estimators when <strong>estimating KL values</strong>. These properties are fundamental in any usage scenario.</p>

<p>Assume samples from $q_\theta$ to estimate reverse KL $D_{\mathrm{KL}}(q_\theta \| p)$.</p>

<h3 id="unbiasedness-analysis">Unbiasedness Analysis</h3>

<p>$$
\begin{aligned}
\mathbb{E}_{q}[k_1] &= \mathbb{E}_{q}\left[\log \tfrac{q}{p}\right] = D_{\mathrm{KL}}(q \| p) \quad \textbf{(unbiased)}\\
\mathbb{E}_{q}[k_3] &= \mathbb{E}_{q}\left[\frac{p}{q} - 1 - \log \frac{p}{q}\right] = 1 - 1 + D_{\mathrm{KL}}(q \| p) = D_{\mathrm{KL}}(q \| p) \quad \textbf{(unbiased)}\\
\mathbb{E}_{q}[k_2] &= \tfrac{1}{2}\mathbb{E}_{q}\left[\left(\log \frac{p}{q}\right)^2\right] \neq D_{\mathrm{KL}}(q \| p) \quad \textbf{(biased)}
\end{aligned}
$$</p>

<p><strong>Conclusion</strong>: For estimating reverse KL <strong>values</strong>, $k_1$ and $k_3$ are unbiased estimators, while $k_2$ is biased.</p>

<h3 id="variance-characteristics">Variance Characteristics</h3>

<p>John Schulman‚Äôs toy experiments ($q = \mathcal{N}(0,1)$, $p = \mathcal{N}(0.1,1)$, true KL = 0.005):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">20</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">0.002</td>
      <td style="text-align: center">1.42</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.42</td>
    </tr>
  </tbody>
</table>

<p>When KL is large ($p = \mathcal{N}(1,1)$, true KL = 0.5):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">0.25</td>
      <td style="text-align: center">1.73</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.7</td>
    </tr>
  </tbody>
</table>

<p><strong>Core intuitive understanding</strong>:</p>
<ul>
  <li>$k_1 = -\log \frac{p}{q}$ starts with a first-order term and when $\frac{p}{q}$ is close to 1, it fluctuates greatly and can be negative</li>
  <li>$k_3 = \frac{p}{q} - 1 - \log \frac{p}{q}$ is a second-order quantity near $\frac{p}{q}=1$ and always non-negative, thus having lower variance when policies are close</li>
  <li>But when coverage is severely insufficient ($\frac{p}{q}$ can explode), $k_3$‚Äôs variance can increase due to weight explosion; in this case, $k_1$ is actually more stable</li>
</ul>

<h3 id="summary-of-value-estimation">Summary of Value Estimation</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Bias for value</th>
      <th style="text-align: center">Variance characteristics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">Unbiased</td>
      <td style="text-align: center">High (can be +/-)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">Biased (minimal)</td>
      <td style="text-align: center">Low (always positive)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">Unbiased</td>
      <td style="text-align: center">Low (always positive)</td>
    </tr>
  </tbody>
</table>

<p>From the perspective of value estimation, $k_3$ is the optimal choice as ‚Äúunbiased + low variance‚Äù.</p>

<blockquote>
  <p><strong>Note</strong>: To estimate the <strong>forward KL value</strong> $D_{\mathrm{KL}}(p \| q) = \mathbb{E}_p\left[\log \frac{p}{q}\right]$, but only sample from $q$, use importance sampling $\mathbb{E}_q\left[\frac{p}{q} \log \frac{p}{q}\right]$.</p>
</blockquote>

<p>However, before choosing an estimator, there‚Äôs a more fundamental question to answer: <strong>should KL be added to rewards, or be part of the loss?</strong> This choice fundamentally affects optimization behavior and credit assignment.</p>

<h2 id="two-ways-to-use-kl-penalty">Two Ways to Use KL Penalty</h2>

<p>Having understood the value properties of these estimators, we must address a more fundamental question: <strong>How should the KL penalty be integrated into the RL algorithm?</strong> This implementation choice determines whether we need only consider the estimator‚Äôs value properties, or must also account for its gradient behavior.</p>

<p>Recall the objective function for KL-regularized reinforcement learning:</p>

<p>$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right] - \beta \cdot D_{\mathrm{KL}}(\pi_\theta \| \pi_{\text{ref}})
$$</p>

<p>This mathematical form looks unified, but when implementing it in Actor-Critic based algorithms (like PPO), it gives rise to two fundamentally different implementation paradigms ‚Äî they may differ by only a few lines of code, but correspond to completely different optimization semantics.</p>

<blockquote>
  <p><strong>Notation</strong>: In this section, we use $\text{KL}_t$ or $\text{KL}(s)$ to generically refer to a token/state-level KL estimator (such as $k_1, k_2, k_3$), with specific definitions from the earlier section ‚ÄúThree Estimators: Definitions and Design Principles‚Äù.</p>
</blockquote>

<h3 id="as-loss-kl-participates-in-gradient-backpropagation">As Loss: KL Participates in Gradient Backpropagation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantage</span> <span class="o">*</span> <span class="n">log_prob</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>  <span class="c1"># kl participates in gradient
</span></code></pre></div></div>

<p>The critic only learns environment value; KL as a regularization term for the actor directly participates in loss gradient backpropagation.</p>

<h3 id="as-reward-kl-added-to-reward-shaping">As Reward: KL Added to Reward Shaping</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kl</span> <span class="o">=</span> <span class="nf">compute_kl</span><span class="p">(</span><span class="n">log_prob_q</span><span class="p">,</span> <span class="n">log_prob_p</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">shaped_reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>
</code></pre></div></div>

<p>KL is treated as part of the environment reward, using shaped reward for standard actor-critic updates. The KL term does not participate in loss gradient backpropagation.</p>

<p>These two approaches may seem like just a <code class="language-plaintext highlighter-rouge">.detach()</code> difference in code, but in reality they correspond to fundamentally different optimization semantics.</p>

<h3 id="core-differences-between-the-two-approaches">Core Differences Between the Two Approaches</h3>

<h4 id="different-optimization-targets">Different Optimization Targets</h4>

<p><strong>KL as Loss</strong>: Optimizes <strong>original task + supervised regularization</strong>. KL doesn‚Äôt change the MDP definition, it‚Äôs just an external constraint term.</p>

<p><strong>KL as Reward</strong>: Optimizes a <strong>regularized new MDP</strong> where the reward function becomes $\tilde{r}(s,a) = r(s,a) - \beta \cdot \text{KL}(s)$.</p>

<p><strong>Intuition</strong>: The former is ‚Äúadding constraints under the original rules‚Äù; the latter is ‚Äúchanging the game rules‚Äù.</p>

<h4 id="different-gradient-paths">Different Gradient Paths</h4>

<p><strong>KL as Loss</strong>: The gradient splits into two independent paths:</p>

<p>$$
g_{\text{loss}} = \underbrace{\mathbb{E}\left[\nabla_\theta \log \pi_\theta \cdot A_t^{\text{env}}\right]}_{\text{RL gradient}} + \underbrace{\beta \cdot \nabla_\theta \text{KL}}_{\text{KL explicit gradient}}
$$</p>

<p><strong>KL as Reward</strong>: Single policy gradient, KL influence is <strong>reflected indirectly through advantage</strong>:</p>

<p>$$
g_{\text{reward}} = \mathbb{E}\left[\nabla_\theta \log \pi_\theta \cdot \tilde{A}_t\right], \quad \tilde{A}_t \text{ based on } (r_t - \beta \cdot \text{KL}_t)
$$</p>

<p><strong>Key distinction</strong>: Is KL‚Äôs force ‚Äúa separate force‚Äù or ‚Äúmultiplied on advantage‚Äù? The former‚Äôs KL gradient is deterministic, unaffected by critic quality.</p>

<h4 id="different-value-functions-and-credit-assignment">Different Value Functions and Credit Assignment</h4>

<p><strong>Value Function</strong>:</p>

<p><strong>KL as Loss</strong>: Critic only learns environment value</p>

<p>$$
V^{\text{env}}(s) = \mathbb{E}\left[\sum_t \gamma^t r_t\right]
$$</p>

<p>Cleaner separation of concerns, making it easier to monitor task return and KL divergence separately.</p>

<p><strong>KL as Reward</strong>: Critic learns mixed value</p>

<p>$$
V^{\text{reg}}(s) = \mathbb{E}\left[\sum_t \gamma^t (r_t - \beta \cdot \text{KL}_t)\right]
$$</p>

<p><strong>Credit Assignment</strong>:</p>

<p>Consider a scenario: first few steps are routing behavior, final step has high reward but also high KL.</p>

<p><strong>KL as Loss</strong>: The terminal state‚Äôs KL only appears in that state‚Äôs gradient term; the policy is still willing to <strong>visit high-reward regions but locally correct</strong> behavior.</p>

<p><strong>KL as Reward</strong>: The terminal state‚Äôs large KL is <strong>propagated back to all previous steps</strong> through TD, so the policy tends to <strong>fundamentally avoid</strong> high-KL regions ‚Äî this is ‚Äúplanning-based KL budget allocation‚Äù.</p>

<h3 id="why-this-distinction-matters">Why This Distinction Matters</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Dimension</th>
      <th style="text-align: center">KL as Loss (gradient backprop)</th>
      <th style="text-align: center">KL as Reward (stop-grad)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Optimization target</td>
      <td style="text-align: center">Original task + supervised regularization</td>
      <td style="text-align: center">Regularized new MDP</td>
    </tr>
    <tr>
      <td style="text-align: center">Actor gradient</td>
      <td style="text-align: center">RL gradient + explicit KL gradient</td>
      <td style="text-align: center">Single PG, based on shaped advantage</td>
    </tr>
    <tr>
      <td style="text-align: center">Critic</td>
      <td style="text-align: center">Learns $V^{\text{env}}$: only environment reward</td>
      <td style="text-align: center">Learns $V^{\text{reg}}$: reward + KL mixed</td>
    </tr>
    <tr>
      <td style="text-align: center">Credit Assignment</td>
      <td style="text-align: center">Local per-state, no planning</td>
      <td style="text-align: center">Multi-step backprop, planning-capable</td>
    </tr>
    <tr>
      <td style="text-align: center">Focus on</td>
      <td style="text-align: center">Estimator‚Äôs <strong>explicit gradient</strong> (corresponds to which optimization objective)</td>
      <td style="text-align: center">KL <strong>value</strong> + whether induced <strong>policy gradient</strong> is correct</td>
    </tr>
  </tbody>
</table>

<p><strong>One-liner</strong>: KL as loss makes the agent ‚Äúvisit but locally correct‚Äù ‚Äî constraints are more local and flexible; KL as reward makes the agent ‚Äúplan to avoid high-KL paths‚Äù ‚Äî constraints are more global and thorough.</p>

<p><strong>Selection Guidelines</strong>:</p>
<ul>
  <li>If you want constraints to be ‚Äú<strong>corrective</strong>‚Äù, allowing the agent to explore but locally correct behavior, choose <strong>KL as Loss</strong></li>
  <li>If you want constraints to be ‚Äú<strong>preventive</strong>‚Äù, making the agent fundamentally avoid high-KL regions, choose <strong>KL as Reward</strong></li>
</ul>

<p>After understanding the difference between these two paradigms, we can clarify:</p>
<ul>
  <li><strong>KL as Loss</strong>: Needs correct explicit gradients of the KL estimator, caring about which optimization objective the gradient corresponds to</li>
  <li><strong>KL as Reward</strong>: Needs accurate value estimation of KL, while also caring about whether the induced policy gradient is correct</li>
</ul>

<p>Below we deeply analyze the gradient properties of estimators according to the two usage modes of ‚Äúas Loss‚Äù and ‚Äúas Reward‚Äù.</p>

<h2 id="gradient-analysis-when-used-as-loss">Gradient Analysis When Used as Loss</h2>

<p>When KL serves as a loss term participating in gradient backpropagation, we must understand the optimization objective each estimator corresponds to. This is the most subtle yet critical aspect in practical implementations.</p>

<h3 id="on-policy-scenario">On-policy Scenario</h3>

<p>We start the analysis from the on-policy scenario, i.e., samples come from the current policy $q_\theta$.</p>

<h4 id="two-differentiation-orders-grad-then-expectation-vs-expectation-then-grad">Two Differentiation Orders: Grad-then-Expectation vs. Expectation-then-Grad</h4>

<p>In code implementation, there are two paths:</p>

<ol>
  <li><strong>Grad-then-expectation</strong>: Compute gradient for each sample‚Äôs $k_i(x)$, then take expectation of gradients (Monte Carlo estimation)</li>
  <li><strong>Expectation-then-grad</strong>: Treat $\mathbb{E}_q[k_i]$ as a loss function, then differentiate the analytical expression</li>
</ol>

<p><strong>In typical deep learning code, we actually execute ‚Äúgrad-then-expectation‚Äù</strong> ‚Äî autograd computes gradients for each sample, then averages over the batch.</p>

<h4 id="gradient-derivations-for-the-three-estimators">Gradient Derivations for the Three Estimators</h4>

<p>Now we compute the gradients of the three estimators to see which KL‚Äôs true gradient their expectations correspond to (refer to the ‚ÄúPreliminaries‚Äù section).</p>

<p><strong>Deriving $\nabla_\theta k_1$</strong>:</p>

<p>$$
k_1 = -\log \frac{p(x)}{q_\theta(x)} = \log q_\theta(x) - \log p(x)
$$</p>

<p>$$
\nabla_\theta k_1 = \nabla_\theta \log q_\theta(x) - \nabla_\theta \log p(x) = s_\theta - 0 = s_\theta
$$</p>

<p><strong>Deriving $\nabla_\theta k_2$</strong>:</p>

<p>$$
k_2 = \frac{1}{2}\left(\log \frac{p}{q}\right)^2
$$</p>

<p>By the chain rule:</p>

<p>$$
\begin{aligned}
\nabla_\theta k_2 
&= \left(\log \frac{p}{q}\right) \cdot \nabla_\theta\left(\log \frac{p}{q}\right) \\
&= \left(\log \frac{p}{q}\right) \cdot \nabla_\theta(\log p(x) - \log q_\theta(x)) \\
&= \left(\log \frac{p}{q}\right)(-s_\theta) \\
&= - \left(\log \frac{p}{q}\right) s_\theta.
\end{aligned}
$$</p>

<p><strong>Deriving $\nabla_\theta k_3$</strong>:</p>

<p>$$
k_3 = \frac{p}{q} - 1 - \log \frac{p}{q}
$$</p>

<p>First, compute $\nabla_\theta \frac{p}{q}$. Since $\frac{p}{q} = p(x) \cdot q_\theta(x)^{-1}$:</p>

<p>$$
\nabla_\theta \frac{p}{q} = p(x) \cdot (-1) \cdot q_\theta(x)^{-2} \cdot \nabla_\theta q_\theta(x) = -\frac{p(x)}{q_\theta(x)} \cdot \frac{\nabla_\theta q_\theta(x)}{q_\theta(x)} = -\frac{p}{q} \cdot s_\theta
$$</p>

<p>Then compute $\nabla_\theta \log \frac{p}{q}$:</p>

<p>$$
\nabla_\theta \log \frac{p}{q} = \frac{q}{p} \nabla_\theta \frac{p}{q} = \frac{q}{p} \cdot \left(-\frac{p}{q} \cdot s_\theta\right) = -s_\theta
$$</p>

<p>Therefore:</p>

<p>$$
\nabla_\theta k_3 = \nabla_\theta \frac{p}{q} - 0 - \nabla_\theta \log \frac{p}{q} = -\frac{p}{q} \cdot s_\theta - (-s_\theta) = \left(1 - \frac{p}{q}\right) \cdot s_\theta
$$</p>

<p>Taking expectations under $q_\theta$:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">$\mathbb{E}_{q}[\nabla_\theta k_i]$</th>
      <th style="text-align: center">Equivalent to</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">$\mathbb{E}_{q}[s_\theta] = 0$</td>
      <td style="text-align: center">Zero (useless as loss)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">$-\mathbb{E}_{q}\left[\left(\log \frac{p}{q}\right) \cdot s_\theta\right] = \nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL gradient</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">$\mathbb{E}_{q}\left[\left(1-\frac{p}{q}\right) \cdot s_\theta\right] = \nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center">Forward KL gradient</td>
    </tr>
  </tbody>
</table>

<p><strong>Key insights</strong>:</p>
<ul>
  <li><strong>$k_2$‚Äôs gradient</strong> equals the true reverse KL gradient ‚Äî the correct choice for optimizing ‚Äúconstraining policy not to deviate from reference‚Äù</li>
  <li><strong>$k_3$‚Äôs gradient</strong> equals the true forward KL gradient ‚Äî corresponding to a ‚Äúcoverage‚Äù objective</li>
  <li><strong>$k_1$‚Äôs gradient expectation is always zero</strong> ‚Äî backpropagating as loss is meaningless!</li>
</ul>

<h4 id="key-finding-inlmath175mathend-ineffective-inlmath176mathend-for-reverse-kl-inlmath177mathend-for-forward-kl">Key Finding: $k_1$ Ineffective, $k_2$ for Reverse KL, $k_3$ for Forward KL</h4>

<p><strong>‚ÄúExpectation-then-grad‚Äù vs. ‚ÄúGrad-then-expectation‚Äù</strong>:</p>

<p>If we analytically treat $\mathbb{E}_q[k_i]$ as a function of $\theta$ and then differentiate (i.e., ‚Äúexpectation-then-grad‚Äù), then:</p>

<p>$$
\nabla_\theta \mathbb{E}_q[k_1] = \nabla_\theta D_{\mathrm{KL}}(q \| p)
$$</p>

<p>$$
\nabla_\theta \mathbb{E}_q[k_3] = \nabla_\theta D_{\mathrm{KL}}(q \| p)
$$</p>

<p>Both give reverse KL‚Äôs gradient. However, when directly calling backpropagation on sample means of $k_3$ in code, autograd executes ‚Äúgrad-then-expectation‚Äù, yielding $\mathbb{E}_q[\nabla_\theta k_3]$, i.e., <strong>the forward KL gradient</strong>.</p>

<p><strong>Key conclusion for on-policy scenario</strong>: For the same estimator, the two differentiation orders can give completely different results. Specifically:</p>
<ul>
  <li>To optimize <strong>reverse KL</strong>: must use $k_2$</li>
  <li>To optimize <strong>forward KL</strong>: use $k_3$</li>
  <li>$k_1$‚Äôs gradient expectation is always zero, useless as loss</li>
</ul>

<h3 id="off-policy-scenario">Off-policy Scenario</h3>

<p>Now consider the off-policy scenario where samples come from a behavior policy $\mu \neq q_\theta$. This situation is very common in practical RL training when using old/mixed policies to generate data or in offline RL with fixed sample distributions.</p>

<p>For an in-depth analysis of off-policy scenarios in large language models, refer to: <a href="/reinforcement-learning/2025/11/15/three-policy-en.html">From Two-Policy to Three-Policy: TRPO Extension Under Behavior-Reference Mismatch in LLM RL</a>.</p>

<h4 id="importance-weighting-and-gradient-equivalence">Importance Weighting and Gradient Equivalence</h4>

<p>When sampling from $\mu$ independent of $\theta$, we use importance weight $w(x) = \frac{q_\theta(x)}{\mu(x)}$ in the loss. A key difference emerges:</p>

<blockquote>
  <p><strong>Before</strong> (on-policy): expectation $\mathbb{E}_{q_{\theta}}[\cdot]$ depends on $\theta$<br />
<strong>Now</strong> (off-policy): expectation $\mathbb{E}_{\mu}[\cdot]$ is independent of $\theta$</p>
</blockquote>

<p>This makes ‚Äúexpectation-then-grad‚Äù and ‚Äúgrad-then-expectation‚Äù <strong>equivalent</strong>:</p>

<p>$$
\nabla_\theta \mathbb{E}_{\mu}[f_\theta(x)] = \mathbb{E}_{\mu}[\nabla_\theta f_\theta(x)]
$$</p>

<h4 id="gradient-derivations-with-importance-weights">Gradient Derivations with Importance Weights</h4>

<p>Since $\nabla_\theta w = w s_\theta$ and using product rule with previously derived $\nabla_\theta k_i$:</p>

<p>$$
\nabla_\theta(w k_1) = w s_\theta (k_1 + 1), \quad
\nabla_\theta(w k_2) = w s_\theta (k_2 - k_1), \quad
\nabla_\theta(w k_3) = w s_\theta k_1
$$</p>

<p>Note that $\nabla_\theta(w k_3) = w s_\theta k_1 = \nabla_\theta(\text{sg}(w) k_2)$  ‚Äî these two are <strong>gradient-identical</strong>.</p>

<p>Taking expectations and using $\mathbb{E}_\mu[w \cdot f] = \mathbb{E}_{q}[f]$ and $\mathbb{E}_{q}[s_\theta]=0$:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Weighted estimator</th>
      <th style="text-align: center">Value target</th>
      <th style="text-align: center">Gradient target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$\frac{q_\theta}{\mu} k_1$</td>
      <td style="text-align: center">$D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$ ‚úì (higher var)</td>
    </tr>
    <tr>
      <td style="text-align: center">$\frac{q_\theta}{\mu} k_2$</td>
      <td style="text-align: center">$\mathbb{E}_q[k_2]$ (f-div)</td>
      <td style="text-align: center">$\nabla \mathbb{E}_q[k_2]$ ‚úó (not reverse KL)</td>
    </tr>
    <tr>
      <td style="text-align: center">$\text{sg}\left(\frac{q_\theta}{\mu}\right) k_2$</td>
      <td style="text-align: center">$\mathbb{E}_q[k_2]$ (f-div)</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$ ‚úì (low var)</td>
    </tr>
    <tr>
      <td style="text-align: center">$\frac{q_\theta}{\mu} k_3$</td>
      <td style="text-align: center">$D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$ ‚úì (low var)</td>
    </tr>
  </tbody>
</table>

<p><strong>Key finding</strong>: $\frac{q}{\mu} k_3$ and $\text{sg}\left(\frac{q}{\mu}\right) k_2$ have <strong>identical gradients</strong> $w s_\theta k_1$ and are statistically equivalent.</p>

<h4 id="variance-analysis">Variance Analysis</h4>

<p>In the typical regime where $q_\theta \approx p \approx \mu$, setting $\frac{p}{q}=1+\varepsilon$ with $|\varepsilon|\ll1$:</p>

<ul>
  <li>$\nabla_\theta(w k_1) \approx w s_\theta(1-\varepsilon)$ ‚Äî contains $O(1)$ noise term $w s_\theta$</li>
  <li>$\nabla_\theta(w k_3) = \nabla_\theta(\text{sg}(w) k_2) \approx w s_\theta(-\varepsilon)$ ‚Äî only $O(\varepsilon)$ terms</li>
</ul>

<p>The variance difference is:</p>

<p>$$
\mathrm{Var}_\mu(g_1) - \mathrm{Var}_\mu(g_\star) = \mathbb{E}_\mu\big[w^2 s_\theta^2 (2k_1+1)\big] \approx \mathbb{E}_\mu[w^2 s_\theta^2] > 0
$$</p>

<p>Therefore, $w k_1$ has roughly one order of magnitude higher variance than $w k_3$ or $\text{sg}(w) k_2$.</p>

<p>This is why DeepSeek v3.2 uses $\frac{q_\theta}{\mu} k_3$ for off-policy KL penalty:</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/dpsk-3d2-k3.png" style="width:95%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Source: <a href="https://arxiv.org/pdf/2512.02556v1">DeepSeek v3.2 Technical Report Section 3.1</a></figcaption>
</figure>

<h4 id="practical-recommendations">Practical Recommendations</h4>

<p><strong>For on-policy reverse KL optimization:</strong></p>
<ul>
  <li><strong>Use $k_2$ as loss</strong>: $\mathcal{L} = \tfrac{1}{2}(\log r)^2$</li>
  <li>Gradient: $\mathbb{E}_q[\nabla k_2] = \nabla D_{\mathrm{KL}}(q \| p)$</li>
</ul>

<p><strong>For on-policy forward KL optimization</strong> (coverage objectives):</p>
<ul>
  <li><strong>Use $k_3$ as loss</strong>: Autograd gives $\mathbb{E}_q[(1-r) s_\theta] = \nabla D_{\mathrm{KL}}(p \| q)$</li>
</ul>

<p><strong>For off-policy reverse KL optimization</strong> (samples from $\mu$):</p>
<ul>
  <li><strong>Recommended</strong>: $\dfrac{q_\theta}{\mu} k_3$ or $\text{sg}\left(\dfrac{q_\theta}{\mu}\right) k_2$ (identical, low variance)</li>
  <li><strong>Fallback</strong>: $\dfrac{q_\theta}{\mu} k_1$ (unbiased but ~10√ó higher variance)</li>
  <li><strong>Avoid</strong>: $\dfrac{q_\theta}{\mu} k_2$ with weight in gradient (biased for reverse KL)</li>
</ul>

<h3 id="comprehensive-gradient-analysis-summary">Comprehensive Gradient Analysis Summary</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Sampling</th>
      <th style="text-align: center">Loss</th>
      <th style="text-align: center">Gradient expectation</th>
      <th style="text-align: center">Optimizes</th>
      <th style="text-align: center">Usable for reverse KL?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$q$ (on)</td>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">$\mathbb{E}_q[s_\theta]=0$</td>
      <td style="text-align: center">None (zero)</td>
      <td style="text-align: center">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$q$ (on)</td>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$q$ (on)</td>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center">Forward KL</td>
      <td style="text-align: center">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu$ (off)</td>
      <td style="text-align: center">$\frac{q}{\mu} k_1$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì (high variance)</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu$ (off)</td>
      <td style="text-align: center">$\frac{q}{\mu} k_2$</td>
      <td style="text-align: center">$\nabla \mathbb{E}_q[k_2]$</td>
      <td style="text-align: center">f-div (wrong)</td>
      <td style="text-align: center">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu$ (off)</td>
      <td style="text-align: center">$\text{sg}(\frac{q}{\mu}) k_2$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì (low variance)</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu$ (off)</td>
      <td style="text-align: center">$\frac{q}{\mu} k_3$</td>
      <td style="text-align: center">$\nabla D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì (low variance, recommended)</td>
    </tr>
  </tbody>
</table>

<h2 id="gradient-analysis-when-used-as-reward">Gradient Analysis When Used as Reward</h2>

<p>Having analyzed the value estimation properties of the three estimators in the previous section, one might naturally assume: since $k_1$ and $k_3$ are both unbiased for reverse KL value, incorporating them (with stop-gradient) as reward penalties should be equally valid.</p>

<p><strong>This intuition, however, is incorrect.</strong></p>

<p>The issue is: when KL is used as a reward penalty, although the KL term itself doesn‚Äôt backpropagate gradients, it will indirectly affect the policy gradient through advantage. Therefore, to evaluate whether an estimator ‚Äúcan be used for reward penalty‚Äù, we shouldn‚Äôt just look at value bias, but whether <strong>the policy gradient it induces is correct</strong>.</p>

<h3 id="the-true-kl-regularized-policy-gradient">The True KL-Regularized Policy Gradient</h3>

<p>Consider the KL-regularized reinforcement learning objective:</p>

<p>$$
J(\theta) = \mathbb{E}_{q_\theta}[R] - \beta \cdot D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p>Its true gradient is:</p>

<p>$$
\nabla_\theta J = \mathbb{E}_{q_\theta}[s_\theta \cdot R] - \beta \cdot \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p>Using the result from the ‚ÄúPreliminaries‚Äù section, the reverse KL gradient is:</p>

<p>$$
\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(-\log \frac{p}{q}\right)\right] = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1]
$$</p>

<p>Therefore, the true KL-regularized policy gradient is:</p>

<p>$$
\nabla_\theta J = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(R - \beta \cdot k_1\right)\right]
$$</p>

<h3 id="gradient-form-when-using-estimator-inlmath258mathend">Gradient Form When Using Estimator $\hat{k}$</h3>

<p>When we use some estimator $\hat{k}$ (with stop-gradient) as a reward penalty, the shaped reward is $\tilde{R} = R - \beta \cdot \text{sg}(\hat{k})$, and the policy gradient becomes:</p>

<p>$$
\nabla_\theta \tilde{J} = \mathbb{E}_{q_\theta}\left[s_\theta \cdot (R - \beta \cdot \hat{k})\right]
$$</p>

<p><strong>Unbiasedness condition</strong>: $\nabla_\theta \tilde{J} = \nabla_\theta J$ if and only if</p>

<p>$$
\mathbb{E}_{q_\theta}[s_\theta \cdot \hat{k}] = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1]
$$</p>

<h3 id="using-inlmath262mathend-as-penalty-gradient-unbiased">Using $k_1$ as Penalty: Gradient Unbiased</h3>

<p>When $\hat{k} = k_1$, the condition is automatically satisfied:</p>

<p>$$
\mathbb{E}_{q_\theta}[s_\theta \cdot k_1] = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1] \quad \checkmark
$$</p>

<p>Therefore, <strong>when $k_1$ is used as a reward penalty, the induced policy gradient is unbiased</strong>.</p>

<h3 id="using-inlmath265mathend-as-penalty-gradient-biased">Using $k_3$ as Penalty: Gradient Biased</h3>

<p>When $\hat{k} = k_3 = \frac{p}{q} - 1 - \log \frac{p}{q}$:</p>

<p>$$
\mathbb{E}_{q_\theta}[s_\theta \cdot k_3] = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(\frac{p}{q} - 1\right)\right] + \mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(-\log \frac{p}{q}\right)\right]
$$</p>

<p>The second term is exactly $\mathbb{E}_{q_\theta}[s_\theta \cdot k_1]$. The problem lies in the first term:</p>

<p>$$
\mathbb{E}_{q_\theta}\left[s_\theta \cdot \left(\frac{p}{q} - 1\right)\right] = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \frac{p}{q}\right] - \underbrace{\mathbb{E}_{q_\theta}[s_\theta]}_{=0} = \mathbb{E}_{q_\theta}\left[s_\theta \cdot \frac{p}{q}\right]
$$</p>

<p>This can be rewritten as:</p>

<p>$$
\mathbb{E}_{q_\theta}\left[s_\theta \cdot \frac{p}{q}\right] = \int q_\theta(x) \cdot \nabla_\theta \log q_\theta(x) \cdot \frac{p(x)}{q_\theta(x)} dx = \int p(x) \cdot \nabla_\theta \log q_\theta(x) dx = \mathbb{E}_p[s_\theta]
$$</p>

<p>Using the forward KL gradient formula $\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = -\mathbb{E}_p[s_\theta]$, we have:</p>

<p>$$
\mathbb{E}_{q_\theta}\left[s_\theta \cdot \frac{p}{q}\right] = -\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta)
$$</p>

<p>Therefore:</p>

<p>$$
\mathbb{E}_{q_\theta}[s_\theta \cdot k_3] = \underbrace{-\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta)}_{\text{bias term}} + \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)
$$</p>

<p><strong>When $k_3$ is used as a reward penalty, the gradient is biased</strong>, with the bias term equal to the negative of the forward KL gradient.</p>

<p><strong>Geometric meaning of the bias</strong>: Using $k_3$ as a reward penalty is equivalent to optimizing a ‚Äúwrong mixed objective‚Äù:</p>
<ul>
  <li>Penalizing reverse KL (hoping policy doesn‚Äôt deviate from reference)</li>
  <li>But also <strong>wrongly encouraging forward KL to increase</strong> (hoping reference doesn‚Äôt cover policy)</li>
</ul>

<p>These two directions conflict, potentially causing optimization instability.</p>

<p><strong>Experimental verification</strong>: Shah et al. (2025)‚Äôs experiments show that in on-policy RL fine-tuning of LLMs:</p>
<ul>
  <li><strong>$k_1$ in reward</strong>: Training is stable</li>
  <li><strong>$k_3$ in reward</strong>: <strong>Training collapses</strong></li>
</ul>

<p>This is completely consistent with our theoretical analysis.</p>

<h3 id="off-policy-scenario-1">Off-policy Scenario</h3>

<p>The above analysis assumes on-policy sampling. Does the conclusion change in off-policy scenarios?</p>

<p>Let samples come from behavior policy $\mu$, using importance-weighted policy gradient:</p>

<p>$$
\nabla_\theta \tilde{J} = \mathbb{E}_\mu\left[\frac{q_\theta}{\mu} \cdot s_\theta \cdot (R - \beta \cdot k)\right]
$$</p>

<p>Using $\mathbb{E}_\mu[\frac{q_\theta}{\mu} \cdot f] = \mathbb{E}_{q_\theta}[f]$, this equals:</p>

<p>$$
= \mathbb{E}_{q_\theta}[s_\theta \cdot R] - \beta \cdot \mathbb{E}_{q_\theta}[s_\theta \cdot k]
$$</p>

<p><strong>The unbiasedness condition</strong> remains $\mathbb{E}_{q_\theta}[s_\theta \cdot k] = \mathbb{E}_{q_\theta}[s_\theta \cdot k_1]$, exactly the same as on-policy.</p>

<p><strong>Key insight</strong>: In the off-policy policy gradient framework, the importance weight $\frac{q_\theta}{\mu}$ acts on the entire policy gradient estimator, <strong>no need to separately weight the KL estimator in shaped reward</strong>. Therefore:</p>

<ul>
  <li>Shaped reward keeps its original form: $\tilde{R} = R - \beta \cdot k_1$ (not $R - \beta \cdot \frac{q_\theta}{\mu} k_1$)</li>
  <li>Conclusion same as on-policy: <strong>only use $k_1$, not $k_3$</strong></li>
</ul>

<h3 id="key-finding-only-inlmath281mathend-can-be-used-for-reward-penalty">Key Finding: Only $k_1$ Can Be Used for Reward Penalty</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Value unbiased?</th>
      <th style="text-align: center">Gradient unbiased when used as reward penalty?</th>
      <th style="text-align: center">Actual performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">‚úì</td>
      <td style="text-align: center">‚úì</td>
      <td style="text-align: center">Stable</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">‚úì</td>
      <td style="text-align: center">‚úó</td>
      <td style="text-align: center">Collapses</td>
    </tr>
  </tbody>
</table>

<p><strong>Core lesson</strong>: When evaluating KL estimators, ‚Äúvalue unbiasedness‚Äù and ‚Äúgradient correctness‚Äù are two independent dimensions. For reward penalty scenarios (whether on-policy or off-policy), <strong>only $k_1$ is the correct choice</strong>. Although $k_3$ is value-unbiased and has lower variance, using it as a reward penalty causes biased gradients and may lead to training collapse.</p>

<h2 id="practical-guide-and-common-pitfalls">Practical Guide and Common Pitfalls</h2>

<p>With the preceding theoretical analysis, this section provides selection recommendations for specific scenarios, convenient for direct reference.</p>

<h3 id="quick-reference-table">Quick Reference Table</h3>

<p>The table below provides recommended estimator choices along three dimensions: ‚Äútarget KL direction‚Äù √ó ‚Äúsampling source‚Äù √ó ‚Äúusage mode‚Äù. ‚ÄúFor <strong>Loss</strong>‚Äù corresponds to KL as loss (gradient backpropagation needed); ‚ÄúFor <strong>Reward</strong>‚Äù corresponds to KL as reward penalty (stop-gradient).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Target</th>
      <th style="text-align: center">Sampling</th>
      <th style="text-align: center">For Loss (gradient backprop)</th>
      <th style="text-align: center">For Reward (stop-grad)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Reverse KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">$q$ (on-policy)</td>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">$k_1$</td>
    </tr>
    <tr>
      <td style="text-align: center">Reverse KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">$\mu$ (off-policy)</td>
      <td style="text-align: center">$\tfrac{q}{\mu} k_3$ or $\text{sg}\left(\tfrac{q}{\mu}\right) k_2$</td>
      <td style="text-align: center">$k_1$</td>
    </tr>
    <tr>
      <td style="text-align: center">Forward KL $D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center">$q$</td>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">$\mathbb{E}_q\left[\frac{p}{q} \log \frac{p}{q}\right]$</td>
    </tr>
  </tbody>
</table>

<h3 id="kl-as-loss-needs-gradient-backpropagation">KL as Loss (Needs Gradient Backpropagation)</h3>

<p>When KL is used as part of a loss for backpropagation, we must consider gradient correctness.</p>

<h4 id="on-policy-optimize-reverse-kl-most-common">On-policy: Optimize Reverse KL (Most Common)</h4>

<p>Goal: Control actor to not deviate from reference policy.</p>

<p><strong>Correct approach</strong>: Use <strong>$k_2$</strong> as loss.</p>

<p>$$
\mathcal{L}_{k_2} = \frac{1}{2}\left(\log \frac{p}{q}\right)^2
$$</p>

<p>Its gradient expectation $\mathbb{E}_q[\nabla k_2] = \nabla_\theta D_{\mathrm{KL}}(q \| p)$ is exactly the true gradient of reverse KL.</p>

<h4 id="on-policy-optimize-forward-kl-coverage-scenario">On-policy: Optimize Forward KL (Coverage Scenario)</h4>

<p>Goal: Make policy cover the support of the reference distribution (offline RL, imitation learning, etc.).</p>

<p><strong>Correct approach</strong>: Use <strong>$k_3$</strong> as loss.</p>

<p>$$
\mathbb{E}_q[\nabla k_3] = \mathbb{E}_q\left[\left(1-\frac{p}{q}\right) \cdot s_\theta\right] = \nabla_\theta D_{\mathrm{KL}}(p \| q)
$$</p>

<p>Directly calling loss gradient backprop on the sample mean of $k_3$, autograd computes $\mathbb{E}_q[\nabla_\theta k_3]$, which is the forward KL gradient, no additional processing needed.</p>

<h4 id="off-policy-optimize-reverse-kl">Off-policy: Optimize Reverse KL</h4>

<p>Goal: Data comes from behavior policy $\mu$, still want to optimize reverse KL.</p>

<p><strong>Recommended approach</strong>: Use <strong>$\dfrac{q_\theta}{\mu} k_3$</strong> or <strong>$\text{sg}\left(\dfrac{q_\theta}{\mu}\right) k_2$</strong> as loss (both have identical gradients).</p>

<p>$$
\mathcal{L} = \dfrac{q_\theta(x)}{\mu(x)} \cdot \left(\dfrac{p(x)}{q_\theta(x)} - 1 - \log \dfrac{p(x)}{q_\theta(x)}\right)
$$</p>

<p>or</p>

<p>$$
\mathcal{L} = \text{sg}\left(\dfrac{q_\theta(x)}{\mu(x)}\right) \cdot \dfrac{1}{2}\left(\log \dfrac{p(x)}{q_\theta(x)}\right)^2
$$</p>

<ul>
  <li>Gradients are unbiased</li>
  <li>When $q_\theta \approx p$, both have much lower variance</li>
</ul>

<p><strong>Fallback</strong>: Use $\dfrac{q_\theta}{\mu} k_1$ (gradient also unbiased but higher variance)</p>

<p><strong>Avoid</strong>: Using $\dfrac{q_\theta}{\mu} k_2$ (with weight in gradient) ‚Äî gradient is biased, not the correct direction for reverse KL</p>

<h3 id="kl-as-reward-penalty-stop-gradient">KL as Reward Penalty (Stop-gradient)</h3>

<p>When KL is used as a scalar penalty added to reward, although the KL term itself doesn‚Äôt backpropagate gradients, it will indirectly affect the policy gradient through advantage. Based on the earlier section ‚ÄúGradient Analysis When Used as Reward‚Äù:</p>

<p><strong>Recommend</strong>:</p>
<ul>
  <li>Use <strong>$k_1$</strong> (value-unbiased and induced policy gradient is also unbiased)</li>
  <li>Conclusion is the same whether on-policy or off-policy</li>
</ul>

<p><strong>Avoid</strong>:</p>
<ul>
  <li>Using $k_3$ (although value-unbiased and lower variance, the induced policy gradient is biased and may cause training collapse)</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: In off-policy policy gradient, the importance weight $\frac{q_\theta}{\mu}$ acts on the entire $s_\theta \cdot \tilde{R}$; the shaped reward itself can keep the form $\tilde{R} = R - \beta \cdot k_1$.</p>
</blockquote>

<h3 id="common-pitfalls">Common Pitfalls</h3>

<h4 id="pitfall-1-using-inlmath315mathend-directly-as-loss-on-policy">Pitfall 1: Using $k_1$ Directly as Loss (On-policy)</h4>

<p>The gradient expectation of $k_1$ is always zero ($\mathbb{E}_q[\nabla k_1] = \mathbb{E}_q[s_\theta] = 0$); using it as a loss is completely ineffective.</p>

<blockquote>
  <p><strong>Solution</strong>: Use $k_2$ for on-policy reverse KL optimization; use $k_3$ for forward KL optimization.</p>
</blockquote>

<h4 id="pitfall-2-confusing-inlmath320mathends-value-unbiasedness-with-gradient-behavior">Pitfall 2: Confusing $k_3$‚Äôs Value Unbiasedness with Gradient Behavior</h4>

<p>$k_3$ is value-unbiased for <strong>reverse KL value</strong>, but its <strong>gradient</strong> corresponds to <strong>forward KL</strong> ‚Äî these are completely different.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Scenario</th>
      <th style="text-align: center">Problem</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Using $k_3$ as Loss (targeting reverse KL)</td>
      <td style="text-align: center">$\nabla k_3$ corresponds to forward KL, optimizing wrong direction</td>
    </tr>
    <tr>
      <td style="text-align: center">Using $k_3$ as Reward penalty (targeting reverse KL)</td>
      <td style="text-align: center">Induces biased policy gradient (bias term $-\nabla D_{\mathrm{KL}}(p\|q)$), may cause training collapse</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Solution</strong>:</p>
  <ul>
    <li>Use as <strong>Loss</strong> to optimize reverse KL ‚Üí use $k_2$; use $k_3$ only for forward KL</li>
    <li>Use as <strong>Reward</strong> penalty ‚Üí only use $k_1$ (whether on-policy or off-policy)</li>
  </ul>
</blockquote>

<h4 id="pitfall-3-off-policy-detach-handling-of-importance-weights">Pitfall 3: Off-policy Detach Handling of Importance Weights</h4>

<p>In off-policy scenarios, whether to detach the importance weight $w = q_\theta / \mu$ leads to completely different results. The following table summarizes the correct detach strategies:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Detach $w$?</th>
      <th style="text-align: center">Gradient corresponds to</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$w k_1$</td>
      <td style="text-align: center">No detach</td>
      <td style="text-align: center">Reverse KL ‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$w k_3$</td>
      <td style="text-align: center">No detach</td>
      <td style="text-align: center">Reverse KL ‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$w k_2$</td>
      <td style="text-align: center">No detach</td>
      <td style="text-align: center">f-divergence ‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\text{sg}(w) k_2$</td>
      <td style="text-align: center">Detach</td>
      <td style="text-align: center">Reverse KL ‚úì</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Solution</strong>: For off-policy reverse KL optimization, recommend using $w k_3$ or $\text{sg}(w) k_2$ (both have identical gradients). If using $w k_1$, gradient is unbiased but variance is higher.</p>
</blockquote>

<h4 id="pitfall-4-using-inlmath338mathend-in-reward-penalty">Pitfall 4: Using $k_3$ in Reward Penalty</h4>

<p>Although $k_3$ is value-unbiased for reverse KL and has lower variance, using it as a reward penalty causes biased policy gradient (bias term $-\nabla D_{\mathrm{KL}}(p\|q)$), potentially leading to training collapse.</p>

<blockquote>
  <p><strong>Solution</strong>: Whether on-policy or off-policy, reward penalty should only use $k_1$.</p>
</blockquote>

<h2 id="summary">Summary</h2>

<p>This post systematically analyzes the three KL estimators $k_1, k_2, k_3$ around three core questions: <strong>who to sample from</strong>, <strong>whose value to estimate</strong>, <strong>whose gradient is needed</strong>.</p>

<p><strong>Core takeaways</strong>:</p>

<ol>
  <li><strong>First clarify usage mode</strong>: KL as Loss (gradient backprop) or as Reward (stop-grad)?</li>
  <li><strong>KL as Loss (on-policy)</strong>: Use $k_2$ for reverse KL; use $k_3$ for forward KL</li>
  <li><strong>KL as Loss (off-policy)</strong>: Use $\frac{q}{\mu} k_3$ or $\text{sg}\left(\frac{q}{\mu}\right) k_2$ (note detach strategy!)</li>
  <li><strong>KL as Reward</strong>: Only use $k_1$ ($k_3$ although value-unbiased causes biased policy gradient)</li>
</ol>

<p>Clarify these points, and the three estimators will no longer be confusing.</p>

<p><strong>One-liners:</strong></p>

<ul>
  <li><strong>Only value (reward penalty):</strong> use $k_1$ or $k_3$ (both unbiased for reverse KL value); off-policy multiply by $\tfrac{q_\theta}{\mu}$.</li>
  <li><strong>Need gradients (loss):</strong>
    <ul>
      <li><strong>On-policy:</strong> reverse KL -&gt; $k_2$; forward KL -&gt; $k_3$.</li>
      <li><strong>Off-policy:</strong> reverse KL -&gt; $\tfrac{q_\theta}{\mu} k_3$ or $\text{sg}(\tfrac{q_\theta}{\mu}) k_2$ (same gradient, low variance); fallback $\tfrac{q_\theta}{\mu} k_1$ (unbiased but noisier).</li>
    </ul>
  </li>
</ul>

<p>Keep three questions clear: <strong>who do we sample from, whose value do we estimate, whose gradient do we need?</strong> Especially note: <strong>on-policy vs. off-policy choose different estimators for reverse KL</strong> ‚Äî on-policy use $k_2$, off-policy use $\tfrac{q_\theta}{\mu} k_3$ or $\text{sg}(\tfrac{q_\theta}{\mu}) k_2$.</p>

<p>Additionally, don‚Äôt forget to determine <strong>the KL usage mode</strong> before choosing an estimator:</p>
<ul>
  <li><strong>KL as reward:</strong> Constraints act on the policy indirectly through shaped advantage, with cross-timestep credit assignment capability; agent will ‚Äúplan to avoid high-KL paths‚Äù</li>
  <li><strong>KL as loss:</strong> Constraints act on the policy directly as an independent gradient term; agent will ‚Äúvisit but locally correct‚Äù</li>
</ul>

<p>This choice is more fundamental than the estimator itself, depending on whether you want constraints to be ‚Äúpreventive‚Äù or ‚Äúcorrective‚Äù.</p>

<h2 id="references">References</h2>

<ol>
  <li>
    <p>Dibya Ghosh. ‚ÄúKL Divergence for Machine Learning‚Äù. <a href="https://dibyaghosh.com/blog/probability/kldivergence">https://dibyaghosh.com/blog/probability/kldivergence</a></p>
  </li>
  <li>
    <p>John Schulman. ‚ÄúApproximating KL Divergence‚Äù. <a href="https://joschu.net/blog/kl-approx.html">https://joschu.net/blog/kl-approx.html</a></p>
  </li>
  <li>
    <p>Verl Documentation. ‚ÄúProximal Policy Optimization (PPO)‚Äù. <a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">https://verl.readthedocs.io/en/latest/algo/ppo.html</a></p>
  </li>
  <li>
    <p>Âàù‰∏É123334. RLHF/RLVR ËÆ≠ÁªÉ‰∏≠ÁöÑ KL Ëøë‰ººÊñπÊ≥ïÊµÖÊûêÔºàk1 / k2 / k3Ôºâ. <a href="https://zhuanlan.zhihu.com/p/1966872846212010437">https://zhuanlan.zhihu.com/p/1966872846212010437</a></p>
  </li>
  <li>
    <p>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. ‚ÄúRethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization‚Äù. <a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></p>
  </li>
  <li>
    <p>Yifan Zhang, Yiping Ji, Gavin Brown, et al. ‚ÄúOn the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning‚Äù. <a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></p>
  </li>
  <li>
    <p>Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro. ‚ÄúA Comedy of Estimators: On KL Regularization in RL Training of LLMs‚Äù. <a href="https://arxiv.org/abs/2512.21852">https://arxiv.org/abs/2512.21852</a></p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>

      </article>

      

      
        
      </div>

    <div class="col-lg-3 d-none d-lg-block toc-sidebar-col">
      <aside class="toc-sidebar" data-toc-sidebar id="toc-sidebar">
  <div class="toc-sidebar__header">
    <div class="toc-sidebar__title">Contents</div>
    <button class="toc-toggle-btn" aria-label="Toggle table of contents" aria-expanded="false" aria-controls="toc-content" data-toc-toggle>
      <i class="fas fa-chevron-right"></i>
    </button>
  </div>
  <nav
    id="toc-content"
    class="toc js-page-toc"
    data-toc
    data-toc-content=".toc-content"
    data-toc-headings="h2,h3"
    data-toc-min-items="2"
    aria-label="Contents"
  ></nav>
</aside>

<!-- Collapsed TOC toggle button (shown when TOC is collapsed) -->
<button class="toc-collapsed-toggle" aria-label="Show table of contents" aria-expanded="false" aria-controls="toc-content" data-toc-expand>
  <i class="fas fa-list"></i>
</button>


    </div>
  </div>
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2026 Xihuai Leo Wang. Last updated: January 07, 2026.
      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/blog_enhancements.js" type="text/javascript"></script>
  <script defer src="/assets/js/sidenotes.js" type="text/javascript"></script>
  <script defer src="/assets/js/footnote_preview.js" type="text/javascript"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>
  <script defer src="/assets/js/toc.js" type="text/javascript"></script>
  <script defer src="/assets/js/venue_filter.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax 3.x with comprehensive configuration -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        // Support all common math delimiters
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        processRefs: true,
        // Common macros for convenience
        macros: {
          RR: '\\mathbb{R}',
          NN: '\\mathbb{N}',
          ZZ: '\\mathbb{Z}',
          CC: '\\mathbb{C}',
          EE: '\\mathbb{E}',
          PP: '\\mathbb{P}',
          bm: ['\\boldsymbol{#1}', 1],
          argmax: '\\operatorname*{arg\\,max}',
          argmin: '\\operatorname*{arg\\,min}',
          sgn: '\\operatorname{sgn}',
          KL: '\\mathrm{KL}',
          Var: '\\operatorname{Var}',
          Cov: '\\operatorname{Cov}',
          tr: '\\operatorname{tr}',
          diag: '\\operatorname{diag}'
        },
        // AMS packages
        packages: {'[+]': ['ams', 'boldsymbol', 'newcommand']}
      },
      loader: {
        load: ['[tex]/ams', '[tex]/boldsymbol', '[tex]/newcommand']
      },
      options: {
        // Skip math rendering in these HTML elements
        skipHtmlTags: [
          'script', 'noscript', 'style', 'textarea', 'pre', 'code',
          'annotation', 'annotation-xml', 'kbd', 'samp', 'var'
        ],
        // Fix issues with underscores being converted to <em> by HTML
        processHtmlClass: 'mathjax-process',
        ignoreHtmlClass: 'tex2jax_ignore|no-mathjax',
        // Render math even with HTML entities
        renderActions: {
          findScript: [10, function (doc) {
            // Pre-process to fix HTML entity issues in math
            for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            }
          }, '']
        }
      },
      svg: {
        fontCache: 'global',
        scale: 1.0
      },
      chtml: {
        scale: 1.0,
        matchFontHeight: true
      },
      startup: {
        ready: function () {
          MathJax.startup.defaultReady();
          // Fix: restore underscores that might have been converted to <em>
          MathJax.startup.promise.then(() => {
            console.log('MathJax typesetting complete');
          });
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  
  <!-- Pre-processing script to protect math from Markdown/HTML interference -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Fix underscores in math that were converted to <em> by Markdown
      function fixMathUnderscores() {
        const mathContainers = document.querySelectorAll('.MathJax, .MathJax_Display, mjx-container');
        // This runs before MathJax, so we need to fix raw content
        const content = document.querySelector('.post-content, article');
        if (!content) return;
        
        // Find math delimiters and restore any <em> or <strong> inside them
        const html = content.innerHTML;
        
        // Pattern to find math blocks and restore underscore formatting
        // This is a fallback; the main protection is in the Jekyll plugin
      }
      
      // Fix HTML entities in display math blocks
      function fixHtmlEntities() {
        document.querySelectorAll('.language-plaintext.highlighter-rouge').forEach(el => {
          // Check if this looks like an HTML figure that wasn't rendered
          const text = el.textContent;
          if (text.includes('<img') || text.includes('<figure') || text.includes('<figcaption')) {
            // This is raw HTML that should be rendered - replace with actual HTML
            const temp = document.createElement('div');
            temp.innerHTML = text;
            el.replaceWith(...temp.childNodes);
          }
        });
      }
      
      fixHtmlEntities();
    });
  </script>

    <!-- Pseudocode -->
  <script defer src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js" integrity="sha256-aVkDxqyzrB+ExUsOY9PdyelkDhn/DfrjWu08aVpqNlo=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/pseudocode-init.js" type="text/javascript"></script>
    <!-- Mermaid -->
  <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.9.3/dist/mermaid.min.js"></script>
  <script defer src="/assets/js/mermaid-init.js" type="text/javascript"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2923RQZBXG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-2923RQZBXG');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    // Use the CSS-defined padding-top value to avoid layout shift
    // The navbar height is already handled by CSS: body.fixed-top-nav { padding-top: 50px; }
    let navbarHeight = $("#navbar").outerHeight(true) || 50;
    // Only set progressBar position, don't override body padding to avoid layout shift
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
