<!DOCTYPE html><html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation | Xihuai Wang's Page</title>
    <meta name="author" content="Xihuai Leo Wang" />
    <meta name="description" content="How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate." />
    <meta name="keywords" content="Reinforcement Learning, Multi-agent System, Language Model" />

    <!-- OpenGraph -->
    <meta property="og:site_name" content="Xihuai Wang's Page" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Xihuai Wang's Page | Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    <meta property="og:url" content="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html" />
    <meta property="og:description" content="How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate." />
    <meta property="og:locale" content="en_US" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation" />
    <meta name="twitter:description" content="How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate." />
    
    

    <!-- Schema.org -->
    <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Xihuai Leo Wang"
        },
        "url": "https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html",
        "@type": "WebSite",
        "description": "How we approximate KL directly affects stability. This post dissects three classic estimators k1, k2, k3, covering on-policy and off-policy, and gives practical rules for using them for reward penalties vs. losses that backpropagate.",
        "headline": "Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation",
        "sameAs": ["https://scholar.google.com/citations?user=hy6v3qUAAAAJ", "https://github.com/xihuai18"],
        "name": "Xihuai Leo Wang",
        "@context": "https://schema.org"
      }
    </script>


    <!-- DNS Prefetch & Preconnect for faster external resource loading -->
    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://fonts.googleapis.com">
    <link rel="dns-prefetch" href="https://fonts.gstatic.com">
    <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" /><!-- Pseudocode -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"><!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

    <!-- Prefetch/Preload for faster navigation -->
    <link rel="prefetch" href="/" as="document">
    <link rel="prefetch" href="/blog/" as="document">
    <link rel="prefetch" href="/publications/" as="document">
    <link rel="prefetch" href="/cv/" as="document">
    
    <!-- Instant.page for instant page loads on hover -->
    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module" defer></script>

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">Xihuai Wang's Page</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">About</a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">Blog</a>
          </li>

          <!-- CV -->
          <!-- 
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/" target="_blank"
              rel="noopener noreferrer">cv</a>
          </li> -->
          <!-- Other pages -->
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">Publications</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/cv/">CV</a>
          </li>
          <!-- Toggle theme mode -->
          <li class="nav-item toggle-container">
            <button id="light-toggle" class="nav-link" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post toc-layout">
  <header class="post-header">
    <h1 class="post-title">Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation</h1>
    <div class="post-meta-container">
      <div class="post-meta-row">
        <span class="post-date">
          <i class="far fa-calendar-alt"></i>
          December 1, 2025
        </span></div>
      <div class="post-tags-row">
        <a href="/blog/?year=2025" data-filter-link data-filter-type="year" data-filter-value="2025">
          üìÖ 2025
        </a>
          &nbsp; &middot; &nbsp;
          
            <a href="/blog/?category=reinforcement-learning" data-filter-link data-filter-type="category" data-filter-value="reinforcement-learning">
              üè∑Ô∏è reinforcement-learning
            </a>
            
          
      </div>
    </div>
  </header>

  <div class="post-links">
  

  <!-- Bilingual Links -->
  

  

  <!-- External Platform Links -->
  

  

  <!-- External Source (from plugin) -->
  

  <!-- Output links -->
  
    <a href="/reinforcement-learning/2025/12/01/kl-estimators-zh.html">‰∏≠ÊñáÁâàÊú¨</a>
  
    <a href="https://zhuanlan.zhihu.com/p/1978993413425763764" target="_blank">Áü•‰πé <img src="/assets/img/icons/zhihu.ico" style="height: 1em; vertical-align: middle;"></a>
  
</div>


  <div class="row">
    
      <div class="col-lg toc-content">
    

      <article class="post-content">
        <p><img src="/assets/img/kl-estimators/kl-estimator-en.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>How we approximate KL divergence directly affects training stability. This post systematically analyzes three estimators $k_1, k_2, k_3$ in both on-policy and off-policy scenarios, and gives practical guidelines for choosing them when KL is used as a reward penalty versus when it is used as a loss for backpropagation.</p>
</blockquote>

<h2 id="introduction-what-kl-does-in-rl">Introduction: What KL Does in RL</h2>

<p>In policy optimization (PPO, GRPO, etc.) and alignment training (RLHF/RLAIF), <strong>KL penalty</strong> keeps the new policy from drifting too far from a reference policy, preventing instability or collapse. However, implementing KL penalty involves multiple layers of choices: <strong>which estimator</strong> ($k_1$, $k_2$, $k_3$), <strong>who to sample from</strong> (on-policy vs off-policy), and <strong>how to use it</strong> (as reward shaping or as a loss for backpropagation). This post systematically dissects these choices and their interrelationships.</p>

<h3 id="forward-vs-reverse-kl">Forward vs. reverse KL</h3>

<p>Let $q_\theta$ be the current actor, $p$ the reference policy. The two directions are:</p>

<p><strong>Reverse KL:</strong>
$$
D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{x \sim q_\theta}\left[\log \frac{q_\theta(x)}{p(x)}\right]
$$</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:80%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Forward KL:</strong>
$$
D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q_\theta(x)}\right]
$$</p>

<figure style="text-align:center;">
<img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:80%;max-width:100%;" />
<figcaption style="font-size:0.9em;color:gray;">Image source: <a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>Intuition:</strong></p>
<ul>
  <li><strong>Reverse KL</strong> is mode-seeking: policy concentrates on high-probability regions of $p$, possibly sacrificing diversity.</li>
  <li><strong>Forward KL</strong> is mass-covering: policy tries to cover the support of $p$.</li>
</ul>

<p>RLHF typically uses <strong>reverse KL</strong> because we want the actor not to move too far from the reference, not necessarily to cover every mode.</p>

<h2 id="three-estimators-definitions-and-design">Three estimators: definitions and design</h2>

<p>Let $r(x) = \dfrac{p(x)}{q_\theta(x)}$. John Schulman defined three single-sample estimators:</p>

<h3 id="inlmath46mathend-the-naive-estimator">$k_1$: the naive estimator</h3>

<p>$$
k_1(x) = -\log r = \log q_\theta(x) - \log p(x)
$$</p>

<p>Direct log-ratio. It is unbiased for reverse KL, but <strong>can be negative</strong> while KL is always nonnegative, giving huge variance because positive and negative samples cancel.</p>

<h3 id="inlmath47mathend-an-f-divergence-lower-variance">$k_2$: an f-divergence, lower variance</h3>

<p>$$
k_2(x) = \frac{1}{2}(\log r)^2
$$</p>

<p><strong>Motivation:</strong> $k_1$ can be positive or negative; $k_2$ squares it so <strong>every sample is positive</strong>, each telling you how far $p$ and $q$ differ.</p>

<p><strong>Why tiny bias?</strong> $k_2$ is an <strong>f-divergence</strong> with $f(x) = \tfrac{1}{2}(\log x)^2$. All smooth f-divergences have the same second-order expansion near $q \approx p$:</p>

<p>$$
D_f(p, q_\theta) = \frac{f^{\prime\prime}(1)}{2} \theta^T F \theta + O(\theta^3)
$$</p>

<p>KL corresponds to $f(x) = -\log x$, so $f^{\prime\prime}(1) = 1$. For $k_2$, $f^{\prime\prime}(1) = 1$ as well. <strong>When policies are close, $k_2$ tracks true KL almost identically</strong>, bias only appears in higher-order terms.</p>

<h3 id="inlmath60mathend-control-variate-optimal-shape">$k_3$: control variate, ‚Äúoptimal‚Äù shape</h3>

<p>$$
k_3(x) = r - 1 - \log r
$$</p>

<p><strong>Motivation:</strong> we want <strong>unbiased and low variance</strong>. Add a <strong>control variate</strong> to $k_1$: something zero-mean and negatively correlated.</p>

<p>Because $\mathbb{E}_q[r - 1] = 1 - 1 = 0$, for any $\lambda$:</p>

<p>$$
k_1 + \lambda(r - 1) = -\log r + \lambda(r - 1)
$$</p>

<p>is still unbiased.</p>

<p><strong>Why $\lambda = 1$?</strong> By concavity of $\log$, $\log x \le x - 1$, so</p>

<p>$$
k_3 = (r - 1) - \log r \ge 0
$$</p>

<p>It is <strong>always nonnegative</strong>, avoiding the cancelation problem.</p>

<p><strong>Geometric view:</strong> $k_3$ is a <strong>Bregman divergence</strong> for $\phi(x) = -\log x$. Its tangent at $x=1$ is $y = 1 - x$, so</p>

<p>$$
\begin{aligned}
D_\phi(r, 1) &= \phi(r) - \phi(1) - \phi'(1)(r - 1) \\
&= -\log r - 0 - (-1)(r - 1) \\
&= r - 1 - \log r = k_3.
\end{aligned}
$$</p>

<p>Convexity keeps $\phi$ above its tangent, so this gap is <strong>nonnegative</strong>. As $r \to 1$, the gap shrinks quadratically $(r-1)^2$, explaining the low variance when policies are close.</p>

<h3 id="quick-comparison">Quick comparison</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Definition</th>
      <th style="text-align: center">Design idea</th>
      <th style="text-align: center">Bias (value)</th>
      <th style="text-align: center">Variance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">$\log r$</td>
      <td style="text-align: center">Naive log-ratio</td>
      <td style="text-align: center">Unbiased</td>
      <td style="text-align: center">High (can be negative)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">$\tfrac{1}{2}(\log r)^2$</td>
      <td style="text-align: center">f-divergence, KL-matching 2nd order</td>
      <td style="text-align: center">Biased (very small)</td>
      <td style="text-align: center">Low (always positive)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">$r - 1 - \log r$</td>
      <td style="text-align: center">Control variate + Bregman</td>
      <td style="text-align: center">Unbiased</td>
      <td style="text-align: center">Low (always positive)</td>
    </tr>
  </tbody>
</table>

<p>For estimating the KL <strong>value</strong>, $k_3$ is ‚Äúunbiased + low variance‚Äù; but as we‚Äôll analyze, <strong>the gradient story is completely different</strong> ‚Äî different estimators‚Äô gradients may correspond to different optimization objectives. Moreover, whether KL is added to the reward for shaping or used as a loss for direct gradient backpropagation will fundamentally affect training behavior.</p>

<h2 id="core-analysis">Core analysis</h2>

<h3 id="bias-and-variance-for-kl-values">Bias and variance for KL values</h3>

<p>Assume samples from $q_\theta$ to estimate reverse KL $D_{\mathrm{KL}}(q_\theta \| p)$.</p>

<p><strong>Unbiasedness:</strong></p>

<p>$$
\begin{aligned}
\mathbb{E}_{q}[k_1] &= \mathbb{E}_{q}\left[\log \tfrac{q}{p}\right] = D_{\mathrm{KL}}(q \| p) \quad \textbf{(unbiased)}\\
\mathbb{E}_{q}[k_3] &= \mathbb{E}_{q}[r - 1 - \log r] = 1 - 1 + D_{\mathrm{KL}}(q \| p) = D_{\mathrm{KL}}(q \| p) \quad \textbf{(unbiased)}\\
\mathbb{E}_{q}[k_2] &= \tfrac{1}{2}\mathbb{E}_{q}[(\log r)^2] \neq D_{\mathrm{KL}}(q \| p) \quad \textbf{(biased)}
\end{aligned}
$$</p>

<p><strong>Variance trade-off:</strong></p>

<p>John Schulman‚Äôs toy experiments ($q = \mathcal{N}(0,1)$, $p = \mathcal{N}(0.1,1)$, true KL = 0.005):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">20</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">0.002</td>
      <td style="text-align: center">1.42</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.42</td>
    </tr>
  </tbody>
</table>

<p>When KL is large ($p = \mathcal{N}(1,1)$, true KL = 0.5):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">bias/true</th>
      <th style="text-align: center">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">2</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">0.25</td>
      <td style="text-align: center">1.73</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.7</td>
    </tr>
  </tbody>
</table>

<p><strong>Intuition:</strong></p>
<ul>
  <li>$k_1 = -\log r$ is first-order around $r=1$, can be negative, so variance explodes when close.</li>
  <li>$k_3 = r - 1 - \log r$ is second-order near $r=1$ and always positive, so lower variance when close.</li>
  <li>When coverage is poor (heavy tails in $r$), $k_3$ can explode; then $k_1$ can be more stable.</li>
</ul>

<blockquote>
  <p><strong>Note:</strong> To estimate <strong>forward KL value</strong> $D_{\mathrm{KL}}(p \| q) = \mathbb{E}_p[\log r]$ but only sample from $q$, use importance sampling $\mathbb{E}_q[r \log r]$.</p>
</blockquote>

<h3 id="gradient-estimation-the-crucial-distinction">Gradient estimation: the crucial distinction</h3>

<p>This is the easiest part to get wrong. First analyze <strong>on-policy</strong> (samples from $q_\theta$), then extend to <strong>off-policy</strong> (samples from behavior $\mu$).</p>

<h4 id="true-gradients-for-reference">True gradients for reference</h4>

<p>Let score function $s_\theta(x) = \nabla_\theta \log q_\theta(x)$, with key property $\mathbb{E}_{q_\theta}[s_\theta] = 0$.</p>

<p><strong>Reverse KL gradient:</strong></p>

<p>$$
D_{\mathrm{KL}}(q_\theta \| p) = \int q_\theta(x) \log \frac{q_\theta(x)}{p(x)} dx
$$</p>

<p>Product rule and $\nabla_\theta q_\theta = q_\theta s_\theta$, $\nabla_\theta \log p = 0$ give</p>

<p>$$
\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_q\left[s_\theta \log \tfrac{q_\theta}{p}\right] = -\mathbb{E}_q[s_\theta \log r].
$$</p>

<p><strong>Forward KL gradient:</strong></p>

<p>$$
D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \log \frac{p(x)}{q_\theta(x)} dx
$$</p>

<p>Since $p$ is $\theta$-independent,</p>

<p>$$
\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = -\mathbb{E}_p[s_\theta] = -\mathbb{E}_q[r s_\theta] = \mathbb{E}_q[(1-r) s_\theta].
$$</p>

<p>These baselines tell us what each estimator‚Äôs expected gradient really targets.</p>

<h4 id="two-differentiation-orders">Two differentiation orders</h4>

<p>1) <strong>Grad then expectation:</strong> autograd on each sample, then batch average (what DL code actually does).
2) <strong>Expectation then grad:</strong> treat $\mathbb{E}_q[k_i]$ as a function of $\theta$ and differentiate analytically.</p>

<p>Typical code does (1).</p>

<h4 id="gradients-of-the-three-estimators-on-policy">Gradients of the three estimators (on-policy)</h4>

<p>$$
\nabla_\theta k_1 = s_\theta
$$</p>

<p>$$
\nabla_\theta k_2 = (\log r) \nabla_\theta(\log r) = (\log r)(-s_\theta) = - (\log r) s_\theta
$$</p>

<p>$$
\nabla_\theta k_3 = (1 - r) s_\theta
$$</p>

<p>Taking expectation under $q_\theta$:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">$\mathbb{E}_{q}[\nabla_\theta k_i]$</th>
      <th style="text-align: center">Equals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">$\mathbb{E}_{q}[s_\theta] = 0$</td>
      <td style="text-align: center">Zero (useless as loss)</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">$-\mathbb{E}_{q}[(\log r) s_\theta] = \nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Gradient of reverse KL</td>
    </tr>
    <tr>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">$\mathbb{E}_{q}[(1-r) s_\theta] = \nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center">Gradient of forward KL</td>
    </tr>
  </tbody>
</table>

<p><strong>Key takeaways:</strong></p>
<ul>
  <li><strong>$k_2$ gradient</strong> matches reverse KL gradient (the usual ‚Äústay near ref‚Äù objective).</li>
  <li><strong>$k_3$ gradient</strong> matches forward KL gradient (coverage objective).</li>
  <li><strong>$k_1$ gradient expectation is zero</strong> ‚Äî useless as a loss.</li>
</ul>

<h4 id="expectation-then-grad-vs-grad-then-expectation">Expectation-then-grad vs. grad-then-expectation</h4>

<p>If you first form $\mathbb{E}_q[k_i]$ and then differentiate (expectation-then-grad):</p>

<p>$$
\nabla_\theta \mathbb{E}_q[k_1] = \nabla_\theta D_{\mathrm{KL}}(q \| p), \quad \nabla_\theta \mathbb{E}_q[k_3] = \nabla_\theta D_{\mathrm{KL}}(q \| p).
$$</p>

<p>Both give reverse KL. But autograd on per-sample $k_3$ averages (grad-then-expectation) yields <strong>forward KL gradient</strong>. Same estimator, different order, different result.</p>

<h3 id="off-policy-gradients-with-importance-sampling">Off-policy gradients with importance sampling</h3>

<p>Real RL often samples from a behavior policy $\mu$ (old or mixed policy, replay buffer). To optimize <strong>reverse KL</strong> you need <strong>importance weights</strong>.</p>

<p>See also my earlier post: <a href="/reinforcement-learning/2025/11/15/three-policy-zh.html">Three-policy TRPO extension for LLM RL</a>.</p>

<h4 id="setup">Setup</h4>

<p>Define importance weight</p>

<p>$$
w(x) = \frac{q_\theta(x)}{\mu(x)}.
$$</p>

<p>Using batch loss $w(x) k_i(x)$ with autograd, what gradients do we get?</p>

<p>A key difference:</p>
<ul>
  <li>Previously expectations were under $q_\theta$ (depends on $\theta$).</li>
  <li>Now expectations are under $\mu$ (independent of $\theta$).</li>
</ul>

<h4 id="crucial-observation-the-two-orders-coincide">Crucial observation: the two orders coincide</h4>

<p>Because $\mu$ is $\theta$-independent,</p>

<p>$$
\nabla_\theta \mathbb{E}_{\mu}[f_\theta] = \mathbb{E}_{\mu}[\nabla_\theta f_\theta].
$$</p>

<p>So autograd on sample means (grad-then-expectation) equals expectation-then-grad. For $k_1$ and $k_3$, both value-unbiased for reverse KL, their gradient expectations also match reverse KL.</p>

<h4 id="value-unbiasedness-remains">Value unbiasedness remains</h4>

<p>By $\mathbb{E}_\mu[w f] = \mathbb{E}_q[f]$:</p>

<p>$$
\mathbb{E}_\mu[w k_1] = D_{\mathrm{KL}}(q_\theta \| p), \quad \mathbb{E}_\mu[w k_3] = D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{(unbiased)}
$$</p>

<p>$$
\mathbb{E}_\mu[w k_2] = \mathbb{E}_{q_\theta}[k_2] \neq D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{(biased)}
$$</p>

<h4 id="gradients-with-weights">Gradients with weights</h4>

<p>Gradient of weight: $\nabla_\theta w = w s_\theta$. Using product rule:</p>

<p>$$
\nabla_\theta(w k_1) = w s_\theta (k_1 + 1)
$$
$$
\nabla_\theta(w k_2) = w s_\theta (k_2 - \log r)
$$
$$
\nabla_\theta(w k_3) = w s_\theta (k_3 + 1 - r) = w s_\theta k_1
$$</p>

<p>Which give expected gradients:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Weighted estimator</th>
      <th style="text-align: center">Value target</th>
      <th style="text-align: center">Expected gradient</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$\tfrac{q_\theta}{\mu} k_1$</td>
      <td style="text-align: center">$D_{\mathrm{KL}}(q_\theta \| p)$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$ (reverse KL) ‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$\tfrac{q_\theta}{\mu} k_2$</td>
      <td style="text-align: center">$\mathbb{E}_q[k_2]$ (f-divergence)</td>
      <td style="text-align: center">$\nabla_\theta \mathbb{E}_q[k_2]$, not reverse KL ‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\text{sg}\left(\tfrac{q_\theta}{\mu}\right) k_2$</td>
      <td style="text-align: center">$\mathbb{E}_q[k_2]$ (f-divergence)</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$ (reverse KL) ‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$\tfrac{q_\theta}{\mu} k_3$</td>
      <td style="text-align: center">$D_{\mathrm{KL}}(q_\theta \| p)$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$ (reverse KL) ‚úì</td>
    </tr>
  </tbody>
</table>

<p><strong>Interesting reversal vs. on-policy:</strong></p>
<ul>
  <li>On-policy: $k_2$ as loss gives reverse KL gradient; $k_1$ gradient is zero.</li>
  <li>Off-policy + weights: $\tfrac{q}{\mu}k_1$ and $\tfrac{q}{\mu}k_3$ give reverse KL gradients; $\tfrac{q}{\mu}k_2$ (with weight in grad) fails.</li>
  <li>Detaching the weight makes $\text{sg}(\tfrac{q}{\mu}) k_2$ also give reverse KL gradient.</li>
</ul>

<h4 id="variance-of-the-three-unbiased-off-policy-gradient-estimators">Variance of the three unbiased off-policy gradient estimators</h4>

<p>Unbiased reverse-KL gradient estimators (off-policy + IS):</p>

<p>$$
L_1 = w k_1, \quad L_2 = \bar w k_2, \quad L_3 = w k_3,
$$</p>

<p>With $w = \tfrac{q_\theta}{\mu}$, $\bar w = \mathrm{sg}(w)$. Using $\nabla_\theta w = w s_\theta$, $\nabla_\theta k_1 = s_\theta$, $\nabla_\theta k_2 = k_1 s_\theta$, $\nabla_\theta k_3 = (1-r) s_\theta$:</p>

<p>$$
\begin{aligned}
g_1 &= w s_\theta (k_1+1),\\
g_2 &= w s_\theta k_1,\\
g_3 &= w s_\theta k_1.
\end{aligned}
$$</p>

<p>So <strong>$g_2 \equiv g_3$</strong>. Only two distinct variance behaviors: $g_1$ vs. $g_\star := g_2 = g_3$.</p>

<p>Let $A = w s_\theta, B = k_1$. Then</p>

<p>$$
g_1 = A(B+1), \quad g_\star = A B.
$$</p>

<p>Variance difference:</p>

<p>$$
\boxed{\mathrm{Var}_\mu(g_1) - \mathrm{Var}_\mu(g_\star) = \mathbb{E}_\mu[A^2(2B+1)]} = \mathbb{E}_\mu\big[w^2 s_\theta^2 (2k_1+1)\big].
$$</p>

<p>In the typical KL-penalty regime $q_\theta \approx p \approx \mu$, write $r = 1 + \varepsilon$, $\lvert\varepsilon\rvert \ll 1$, so $k_1 \approx -\varepsilon$, $2k_1+1 \approx 1 - 2\varepsilon > 0$. Thus $\mathrm{Var}(g_1) > \mathrm{Var}(g_\star)$.</p>

<p>Intuition:</p>
<ul>
  <li>$g_1$ includes an $O(1)$ zero-mean noise term $w s_\theta$.</li>
  <li>$g_\star$ cancels that term; remaining magnitude is $O(\varepsilon)$, giving much lower variance.</li>
</ul>

<p>Table summary:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Estimator</th>
      <th style="text-align: center">Gradient rv</th>
      <th style="text-align: center">Scale ($r\approx1$)</th>
      <th style="text-align: center">Variance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$w k_1$</td>
      <td style="text-align: center">$w s_\theta (k_1+1)$</td>
      <td style="text-align: center">$O(1)$</td>
      <td style="text-align: center">High</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mathrm{sg}(w) k_2$</td>
      <td style="text-align: center">$w s_\theta k_1$</td>
      <td style="text-align: center">$O(\varepsilon)$</td>
      <td style="text-align: center">Low</td>
    </tr>
    <tr>
      <td style="text-align: center">$w k_3$</td>
      <td style="text-align: center">$w s_\theta k_1$</td>
      <td style="text-align: center">$O(\varepsilon)$</td>
      <td style="text-align: center">Low</td>
    </tr>
  </tbody>
</table>

<p>Conclusion: off-policy IS with reverse-KL gradients has three unbiased options: $w k_1$, $\bar w k_2$, $w k_3$. The latter two are identical in gradient and variance and are preferred; $w k_1$ is unbiased but noisier.</p>

<p><strong>When far off-policy:</strong> If $w$ explodes (little overlap), any $\tfrac{q}{\mu}$ method suffers. Then the variance advantage of $k_3$ over $k_1$ is not guaranteed; clipping/regularization becomes necessary.</p>

<h3 id="gradient-cheat-sheet">Gradient cheat sheet</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Sampling</th>
      <th style="text-align: center">Loss</th>
      <th style="text-align: center">$\mathbb{E}[\nabla_\theta \text{Loss}]$</th>
      <th style="text-align: center">Optimizes</th>
      <th style="text-align: center">Right for reverse KL?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$q$ (on)</td>
      <td style="text-align: center">$k_1$</td>
      <td style="text-align: center">$\mathbb{E}_q[s_\theta] = 0$</td>
      <td style="text-align: center">None (zero grad)</td>
      <td style="text-align: center">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$q$ (on)</td>
      <td style="text-align: center">$k_2$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$q$ (on)</td>
      <td style="text-align: center">$k_3$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center">Forward KL</td>
      <td style="text-align: center">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu$ (off)</td>
      <td style="text-align: center">$\tfrac{q}{\mu} k_1$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì (higher var)</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu$ (off)</td>
      <td style="text-align: center">$\tfrac{q}{\mu} k_2$</td>
      <td style="text-align: center">$\nabla_\theta \mathbb{E}_q[k_2]$</td>
      <td style="text-align: center">f-divergence (not KL)</td>
      <td style="text-align: center">‚úó</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu$ (off)</td>
      <td style="text-align: center">$\text{sg}\left(\tfrac{q}{\mu}\right) k_2$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu$ (off)</td>
      <td style="text-align: center">$\tfrac{q}{\mu} k_3$</td>
      <td style="text-align: center">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">Reverse KL</td>
      <td style="text-align: center">‚úì (recommended, low var)</td>
    </tr>
  </tbody>
</table>

<p><strong>Key conclusions:</strong>
1) <strong>On-policy reverse KL:</strong> use $k_2$ (only correct choice).
2) <strong>Off-policy reverse KL:</strong> three correct options: $\tfrac{q}{\mu} k_1$ (unbiased, higher var); $\text{sg}(\tfrac{q}{\mu}) k_2$ (unbiased, equals next); $\tfrac{q}{\mu} k_3$ (unbiased, lower var; equals previous).
3) <strong>$\tfrac{q}{\mu} k_2$ with weight in grad is wrong</strong> for reverse KL.</p>

<p>However, before choosing an estimator, there‚Äôs a more fundamental question to answer: <strong>should KL be added to rewards, or be part of the loss?</strong> This choice fundamentally affects optimization behavior and credit assignment.</p>

<h2 id="two-ways-to-use-kl-as-reward-vs-as-loss">Two Ways to Use KL: As Reward vs. As Loss</h2>

<p>In practice, KL penalty can be used in two fundamentally different ways: added to rewards for shaping (no gradient backpropagation needed), or as part of the loss for backpropagation (gradient needed).</p>

<p>These two approaches may seem like just a <code class="language-plaintext highlighter-rouge">detach</code> difference in code, but they correspond to completely different optimization behaviors.</p>

<h3 id="definitions">Definitions</h3>

<p><strong>KL as Reward (stop-gradient):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kl</span> <span class="o">=</span> <span class="nf">compute_kl</span><span class="p">(</span><span class="n">log_prob_q</span><span class="p">,</span> <span class="n">log_prob_p</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">shaped_reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>
</code></pre></div></div>

<p>Use shaped reward for standard actor-critic updates.</p>

<p><strong>KL as Loss (backprop):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantage</span> <span class="o">*</span> <span class="n">log_prob</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>  <span class="c1"># kl participates in gradient
</span></code></pre></div></div>

<p>Critic only learns environment value; KL is a regularization term for the actor that backpropagates gradients.</p>

<h3 id="key-difference-1-optimization-target">Key Difference 1: Optimization Target</h3>

<p><strong>KL as Reward:</strong> Optimizes a <strong>regularized new MDP</strong> where the reward function becomes $\tilde{r}(s,a) = r(s,a) - \beta \cdot \text{KL}(s)$.</p>

<p><strong>KL as Loss:</strong> Optimizes the <strong>original task + supervised regularization</strong>; KL doesn‚Äôt change the MDP definition, it‚Äôs just an external constraint term.</p>

<p><strong>Intuition:</strong> The former ‚Äúchanges the game rules‚Äù; the latter ‚Äúadds constraints under the original rules‚Äù.</p>

<h3 id="key-difference-2-actor-gradient">Key Difference 2: Actor Gradient</h3>

<p><strong>KL as Reward:</strong> Single policy gradient, KL influence is <strong>reflected indirectly through advantage</strong>:</p>

<p>$$
g_{\text{reward}} = \mathbb{E}\left[s_\theta \cdot \tilde{A}_t\right], \quad \tilde{A}_t \text{ based on } (r_t - \beta \cdot \text{KL}_t)
$$</p>

<p><strong>KL as Loss:</strong> Gradient splits into two independent paths:</p>

<p>$$
g_{\text{loss}} = \underbrace{\mathbb{E}\left[s_\theta \cdot A_t^{\text{env}}\right]}_{\text{RL gradient}} + \underbrace{\beta \cdot \mathbb{E}\left[\nabla_\theta \text{KL}_t\right]}_{\text{KL explicit gradient}}
$$</p>

<p><strong>Key distinction:</strong> Is KL‚Äôs force ‚Äúmultiplied on advantage‚Äù or ‚Äúa separate force‚Äù? The latter‚Äôs KL gradient is deterministic, unaffected by critic quality.</p>

<h3 id="key-difference-3-critic-learning-target">Key Difference 3: Critic Learning Target</h3>

<p><strong>KL as Reward:</strong> Critic learns mixed value</p>

<p>$$
V^{\text{reg}}(s) = \mathbb{E}\left[\sum_t \gamma^t (r_t - \beta \cdot \text{KL}_t)\right]
$$</p>

<p><strong>KL as Loss:</strong> Critic only learns environment value</p>

<p>$$
V^{\text{env}}(s) = \mathbb{E}\left[\sum_t \gamma^t r_t\right]
$$</p>

<p>The latter has cleaner separation, making it easier to monitor task return and KL divergence separately.</p>

<h3 id="key-difference-4-credit-assignment">Key Difference 4: Credit Assignment</h3>

<p>Consider a scenario: first few steps are routing behavior, final step has high reward but also high KL.</p>

<p><strong>KL as Reward:</strong> The large KL at the terminal state is <strong>propagated back to all previous steps</strong> through TD, so the policy tends to <strong>fundamentally avoid</strong> high-KL regions ‚Äî this is ‚Äúplanning-based KL budget allocation‚Äù.</p>

<p><strong>KL as Loss:</strong> The terminal state‚Äôs KL only appears in that state‚Äôs gradient term; the policy is still willing to <strong>visit high-reward regions but locally correct</strong> behavior.</p>

<h3 id="summary">Summary</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Dimension</th>
      <th style="text-align: center">KL as Reward (stop-grad)</th>
      <th style="text-align: center">KL as Loss (backprop)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Optimization target</td>
      <td style="text-align: center">Regularized new MDP</td>
      <td style="text-align: center">Original task + supervised regularization</td>
    </tr>
    <tr>
      <td style="text-align: center">Actor gradient</td>
      <td style="text-align: center">Single PG, based on shaped advantage</td>
      <td style="text-align: center">RL gradient + explicit KL gradient</td>
    </tr>
    <tr>
      <td style="text-align: center">Critic</td>
      <td style="text-align: center">Learns $V^{\text{reg}}$: reward + KL mixed</td>
      <td style="text-align: center">Learns $V^{\text{env}}$: only environment reward</td>
    </tr>
    <tr>
      <td style="text-align: center">Credit Assignment</td>
      <td style="text-align: center">Multi-step backprop, planning-capable</td>
      <td style="text-align: center">Local per-state, no planning</td>
    </tr>
  </tbody>
</table>

<p><strong>One-liner:</strong> KL as reward makes the agent ‚Äúplan to avoid high-KL paths‚Äù ‚Äî constraints are more global and thorough; KL as loss makes the agent ‚Äúvisit but locally correct‚Äù ‚Äî constraints are more local and flexible. The choice depends on whether you need cross-timestep KL budget allocation capability, and whether you want constraints to be ‚Äúpreventive‚Äù or ‚Äúcorrective‚Äù.</p>

<h2 id="rl-practice-guide">RL practice guide</h2>

<p>Combining the preceding analysis of ‚Äúestimator mathematical properties‚Äù and ‚Äúusage modes‚Äù, this section provides practical recommendations for specific scenarios.</p>

<h3 id="kl-as-reward-penalty-no-gradient-needed">KL as reward penalty (no gradient needed)</h3>

<p>When KL is a scalar penalty in rewards, we only need accurate <strong>values</strong>, no backprop. Refer to the earlier section on ‚ÄúBias and variance for KL values‚Äù.</p>

<p><strong>Recommend:</strong></p>
<ul>
  <li>Use <strong>$k_1$</strong> or <strong>$k_3$</strong> (both unbiased for reverse KL value).</li>
  <li>When policies are close, $k_3$ is typically lower variance.</li>
  <li>With poor coverage or heavy tails, $k_1$ is more robust.</li>
  <li>Off-policy: multiply by $\tfrac{q_\theta}{\mu}$.</li>
</ul>

<blockquote>
  <p>For a <strong>forward KL penalty</strong>, use $\mathbb{E}_q[r \log r]$ or (if sampling from $p$) $\mathbb{E}_p[\log r]$.</p>
</blockquote>

<h3 id="kl-as-loss-needs-gradients">KL as loss (needs gradients)</h3>

<h4 id="on-policy-optimize-reverse-kl-most-common">On-policy: optimize reverse KL (most common)</h4>

<p>Goal: keep actor near reference.</p>

<p><strong>Use $k_2$ as loss.</strong></p>

<p>$$
\mathcal{L}_{k_2} = \tfrac{1}{2}(\log r)^2
$$</p>

<p>Then $\mathbb{E}_q[\nabla k_2] = \nabla_\theta D_{\mathrm{KL}}(q \| p)$.</p>

<h4 id="on-policy-optimize-forward-kl-coverage">On-policy: optimize forward KL (coverage)</h4>

<p>Goal: cover the reference distribution (offline RL, imitation, etc.).</p>

<p><strong>Use $k_3$ as loss.</strong> Autograd on sample means gives $\mathbb{E}_q[(1-r) s_\theta] = \nabla_\theta D_{\mathrm{KL}}(p \| q)$.</p>

<h4 id="off-policy-optimize-reverse-kl">Off-policy: optimize reverse KL</h4>

<p>Goal: samples from behavior $\mu$, still optimize reverse KL.</p>

<p><strong>Recommended:</strong> $\dfrac{q_\theta}{\mu} k_3$ or $\mathrm{sg}\left(\dfrac{q_\theta}{\mu}\right) k_2$ (identical gradients).</p>

<p>$$
\mathcal{L} = \dfrac{q_\theta(x)}{\mu(x)} \Big( \dfrac{p(x)}{q_\theta(x)} - 1 - \log \dfrac{p(x)}{q_\theta(x)} \Big)
$$</p>

<p>or</p>

<p>$$
\mathcal{L} = \mathrm{sg}\left(\dfrac{q_\theta(x)}{\mu(x)}\right) \cdot \tfrac{1}{2}\left(\log \dfrac{p(x)}{q_\theta(x)}\right)^2.
$$</p>

<ul>
  <li>Gradients are unbiased.</li>
  <li>When $q_\theta \approx p$, both have much lower variance.</li>
</ul>

<p><strong>Fallback:</strong> $\dfrac{q_\theta}{\mu} k_1$ (unbiased but higher variance).</p>

<p><strong>Avoid:</strong> $\dfrac{q_\theta}{\mu} k_2$ with weight in gradient ‚Äî biased for reverse KL.</p>

<h2 id="grab-and-use-crib-sheet">‚ÄúGrab-and-use‚Äù crib sheet</h2>

<p>The table below provides recommended estimator choices along three dimensions: ‚Äútarget KL direction‚Äù √ó ‚Äúsampling source‚Äù √ó ‚Äúusage mode‚Äù. ‚ÄúFor <strong>value</strong>‚Äù corresponds to KL as reward penalty (no gradient needed); ‚ÄúFor <strong>gradient</strong>‚Äù corresponds to KL as loss (gradient backpropagation needed).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Target</th>
      <th style="text-align: center">Sampling</th>
      <th style="text-align: center">For value (KL as Reward)</th>
      <th style="text-align: center">For gradient (KL as Loss)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Reverse KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">$q$ (on-policy)</td>
      <td style="text-align: center">$k_1$ or $k_3$ (unbiased)</td>
      <td style="text-align: center">$k_2$</td>
    </tr>
    <tr>
      <td style="text-align: center">Reverse KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center">$\mu$ (off-policy)</td>
      <td style="text-align: center">$\tfrac{q}{\mu} k_1$ or $\tfrac{q}{\mu} k_3$ (unbiased)</td>
      <td style="text-align: center">$\tfrac{q}{\mu} k_3$ (recommended) or $\text{sg}(\tfrac{q}{\mu}) k_2$</td>
    </tr>
    <tr>
      <td style="text-align: center">Forward KL $D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center">$q$</td>
      <td style="text-align: center">$\mathbb{E}_q[r\log r]$</td>
      <td style="text-align: center">$k_3$</td>
    </tr>
  </tbody>
</table>

<h2 id="common-implementation-traps">Common implementation traps</h2>

<p><strong>Trap 1: Using $k_1$ directly as loss (on-policy)</strong></p>

<p>When KL is used as a loss, $k_1$ gradient expectation is zero ($\mathbb{E}_q[s_\theta]=0$); as a loss it does nothing.</p>

<blockquote>
  <p><strong>Fix:</strong> First clarify the KL usage mode. For reward shaping (no gradient needed), both $k_1$ and $k_3$ work; for losses (gradient needed), use $k_2$ (reverse KL) or $k_3$ (forward KL) on-policy.</p>
</blockquote>

<p><strong>Trap 2: Mixing up $k_3$ value-unbiasedness vs. gradient target</strong></p>

<p>$k_3$ is value-unbiased for reverse KL, but its <strong>gradient</strong> is <strong>forward KL</strong>. If you want reverse KL and backprop $k_3$, you are actually optimizing forward KL.</p>

<blockquote>
  <p><strong>Fix:</strong> be explicit: reverse KL -&gt; $k_2$; forward KL -&gt; $k_3$.</p>
</blockquote>

<p><strong>Trap 3: Heavy-tailed $r$ blows up variance</strong></p>

<p>If $r = p/q$ has extreme values, $k_3$ variance can explode.</p>

<blockquote>
  <p><strong>Fix:</strong> enforce KL constraint or clip $r$.</p>
</blockquote>

<p><strong>Trap 4: Off-policy but still using $k_2$ or $\tfrac{q_\theta}{\mu} k_2$ (with grad on weight)</strong></p>

<p>If $\mu \neq q_\theta$:</p>
<ul>
  <li>Plain $k_2$ (no weight): expectation is under $\mu$, estimator fails.</li>
  <li>$\tfrac{q_\theta}{\mu} k_2$ with weight in grad: gradient is biased (f-divergence), not reverse KL.</li>
</ul>

<blockquote>
  <p><strong>Fix:</strong> off-policy reverse KL -&gt; use $\tfrac{q_\theta}{\mu} k_3$ (recommended), $\text{sg}(\tfrac{q_\theta}{\mu}) k_2$, or $\tfrac{q_\theta}{\mu} k_1$.</p>
</blockquote>

<p><strong>Trap 5: Wrong detach on importance weights</strong></p>

<p>$w = q_\theta / \mu$ often comes from <code class="language-plaintext highlighter-rouge">log_prob_q - log_prob_mu</code> then <code class="language-plaintext highlighter-rouge">exp</code>. Detaching $w$ matters:</p>

<ul>
  <li><strong>Using $k_1$ or $k_3$:</strong> $w$ <strong>must participate in gradient</strong> (do not detach), otherwise you drop $\nabla_\theta w = w s_\theta$ and get wrong gradients.</li>
  <li><strong>Using $k_2$:</strong> <strong>detach $w$</strong> to get reverse KL gradient. If $w$ stays in the graph, you get f-divergence gradient instead.</li>
</ul>

<blockquote>
  <p><strong>Summary:</strong> match estimator with the right detach strategy.</p>
</blockquote>

<h2 id="summary-1">Summary</h2>

<p><strong>One-liners:</strong></p>

<ul>
  <li><strong>Only value (reward penalty):</strong> use $k_1$ or $k_3$ (both unbiased for reverse KL value); off-policy multiply by $\tfrac{q_\theta}{\mu}$.</li>
  <li><strong>Need gradients (loss):</strong>
    <ul>
      <li><strong>On-policy:</strong> reverse KL -&gt; $k_2$; forward KL -&gt; $k_3$.</li>
      <li><strong>Off-policy:</strong> reverse KL -&gt; $\tfrac{q_\theta}{\mu} k_3$ or $\text{sg}(\tfrac{q_\theta}{\mu}) k_2$ (same gradient, low variance); fallback $\tfrac{q_\theta}{\mu} k_1$ (unbiased but noisier).</li>
    </ul>
  </li>
</ul>

<p>Keep three questions clear: <strong>who do we sample from, whose value do we estimate, whose gradient do we need?</strong> Especially note: <strong>on-policy vs. off-policy choose different estimators for reverse KL</strong> ‚Äî on-policy use $k_2$, off-policy use $\tfrac{q_\theta}{\mu} k_3$ or $\text{sg}(\tfrac{q_\theta}{\mu}) k_2$.</p>

<p>Additionally, don‚Äôt forget to determine <strong>the KL usage mode</strong> before choosing an estimator:</p>
<ul>
  <li><strong>KL as reward:</strong> Constraints act on the policy indirectly through shaped advantage, with cross-timestep credit assignment capability; agent will ‚Äúplan to avoid high-KL paths‚Äù</li>
  <li><strong>KL as loss:</strong> Constraints act on the policy directly as an independent gradient term; agent will ‚Äúvisit but locally correct‚Äù</li>
</ul>

<p>This choice is more fundamental than the estimator itself, depending on whether you want constraints to be ‚Äúpreventive‚Äù or ‚Äúcorrective‚Äù.</p>

<h2 id="references">References</h2>

<ol>
  <li>Dibya Ghosh. ‚ÄúKL Divergence for Machine Learning‚Äù. <a href="https://dibyaghosh.com/blog/probability/kldivergence">https://dibyaghosh.com/blog/probability/kldivergence</a></li>
  <li>John Schulman. ‚ÄúApproximating KL Divergence‚Äù. <a href="https://joschu.net/blog/kl-approx.html">https://joschu.net/blog/kl-approx.html</a></li>
  <li>Verl Documentation. ‚ÄúProximal Policy Optimization (PPO)‚Äù. <a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">https://verl.readthedocs.io/en/latest/algo/ppo.html</a></li>
  <li>Âàù‰∏É123334. ‚ÄúRLHF/RLVR ËÆ≠ÁªÉ‰∏≠ÁöÑ KL Ëøë‰ººÊñπÊ≥ïÊµÖÊûêÔºàk1 / k2 / k3)‚Äù. <a href="https://zhuanlan.zhihu.com/p/1966872846212010437">https://zhuanlan.zhihu.com/p/1966872846212010437</a></li>
  <li>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. ‚ÄúRethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization‚Äù. <a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></li>
  <li>Yifan Zhang, Yiping Ji, Gavin Brown, et al. ‚ÄúOn the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning‚Äù. <a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding {KL} Divergence Estimators in {RL}: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
	<span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
	<span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
	<span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>

      </article>

      

      
        
      </div>

    <div class="col-lg-3 d-none d-lg-block toc-sidebar-col">
      <aside class="toc-sidebar" data-toc-sidebar id="toc-sidebar">
  <div class="toc-sidebar__header">
    <div class="toc-sidebar__title">Contents</div>
    <button class="toc-toggle-btn" aria-label="Toggle table of contents" aria-expanded="false" aria-controls="toc-content" data-toc-toggle>
      <i class="fas fa-chevron-right"></i>
    </button>
  </div>
  <nav
    id="toc-content"
    class="toc js-page-toc"
    data-toc
    data-toc-content=".toc-content"
    data-toc-headings="h2,h3"
    data-toc-min-items="2"
    aria-label="Contents"
  ></nav>
</aside>

<!-- Collapsed TOC toggle button (shown when TOC is collapsed) -->
<button class="toc-collapsed-toggle" aria-label="Show table of contents" aria-expanded="false" aria-controls="toc-content" data-toc-expand>
  <i class="fas fa-list"></i>
</button>


    </div>
  </div>
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2026 Xihuai Leo Wang. Last updated: January 02, 2026.
      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/blog_enhancements.js" type="text/javascript"></script>
  <script defer src="/assets/js/sidenotes.js" type="text/javascript"></script>
  <script defer src="/assets/js/footnote_preview.js" type="text/javascript"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>
  <script defer src="/assets/js/toc.js" type="text/javascript"></script>
  <script defer src="/assets/js/venue_filter.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax 3.x with comprehensive configuration -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        // Support all common math delimiters
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        processRefs: true,
        // Common macros for convenience
        macros: {
          RR: '\\mathbb{R}',
          NN: '\\mathbb{N}',
          ZZ: '\\mathbb{Z}',
          CC: '\\mathbb{C}',
          EE: '\\mathbb{E}',
          PP: '\\mathbb{P}',
          bm: ['\\boldsymbol{#1}', 1],
          argmax: '\\operatorname*{arg\\,max}',
          argmin: '\\operatorname*{arg\\,min}',
          sgn: '\\operatorname{sgn}',
          KL: '\\mathrm{KL}',
          Var: '\\operatorname{Var}',
          Cov: '\\operatorname{Cov}',
          tr: '\\operatorname{tr}',
          diag: '\\operatorname{diag}'
        },
        // AMS packages
        packages: {'[+]': ['ams', 'boldsymbol', 'newcommand']}
      },
      loader: {
        load: ['[tex]/ams', '[tex]/boldsymbol', '[tex]/newcommand']
      },
      options: {
        // Skip math rendering in these HTML elements
        skipHtmlTags: [
          'script', 'noscript', 'style', 'textarea', 'pre', 'code',
          'annotation', 'annotation-xml', 'kbd', 'samp', 'var'
        ],
        // Fix issues with underscores being converted to <em> by HTML
        processHtmlClass: 'mathjax-process',
        ignoreHtmlClass: 'tex2jax_ignore|no-mathjax',
        // Render math even with HTML entities
        renderActions: {
          findScript: [10, function (doc) {
            // Pre-process to fix HTML entity issues in math
            for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            }
          }, '']
        }
      },
      svg: {
        fontCache: 'global',
        scale: 1.0
      },
      chtml: {
        scale: 1.0,
        matchFontHeight: true
      },
      startup: {
        ready: function () {
          MathJax.startup.defaultReady();
          // Fix: restore underscores that might have been converted to <em>
          MathJax.startup.promise.then(() => {
            console.log('MathJax typesetting complete');
          });
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  
  <!-- Pre-processing script to protect math from Markdown/HTML interference -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Fix underscores in math that were converted to <em> by Markdown
      function fixMathUnderscores() {
        const mathContainers = document.querySelectorAll('.MathJax, .MathJax_Display, mjx-container');
        // This runs before MathJax, so we need to fix raw content
        const content = document.querySelector('.post-content, article');
        if (!content) return;
        
        // Find math delimiters and restore any <em> or <strong> inside them
        const html = content.innerHTML;
        
        // Pattern to find math blocks and restore underscore formatting
        // This is a fallback; the main protection is in the Jekyll plugin
      }
      
      // Fix HTML entities in display math blocks
      function fixHtmlEntities() {
        document.querySelectorAll('.language-plaintext.highlighter-rouge').forEach(el => {
          // Check if this looks like an HTML figure that wasn't rendered
          const text = el.textContent;
          if (text.includes('<img') || text.includes('<figure') || text.includes('<figcaption')) {
            // This is raw HTML that should be rendered - replace with actual HTML
            const temp = document.createElement('div');
            temp.innerHTML = text;
            el.replaceWith(...temp.childNodes);
          }
        });
      }
      
      fixHtmlEntities();
    });
  </script>

    <!-- Pseudocode -->
  <script defer src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js" integrity="sha256-aVkDxqyzrB+ExUsOY9PdyelkDhn/DfrjWu08aVpqNlo=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/pseudocode-init.js" type="text/javascript"></script>
    <!-- Mermaid -->
  <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.9.3/dist/mermaid.min.js"></script>
  <script defer src="/assets/js/mermaid-init.js" type="text/javascript"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2923RQZBXG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-2923RQZBXG');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    // Use the CSS-defined padding-top value to avoid layout shift
    // The navbar height is already handled by CSS: body.fixed-top-nav { padding-top: 50px; }
    let navbarHeight = $("#navbar").outerHeight(true) || 50;
    // Only set progressBar position, don't override body padding to avoid layout shift
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
