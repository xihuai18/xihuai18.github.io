<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>简单理解 RL 中的 KL 散度估计器：从数值估计到梯度估计 | Xihuai Wang's Page</title>
    <meta name="author" content="Xihuai Leo Wang" />
    <meta name="description" content="在强化学习中，KL 散度的估计方式直接影响训练稳定性。本文系统剖析三种经典估计器 k1, k2, k3 的性质差异，涵盖 on-policy 与 off-policy 两种场景，并给出「用于 reward 惩罚」与「用于 loss 回传」时的选型指南。" />
    <meta name="keywords" content="Reinforcement Learning, Multi-agent System" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🤖</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-cn.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/">Xihuai Wang's Page</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about</a>
          </li>
          

          <!-- CV -->
          <!-- 
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/" target="_blank"
              rel="noopener noreferrer">cv</a>
          </li> -->
          <!-- Other pages -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">Xihuai's Blog</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">publications</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/cv/">cv</a>
          </li>

          <!-- Toogle theme mode -->
          <li class="toggle-container">
            <button id="light-toggle" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">简单理解 RL 中的 KL 散度估计器：从数值估计到梯度估计</h1>
    <p class="post-meta">December 1, 2025</p>
    <p class="post-tags">
  <a href="/blog/2025"> 📅 2025 </a>
      &nbsp; &middot; &nbsp;
        <a href="/blog/category/reinforcement-learning">
          🏷️ reinforcement-learning</a> &nbsp;
          

    </p>
  </header>

  <article class="post-content">
    <ul id="markdown-toc">
  <li><a href="#引言kl-散度在强化学习中的角色" id="markdown-toc-引言kl-散度在强化学习中的角色">引言：KL 散度在强化学习中的角色</a>    <ul>
      <li><a href="#正向-kl-与反向-kl-的区别" id="markdown-toc-正向-kl-与反向-kl-的区别">正向 KL 与反向 KL 的区别</a></li>
    </ul>
  </li>
  <li><a href="#三种估计器的定义与设计原理" id="markdown-toc-三种估计器的定义与设计原理">三种估计器的定义与设计原理</a>    <ul>
      <li><a href="#k_1最朴素的估计器" id="markdown-toc-k_1最朴素的估计器">$k_1$：最朴素的估计器</a></li>
      <li><a href="#k_2基于-f-散度的低方差估计器" id="markdown-toc-k_2基于-f-散度的低方差估计器">$k_2$：基于 f-散度的低方差估计器</a></li>
      <li><a href="#k_3控制变量法构造的最优估计器" id="markdown-toc-k_3控制变量法构造的最优估计器">$k_3$：控制变量法构造的「最优」估计器</a></li>
      <li><a href="#三者对比总结" id="markdown-toc-三者对比总结">三者对比总结</a></li>
    </ul>
  </li>
  <li><a href="#核心分析" id="markdown-toc-核心分析">核心分析</a>    <ul>
      <li><a href="#估计-kl-数值时的偏差与方差" id="markdown-toc-估计-kl-数值时的偏差与方差">估计 KL 数值时的偏差与方差</a></li>
      <li><a href="#估计-kl-梯度时的关键区分" id="markdown-toc-估计-kl-梯度时的关键区分">估计 KL 梯度时的关键区分</a>        <ul>
          <li><a href="#正向与反向-kl-真梯度的推导" id="markdown-toc-正向与反向-kl-真梯度的推导">正向与反向 KL 真梯度的推导</a></li>
          <li><a href="#两种求导顺序" id="markdown-toc-两种求导顺序">两种求导顺序</a></li>
          <li><a href="#三种估计器的梯度推导" id="markdown-toc-三种估计器的梯度推导">三种估计器的梯度推导</a></li>
          <li><a href="#先期望后梯度vs先梯度后期望" id="markdown-toc-先期望后梯度vs先梯度后期望">「先期望后梯度」vs「先梯度后期望」</a></li>
        </ul>
      </li>
      <li><a href="#扩展从行为策略-mu-采样时的-kl-梯度估计" id="markdown-toc-扩展从行为策略-mu-采样时的-kl-梯度估计">扩展：从行为策略 $\mu$ 采样时的 KL 梯度估计</a>        <ul>
          <li><a href="#设置与记号" id="markdown-toc-设置与记号">设置与记号</a></li>
          <li><a href="#关键观察两种求导顺序的等价性" id="markdown-toc-关键观察两种求导顺序的等价性">关键观察：两种求导顺序的等价性</a></li>
          <li><a href="#数值层面无偏性仍然保持" id="markdown-toc-数值层面无偏性仍然保持">数值层面：无偏性仍然保持</a></li>
          <li><a href="#梯度推导" id="markdown-toc-梯度推导">梯度推导</a></li>
          <li><a href="#哪些给出无偏的反向-kl-梯度" id="markdown-toc-哪些给出无偏的反向-kl-梯度">哪些给出无偏的反向 KL 梯度？</a></li>
          <li><a href="#fracq_thetamu-k_3-的梯度方差是否更低" id="markdown-toc-fracq_thetamu-k_3-的梯度方差是否更低">$\frac{q_\theta}{\mu} k_3$ 的梯度方差是否更低？</a></li>
          <li><a href="#小结" id="markdown-toc-小结">小结</a></li>
        </ul>
      </li>
      <li><a href="#梯度估计总览" id="markdown-toc-梯度估计总览">梯度估计总览</a></li>
    </ul>
  </li>
  <li><a href="#rl-实践指南" id="markdown-toc-rl-实践指南">RL 实践指南</a>    <ul>
      <li><a href="#kl-作为-reward-惩罚不需要梯度" id="markdown-toc-kl-作为-reward-惩罚不需要梯度">KL 作为 Reward 惩罚（不需要梯度）</a></li>
      <li><a href="#kl-作为-loss需要梯度回传" id="markdown-toc-kl-作为-loss需要梯度回传">KL 作为 Loss（需要梯度回传）</a>        <ul>
          <li><a href="#on-policy优化反向-kl最常见场景" id="markdown-toc-on-policy优化反向-kl最常见场景">On-policy：优化反向 KL（最常见场景）</a></li>
          <li><a href="#on-policy优化正向-kl覆盖型场景" id="markdown-toc-on-policy优化正向-kl覆盖型场景">On-policy：优化正向 KL（覆盖型场景）</a></li>
          <li><a href="#off-policy优化反向-kl" id="markdown-toc-off-policy优化反向-kl">Off-policy：优化反向 KL</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#一份拿来就用的对照表" id="markdown-toc-一份拿来就用的对照表">一份「拿来就用」的对照表</a></li>
  <li><a href="#常见实现陷阱" id="markdown-toc-常见实现陷阱">常见实现陷阱</a></li>
  <li><a href="#总结" id="markdown-toc-总结">总结</a></li>
  <li><a href="#参考文献" id="markdown-toc-参考文献">参考文献</a></li>
</ul>

<p><img src="/assets/img/kl-estimators/kl-estimator-cn.png" alt="Mini-class" style="display:block;margin:0 auto;width:95%;max-width:100%;" /></p>

<blockquote>
  <p>在强化学习中，KL 散度的估计方式直接影响训练稳定性。本文系统剖析三种经典估计器 $k_1, k_2, k_3$ 在 on-policy 和 off-policy 场景的性质差异，并给出「用于 reward 惩罚」与「用于 loss 回传」时的选型指南。</p>
</blockquote>

<p><a href="/reinforcement-learning/2025/12/01/kl-estimators-en.html">English Version</a> | <a href="https://zhuanlan.zhihu.com/p/1978993413425763764">知乎版本 <img src="https://static.zhihu.com/heifetz/favicon.ico" alt="Zhihu" /></a></p>

<h2 id="引言kl-散度在强化学习中的角色">引言：KL 散度在强化学习中的角色</h2>

<p>在策略优化（PPO、GRPO 等）或对齐训练（RLHF/RLAIF）中，<strong>KL 惩罚</strong>是约束新策略不偏离参考策略的核心手段，用以防止训练不稳定或策略崩溃。</p>

<h3 id="正向-kl-与反向-kl-的区别">正向 KL 与反向 KL 的区别</h3>

<p>设 $q_\theta$ 为当前 actor 策略，$p$ 为参考策略，两种方向的 KL 散度分别为：</p>

<p><strong>反向 KL（Reverse KL）</strong>：
\(D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_{x \sim q_\theta}\left[\log \frac{q_\theta(x)}{p(x)}\right]\)</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/kl-estimator-reverse.png" style="width:95%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>正向 KL（Forward KL）</strong>：
\(D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q_\theta(x)}\right]\)</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/kl-estimator-forward.png" style="width:95%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://dibyaghosh.com/blog/probability/kldivergence/">Dibya Ghosh's Blog</a></figcaption>
</figure>

<p><strong>直觉理解</strong>：</p>
<ul>
  <li><strong>反向 KL</strong> 倾向于「模式寻优」（mode-seeking）——策略会集中在参考分布的高概率区域，可能牺牲多样性</li>
  <li><strong>正向 KL</strong> 倾向于「质量覆盖」（mass-covering）——策略会尽量覆盖参考分布的支撑集</li>
</ul>

<p>在 RLHF 的主流实现中，<strong>反向 KL</strong> 更为常见，因为我们希望 actor 不要偏离 reference policy 太远，而非要求完全覆盖所有模式。</p>

<h2 id="三种估计器的定义与设计原理">三种估计器的定义与设计原理</h2>

<p>设比值 $r(x) = \frac{p(x)}{q_\theta(x)}$，John Schulman 提出的三种单样本估计子定义如下：</p>

<h3 id="k_1最朴素的估计器">$k_1$：最朴素的估计器</h3>

\[k_1(x) = -\log r = \log q_\theta(x) - \log p(x)\]

<p>这是最直接的定义——直接取 log-ratio 的负值。它对反向 KL 无偏，但有一个致命缺陷：<strong>可能取负值</strong>，而 KL 散度始终非负。这导致其方差极高，因为正负样本会相互抵消。</p>

<h3 id="k_2基于-f-散度的低方差估计器">$k_2$：基于 f-散度的低方差估计器</h3>

\[k_2(x) = \frac{1}{2}(\log r)^2\]

<p><strong>设计动机</strong>：$k_1$ 的问题在于可正可负，而 $k_2$ 通过取平方保证<strong>每个样本都是正的</strong>，直观上每个样本都在告诉你 $p$ 和 $q$ 相差多远。</p>

<p><strong>为什么偏差很小？</strong> $k_2$ 本质上是一个 <strong>f-散度</strong>（f-divergence），其中 $f(x) = \frac{1}{2}(\log x)^2$。f-散度有一个优美的性质：<strong>所有可微的 f-散度在 $q \approx p$ 时，二阶展开都形如</strong></p>

\[D_f(p, q_\theta) = \frac{f^{\prime\prime}(1)}{2} \theta^T F \theta + O(\theta^3)\]

<p>其中 $F$ 是 Fisher 信息矩阵。KL 散度对应 $f(x) = -\log x$，有 $f^{\prime\prime}(1) = 1$；而 $k_2$ 对应的 $f(x) = \frac{1}{2}(\log x)^2$，同样有 $f^{\prime\prime}(1) = 1$。这意味着<strong>当策略接近时，$k_2$ 与真实 KL 的行为几乎一致</strong>，偏差仅体现在高阶项。</p>

<h3 id="k_3控制变量法构造的最优估计器">$k_3$：控制变量法构造的「最优」估计器</h3>

\[k_3(x) = r - 1 - \log r\]

<p><strong>设计动机</strong>：我们想要一个<strong>既无偏又低方差</strong>的估计器。标准做法是给 $k_1$ 加一个<strong>控制变量</strong>（control variate）——一个期望为零但与 $k_1$ 负相关的量。</p>

<p>注意到 $\mathbb{E}_q[r - 1] = \mathbb{E}_q\left[\frac{p}{q}\right] - 1 = 1 - 1 = 0$，所以对于任意 $\lambda$，</p>

\[k_1 + \lambda(r - 1) = -\log r + \lambda(r - 1)\]

<p>仍然是无偏估计。</p>

<p><strong>为什么选 $\lambda = 1$？</strong> 由于 $\log$ 是凹函数，有 $\log x \leq x - 1$，因此</p>

\[k_3 = (r - 1) - \log r \geq 0\]

<p><strong>始终非负</strong>！这保证了每个样本都在「正向」贡献信息，消除了 $k_1$ 正负抵消的问题。</p>

<p><strong>几何直觉</strong>：$k_3$ 实际上是一个 <strong>Bregman 散度</strong>。考虑凸函数 $\phi(x) = -\log x$，它在 $x=1$ 处的切线为 $y = 1 - x$。Bregman 散度定义为「函数值与切线值之差」：</p>

\[\begin{aligned}
D_\phi(r, 1) &amp;= \phi(r) - \phi(1) - \phi'(1)(r - 1) \\
&amp;= -\log r - 0 - (-1)(r - 1) \\
&amp;= r - 1 - \log r \\
&amp;= k_3.
\end{aligned}\]

<p>由于凸函数始终位于其切线上方，这个差值<strong>天然非负</strong>。更重要的是，在 $r \to 1$ 时，函数与切线「贴合」得越来越紧，差值以 $(r-1)^2$ 的二阶速度趋近于零——这正是 $k_3$ 在策略接近时方差小的根本原因。</p>

<h3 id="三者对比总结">三者对比总结</h3>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">估计器</th>
      <th style="text-align: center;">定义</th>
      <th style="text-align: center;">设计原理</th>
      <th style="text-align: center;">对数值的偏差</th>
      <th style="text-align: center;">方差特性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">$\log r$</td>
      <td style="text-align: center;">最朴素定义</td>
      <td style="text-align: center;">无偏</td>
      <td style="text-align: center;">高（可正可负）</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">$\frac{1}{2}(\log r)^2$</td>
      <td style="text-align: center;">f-散度，二阶行为与 KL 一致</td>
      <td style="text-align: center;">有偏（但极小）</td>
      <td style="text-align: center;">低（恒正）</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">$r - 1 - \log r$</td>
      <td style="text-align: center;">控制变量 + Bregman 散度</td>
      <td style="text-align: center;">无偏</td>
      <td style="text-align: center;">低（恒正）</td>
    </tr>
  </tbody>
</table>
</div>

<p>从数值估计的角度看，$k_3$ 是「无偏 + 低方差」的最优选择；但正如后文将分析的，<strong>梯度层面的故事完全不同</strong>。</p>

<h2 id="核心分析">核心分析</h2>

<h3 id="估计-kl-数值时的偏差与方差">估计 KL 数值时的偏差与方差</h3>

<p>假设从 $q_\theta$ 采样来估计反向 KL $D_{\mathrm{KL}}(q_\theta | p)$：</p>

<p><strong>无偏性分析</strong>：</p>

\[\begin{aligned}
\mathbb{E}_{q}[k_1] &amp;= \mathbb{E}_{q}\left[\log \frac{q}{p}\right] = D_{\mathrm{KL}}(q \| p) \quad \textbf{（无偏）}\\
\mathbb{E}_{q}[k_3] &amp;= \mathbb{E}_{q}[r - 1 - \log r] \\
&amp;= 1 - 1 + D_{\mathrm{KL}}(q \| p) \\
&amp;= D_{\mathrm{KL}}(q \| p) \quad \textbf{（无偏）}\\
\mathbb{E}_{q}[k_2] &amp;= \frac{1}{2}\mathbb{E}_{q}[(\log r)^2] \neq D_{\mathrm{KL}}(q \| p) \quad \textbf{（有偏）}
\end{aligned}\]

<p><strong>结论</strong>：对于估计反向 KL 的<strong>数值</strong>，$k_1$ 和 $k_3$ 是无偏估计，而 $k_2$ 是有偏的。</p>

<p><strong>方差特性的 Trade-off</strong>：</p>

<p>John Schulman 的实验（$q = \mathcal{N}(0,1)$，$p = \mathcal{N}(0.1,1)$，真实 KL = 0.005）表明：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">估计器</th>
      <th style="text-align: center;">bias/true</th>
      <th style="text-align: center;">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">20</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">0.002</td>
      <td style="text-align: center;">1.42</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">1.42</td>
    </tr>
  </tbody>
</table>
</div>

<p>当 KL 较大时（$p = \mathcal{N}(1,1)$，真实 KL = 0.5）：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">估计器</th>
      <th style="text-align: center;">bias/true</th>
      <th style="text-align: center;">stdev/true</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">2</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">0.25</td>
      <td style="text-align: center;">1.73</td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">0</td>
      <td style="text-align: center;">1.7</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>核心直觉</strong>：</p>
<ul>
  <li>$k_1 = -\log r$ 以一阶项起步，当 $r$ 接近 1 时波动较大，且可能取负值</li>
  <li>$k_3 = r - 1 - \log r$ 在 $r=1$ 处是二阶小量，始终非负，因此在策略接近时方差更小</li>
  <li>但当覆盖严重不足（$r$ 可能爆炸）时，$k_3$ 的方差会被权重爆炸拖累；此时 $k_1$ 反而更稳定</li>
</ul>

<blockquote>
  <p><strong>注</strong>：若要估计<strong>正向 KL 的数值</strong> $D_{\mathrm{KL}}(p | q) = \mathbb{E}_p[\log r]$，而只能从 $q$ 采样，可用重要性采样 $\mathbb{E}_q[r \log r]$。</p>
</blockquote>

<h3 id="估计-kl-梯度时的关键区分">估计 KL 梯度时的关键区分</h3>

<p><strong>这是最容易混淆、也是实践中最关键的部分。</strong> 本节先分析<strong>从 $q_\theta$ 采样</strong>（on-policy）的情形，后文将进一步讨论从行为策略 $\mu$ 采样（off-policy）时的变化。</p>

<h4 id="正向与反向-kl-真梯度的推导">正向与反向 KL 真梯度的推导</h4>

<p>在分析估计器之前，我们先推导正向和反向 KL 散度对 $\theta$ 的<strong>真梯度</strong>作为参照。</p>

<p>记 score function $s_\theta(x) = \nabla_\theta \log q_\theta(x)$，它有一个重要性质：$\mathbb{E}_{q_\theta}[s_\theta] = 0$（因为 $\int \nabla_\theta q_\theta dx = \nabla_\theta \int q_\theta dx = \nabla_\theta 1 = 0$）。</p>

<p><strong>反向 KL 的梯度</strong>：</p>

\[D_{\mathrm{KL}}(q_\theta \| p) = \int q_\theta(x) \log \frac{q_\theta(x)}{p(x)} dx\]

<p>对 $\theta$ 求梯度（使用乘积法则）：</p>

\[\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \int \nabla_\theta q_\theta \cdot \log \frac{q_\theta}{p} dx + \int q_\theta \cdot \nabla_\theta \log \frac{q_\theta}{p} dx\]

<p>利用 $\nabla_\theta q_\theta = q_\theta \cdot s_\theta$ 以及 $\nabla_\theta \log q_\theta = s_\theta$、$\nabla_\theta \log p = 0$：</p>

\[= \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] + \mathbb{E}_q[s_\theta] = \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right]\]

<p>即：</p>

\[\boxed{\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) = \mathbb{E}_q\left[s_\theta \cdot \log \frac{q_\theta}{p}\right] = -\mathbb{E}_q[s_\theta \cdot \log r]}\]

<p><strong>正向 KL 的梯度</strong>：</p>

\[D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \log \frac{p(x)}{q_\theta(x)} dx\]

<p>由于 $p(x)$ 不依赖于 $\theta$：</p>

\[\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \int p(x) \cdot \nabla_\theta \left(-\log q_\theta(x)\right) dx = -\mathbb{E}_p[s_\theta]\]

<p>为了用 $q$ 的样本估计这个量，进行重要性采样：</p>

\[-\mathbb{E}_p[s_\theta] = -\mathbb{E}_q\left[\frac{p}{q_\theta} \cdot s_\theta\right] = -\mathbb{E}_q[r \cdot s_\theta]\]

<p>利用 $\mathbb{E}_q[s_\theta] = 0$，可改写为：</p>

\[\boxed{\nabla_\theta D_{\mathrm{KL}}(p \| q_\theta) = \mathbb{E}_q[(1-r) \cdot s_\theta]}\]

<p>有了这两个结果，我们就能判断各估计器的梯度期望究竟对应哪个 KL 的真梯度。</p>

<h4 id="两种求导顺序">两种求导顺序</h4>

<p>在代码实现中，存在两条路径：</p>

<ol>
  <li><strong>先梯度、后期望</strong>：对每个样本的 $k_i(x)$ 求梯度，再对梯度求期望（Monte Carlo 估计）</li>
  <li><strong>先期望、后梯度</strong>：把 $\mathbb{E}_q[k_i]$ 当作损失函数，对解析表达式求梯度</li>
</ol>

<p><strong>在典型的深度学习代码中，我们实际执行的是「先梯度、后期望」</strong>——自动微分对每个样本计算梯度，然后在 batch 上取平均。</p>

<h4 id="三种估计器的梯度推导">三种估计器的梯度推导</h4>

<p>现在我们计算三种估计器的梯度，看它们的期望分别对应哪个 KL 的真梯度。</p>

<p><strong>推导 $\nabla_\theta k_1$</strong>：</p>

\[k_1 = -\log r = -\log \frac{p(x)}{q_\theta(x)} = \log q_\theta(x) - \log p(x)\]

\[\nabla_\theta k_1 = \nabla_\theta \log q_\theta(x) - \nabla_\theta \log p(x) = s_\theta - 0 = s_\theta\]

<p><strong>推导 $\nabla_\theta k_2$</strong>：</p>

\[k_2 = \frac{1}{2}(\log r)^2\]

<p>由链式法则：</p>

\[\begin{aligned}
\nabla_\theta k_2 
&amp;= (\log r) \cdot \nabla_\theta(\log r) \\
&amp;= (\log r) \cdot \nabla_\theta(\log p(x) - \log q_\theta(x)) \\
&amp;= (\log r)(-s_\theta) \\
&amp;= - (\log r) s_\theta.
\end{aligned}\]

<p><strong>推导 $\nabla_\theta k_3$</strong>：</p>

\[k_3 = r - 1 - \log r\]

<p>首先计算 $\nabla_\theta r$。由于 $r = p(x) \cdot q_\theta(x)^{-1}$：</p>

\[\nabla_\theta r = p(x) \cdot (-1) \cdot q_\theta(x)^{-2} \cdot \nabla_\theta q_\theta(x) = -\frac{p(x)}{q_\theta(x)} \cdot \frac{\nabla_\theta q_\theta(x)}{q_\theta(x)} = -r \cdot s_\theta\]

<p>再计算 $\nabla_\theta \log r$：</p>

\[\nabla_\theta \log r = \frac{1}{r} \nabla_\theta r = \frac{1}{r} \cdot (-r \cdot s_\theta) = -s_\theta\]

<p>因此：</p>

\[\nabla_\theta k_3 = \nabla_\theta r - 0 - \nabla_\theta \log r = -r \cdot s_\theta - (-s_\theta) = (1 - r) \cdot s_\theta\]

<p>对它们在 $q_\theta$ 下取期望：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">Estimator</th>
      <th style="text-align: center;">$\mathbb{E}_{q}[\nabla_\theta k_i]$</th>
      <th style="text-align: center;">Equals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">$\mathbb{E}_{q}[s_\theta] = 0$</td>
      <td style="text-align: center;"><strong>Zero (useless as loss)</strong></td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">$-\mathbb{E}_{q}[(\log r) \cdot s_\theta] = \nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>Gradient of reverse KL</strong></td>
    </tr>
    <tr>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">$\mathbb{E}_{q}[(1-r) \cdot s_\theta] = \nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;"><strong>Gradient of forward KL</strong></td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>关键洞察</strong>：</p>
<ul>
  <li><strong>$k_2$ 的梯度</strong>等价于反向 KL 的真梯度——这是优化「约束策略不偏离 ref」的正确选择</li>
  <li><strong>$k_3$ 的梯度</strong>等价于正向 KL 的真梯度——这对应「覆盖型」目标</li>
  <li><strong>$k_1$ 的梯度期望恒为零</strong>——作为 loss 反传毫无意义！</li>
</ul>

<h4 id="先期望后梯度vs先梯度后期望">「先期望后梯度」vs「先梯度后期望」</h4>

<p>如果从解析角度把 $\mathbb{E}_q[k_i]$ 当作一个关于 $\theta$ 的函数再求梯度（即「先期望后梯度」），那么：</p>

\[\nabla_\theta \mathbb{E}_q[k_1] = \nabla_\theta D_{\mathrm{KL}}(q \| p)\]

\[\nabla_\theta \mathbb{E}_q[k_3] = \nabla_\theta D_{\mathrm{KL}}(q \| p)\]

<p>两者都给出反向 KL 的梯度。但在代码中直接对 $k_3$ 的样本均值调用反传时，自动微分执行的是「先梯度后期望」，得到的是 $\mathbb{E}_q[\nabla_\theta k_3]$，即<strong>正向 KL 的梯度</strong>。</p>

<p>这个区分非常重要：<strong>同一个估计器，两种求导顺序可能给出完全不同的结果</strong>。</p>

<h3 id="扩展从行为策略-mu-采样时的-kl-梯度估计">扩展：从行为策略 $\mu$ 采样时的 KL 梯度估计</h3>

<p>前面的分析都默认<strong>样本来自当前策略 $q_\theta$</strong>。然而在实际 RL 训练中，我们常常遇到这样的 off-policy 场景：</p>

<ul>
  <li>用旧策略或混合策略生成数据，再更新当前 actor $q_\theta$</li>
  <li>离线 RL / 经验回放中，样本分布固定为 $\mu$，而不是当前的 $q_\theta$</li>
</ul>

<p>这时，如果我们仍然希望优化<strong>反向 KL</strong> $D_{\mathrm{KL}}(q_\theta | p)$，就必须引入<strong>重要性权重</strong>。</p>

<p>关于大模型 off-policy 场景的深入分析，可以参考我之前的博客：<a href="/reinforcement-learning/2025/11/15/three-policy-cn.html">从两策略到三策略：LLM RL 中行为策略–参考策略不一致下的 TRPO 扩展</a>。</p>

<h4 id="设置与记号">设置与记号</h4>

<p>仍然沿用前文的记号，现在加入采样分布 $\mu(x)$，并定义<strong>重要性权重</strong></p>

\[w(x) = \frac{q_\theta(x)}{\mu(x)}\]

<p>当从 $x \sim \mu$ 采样时，用 $w(x) k_i(x)$ 的 batch 均值作为 loss，然后调用自动微分。那么三种估计器分别给出什么梯度？</p>

<p>一个关键差异是：</p>

<blockquote>
  <p><strong>以前</strong>的期望是 $\mathbb{E}_{q_{\theta}}[\cdot]$，分布本身依赖 $\theta$；
<strong>现在</strong>的期望是 $\mathbb{E}_{\mu}[\cdot]$，而 $\mu$ 与 $\theta$ 无关。</p>
</blockquote>

<p>这会让「先期望后梯度」与「先梯度后期望」的关系发生根本变化。</p>

<h4 id="关键观察两种求导顺序的等价性">关键观察：两种求导顺序的等价性</h4>

<p>因为 $\mu$ 与 $\theta$ 无关，对任何关于 $\theta$ 可微的函数 $f_\theta(x)$，有</p>

\[\nabla_\theta \mathbb{E}_{\mu}[f_\theta(x)] = \mathbb{E}_{\mu}[\nabla_\theta f_\theta(x)]\]

<p>换句话说，<strong>代码中对样本均值反传（先梯度后期望）就等价于对解析形式求梯度（先期望后梯度）</strong>，不会再像 on-policy 时那样分裂成两个不同的结果。</p>

<p><strong>所以在 off-policy + 重要性加权 的情形下，对反向 KL 数值无偏的估计器 $k_1$ 和 $k_3$，它们的梯度期望都将对应于反向 KL 的真梯度。</strong></p>

<p>这是与 on-policy 情形的根本区别。</p>

<h4 id="数值层面无偏性仍然保持">数值层面：无偏性仍然保持</h4>

<p>由标准的重要性采样关系 $\mathbb{E}_\mu[w \cdot f] = \mathbb{E}_{q_\theta}[f]$，有</p>

\[\mathbb{E}_\mu[w k_1] = D_{\mathrm{KL}}(q_\theta \| p), \quad
\mathbb{E}_\mu[w k_3] = D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{（无偏）}\]

\[\mathbb{E}_\mu[w k_2] = \mathbb{E}_{q_\theta}[k_2] \neq D_{\mathrm{KL}}(q_\theta \| p) \quad \textbf{（有偏）}\]

<p>这与 on-policy 情形完全一致。</p>

<h4 id="梯度推导">梯度推导</h4>

<p>首先计算重要性权重的梯度。由 $w = q_\theta / \mu$ 且 $\mu$ 不依赖 $\theta$：</p>

\[\nabla_\theta w(x) = w(x) s_\theta(x)\]

<p>结合前文已推导的 $\nabla_\theta k_i$，用乘积法则：</p>

<p><strong>$\nabla_\theta(w k_1)$</strong>：</p>

\[\nabla_\theta(w k_1) = (\nabla_\theta w) k_1 + w (\nabla_\theta k_1) = w s_\theta k_1 + w s_\theta = w s_\theta (k_1 + 1)\]

<p><strong>$\nabla_\theta(w k_2)$</strong>：</p>

\[\nabla_\theta(w k_2) = w s_\theta k_2 + w (-\log r) s_\theta = w s_\theta (k_2 - \log r)\]

<p><strong>$\nabla_\theta(w k_3)$</strong>：</p>

\[\nabla_\theta(w k_3) = w s_\theta k_3 + w (1-r) s_\theta = w s_\theta (k_3 + 1 - r)\]

<p>代入 $k_3 = r - 1 - \log r$：</p>

\[k_3 + 1 - r = (r - 1 - \log r) + 1 - r = -\log r = k_1\]

<p>因此有一个漂亮的简化：</p>

\[\boxed{\nabla_\theta(w k_3) = w s_\theta k_1 = -w s_\theta \log r}\]

<h4 id="哪些给出无偏的反向-kl-梯度">哪些给出无偏的反向 KL 梯度？</h4>

<p>利用 $\mathbb{E}_\mu[w \cdot f] = \mathbb{E}_{q_\theta}[f]$ 和 $\mathbb{E}_{q_\theta}[s_\theta] = 0$：</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(w k_1)]$</strong>：</p>

\[\mathbb{E}_\mu[w s_\theta (k_1 + 1)] = \mathbb{E}_{q}[s_\theta k_1] + \underbrace{\mathbb{E}_{q}[s_\theta]}_{=0} = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark\]

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(w k_2)]$</strong>：</p>

\[\mathbb{E}_\mu[w s_\theta (k_2 - \log r)] = \mathbb{E}_{q}[s_\theta (k_2 - \log r)] = \nabla_\theta \mathbb{E}_{q}[k_2]\]

<p>这是 $\mathbb{E}_q[k_2]$ 这个 f-散度的真梯度，<strong>不是</strong>反向 KL 的梯度。</p>

<p><strong>$\mathbb{E}_\mu[\nabla_\theta(w k_3)]$</strong>：</p>

\[\mathbb{E}_\mu[w s_\theta k_1] = \mathbb{E}_{q}[s_\theta k_1] = \nabla_\theta D_{\mathrm{KL}}(q_\theta \| p) \quad \checkmark\]

<p><strong>总结表格</strong>：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">加权估计器</th>
      <th style="text-align: center;">期望对应的目标</th>
      <th style="text-align: center;">梯度期望对应的真梯度</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$\frac{q_\theta}{\mu} k_1$</td>
      <td style="text-align: center;">$D_{\mathrm{KL}}(q_\theta \| p)$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$（反向 KL） ✓</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\frac{q_\theta}{\mu} k_2$</td>
      <td style="text-align: center;">$\mathbb{E}_q[k_2]$（f-散度）</td>
      <td style="text-align: center;">$\nabla_\theta \mathbb{E}_q[k_2]$，<strong>不是</strong>反向 KL ✗</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\frac{q_\theta}{\mu} k_3$</td>
      <td style="text-align: center;">$D_{\mathrm{KL}}(q_\theta \| p)$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q_\theta \| p)$（反向 KL） ✓</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>与 on-policy 情形的对比——一个有趣的反转</strong>：</p>

<ul>
  <li>On-policy 时，用 $k_2$ 做 loss 的梯度是反向 KL，而 $k_1$ 的梯度期望恒为零</li>
  <li>Off-policy + 重要性加权时，$\frac{q_\theta}{\mu} k_1$ 和 $\frac{q_\theta}{\mu} k_3$ 给出反向 KL 的真梯度，而 $\frac{q_\theta}{\mu} k_2$ <strong>不再适用</strong></li>
</ul>

<h4 id="fracq_thetamu-k_3-的梯度方差是否更低">$\frac{q_\theta}{\mu} k_3$ 的梯度方差是否更低？</h4>

<p>现在我们关心的梯度随机变量为：</p>

\[g_1(x) := \nabla_\theta(w k_1) = w(x) s_\theta(x) (k_1(x) + 1)\]

\[g_3(x) := \nabla_\theta(w k_3) = w(x) s_\theta(x) k_1(x)\]

<p>两者的期望相同，都是 $\nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$，但存在一个简单的关系：</p>

\[g_1(x) - g_3(x) = w(x) s_\theta(x)\]

<p>又因为 $\mathbb{E}_\mu[w s_\theta] = \mathbb{E}_{q_\theta}[s_\theta] = 0$，可知 $w s_\theta$ 是一个<strong>零均值的「纯噪声项」</strong>。</p>

<p><strong>一阶展开分析</strong>：</p>

<p>在 KL 惩罚常用的 regime 下，三个分布彼此接近：$q_\theta \approx p \approx \mu$。此时令 $r(x) = 1 + \varepsilon(x)$，$\vert \varepsilon \vert \ll 1$，做一阶展开：</p>

\[k_1 = -\log r \approx -\varepsilon + O(\varepsilon^2)\]

<p>代入 $g_1, g_3$ 的系数：</p>

\[k_1 + 1 \approx 1 - \varepsilon + O(\varepsilon^2), \quad k_1 \approx -\varepsilon + O(\varepsilon^2)\]

<p>于是：</p>

\[g_1(x) \approx w(x) s_\theta(x) \cdot \big(1 - \varepsilon(x) + O(\varepsilon^2)\big)\]

\[g_3(x) \approx w(x) s_\theta(x) \cdot \big(-\varepsilon(x) + O(\varepsilon^2)\big)\]

<p><strong>核心直觉</strong>：</p>

<ul>
  <li>$g_1$ 含有一个「量级为 1 但期望为零」的常数项 $w s_\theta$。真梯度是这部分与其它项相互抵消后的<strong>微小差值</strong>，因此单样本方差很大。</li>
  <li>$g_3$ 在解析形式上已经把这个常数项消掉了，剩下的是与偏差 $\varepsilon(x) = r(x) - 1$ 成正比的<strong>一阶小量</strong>。当策略接近时 $\vert \varepsilon \vert$ 很小，$g_3$ 的波动自然显著更小。</li>
</ul>

<p>这与前文「数值估计」一节的直觉完全一致：$k_3$ 在 $r = 1$ 处是二阶小量，而 $k_1$ 是一阶量。加上重要性权重后，这一性质被保留到了梯度估计中。</p>

<blockquote>
  <p><strong>结论</strong>：在 $q_\theta \approx p \approx \mu$ 的典型 KL 惩罚场景下，$\frac{q_\theta}{\mu} k_3$ 对应的<strong>梯度方差严格低阶</strong>于 $\frac{q_\theta}{\mu} k_1$，是「无偏 + 低方差」的选择。</p>
</blockquote>

<p><strong>极度 off-policy 时的警示</strong>：</p>

<p>当 $\mu$ 与 $q_\theta$ 差异很大——比如 $\mu$ 在 $q_\theta$ 的高密度区域几乎没有采样，或 $w = q_\theta / \mu$ 在尾部爆炸——任何基于 $\frac{q_\theta}{\mu}$ 的方法都会遭遇严重的方差问题。此时 $\frac{q_\theta}{\mu} k_3$ 相对 $\frac{q_\theta}{\mu} k_1$ 的优势不再有理论保证，需要结合 clipping、正则化等策略综合处理。</p>

<p>不过，在 RL 实践中我们通常会控制 KL 约束、限制 off-policy 程度（比如使用近邻策略 $\mu = q_{\theta_\text{old}}$），在这个常见的 regime 里，可以相当有信心地说：</p>

<blockquote>
  <p><strong>如果已经决定用 off-policy + 重要性采样来优化反向 KL，用 $\frac{q_\theta}{\mu} k_3$ 做 loss，通常比 $\frac{q_\theta}{\mu} k_1$ 有更低的梯度方差。</strong></p>
</blockquote>

<p>这就是为什么 DeepSeek v3.2 技术报告中使用的是 $\frac{q_\theta}{\mu} k_3$ 作为 off-policy KL 惩罚的估计器。</p>

<figure style="text-align:center;">
  <img src="/assets/img/kl-estimators/dpsk-3d2-k3.png" style="width:95%;max-width:100%;" />
  <figcaption style="font-size:0.9em;color:gray;">图片来源：<a href="https://arxiv.org/pdf/2512.02556v1">DeepSeek v3.2 技术报告 3.1 章节</a></figcaption>
</figure>

<h4 id="小结">小结</h4>

<ul>
  <li>从行为策略 $\mu$ 采样时，自然的 off-policy KL 估计为 $\frac{q_\theta}{\mu} k_i$。</li>
  <li><strong>数值上</strong>，$\frac{q_\theta}{\mu} k_1$ 与 $\frac{q_\theta}{\mu} k_3$ 仍然是反向 KL 的无偏估计。</li>
  <li><strong>梯度上</strong>，因为 $\mu$ 与 $\theta$ 无关，「先期望后梯度」与「先梯度后期望」等价：
    <ul>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\frac{q_\theta}{\mu} k_1)] = \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\frac{q_\theta}{\mu} k_3)] = \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
      <li>$\mathbb{E}_\mu[\nabla_\theta(\frac{q_\theta}{\mu} k_2)] \neq \nabla_\theta D_{\mathrm{KL}}(q_\theta | p)$</li>
    </ul>
  </li>
  <li><strong>方差上</strong>，$\frac{q_\theta}{\mu} k_3$ 的梯度可以看作对 $\frac{q_\theta}{\mu} k_1$ 梯度减去了一个零均值的噪声项 $w s_\theta$。在 $q_\theta \approx p \approx \mu$ 且重要性权重不过于极端时，<strong>$\frac{q_\theta}{\mu} k_3$ 的梯度更平稳、方差更低</strong>。</li>
</ul>

<h3 id="梯度估计总览">梯度估计总览</h3>

<p>下表汇总了 on-policy 与 off-policy 两种场景下，各估计器的梯度期望及其对应的优化目标：</p>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center; white-space: nowrap;">采样来源</th>
      <th style="text-align: center;">Loss</th>
      <th style="text-align: center;">$\nabla_\theta$ Loss 的期望</th>
      <th style="text-align: center;">对应的优化目标</th>
      <th style="text-align: center; white-space: nowrap;">能否用于优化反向 KL？</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k_1$</td>
      <td style="text-align: center;">$\mathbb{E}_q[s_\theta] = 0$</td>
      <td style="text-align: center;">无（梯度恒为零）</td>
      <td style="text-align: center;">✗</td>
    </tr>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k_2$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>反向 KL</strong></td>
      <td style="text-align: center;">✓</td>
    </tr>
    <tr>
      <td style="text-align: center;">$q$ (on)</td>
      <td style="text-align: center;">$k_3$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;">正向 KL</td>
      <td style="text-align: center;">✗</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_1$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>反向 KL</strong></td>
      <td style="text-align: center;">✓（但方差较高）</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_2$</td>
      <td style="text-align: center;">$\nabla_\theta \mathbb{E}_q[k_2]$</td>
      <td style="text-align: center;">f-散度（非 KL）</td>
      <td style="text-align: center;">✗</td>
    </tr>
    <tr>
      <td style="text-align: center;">$\mu$ (off)</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_3$</td>
      <td style="text-align: center;">$\nabla_\theta D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;"><strong>反向 KL</strong></td>
      <td style="text-align: center;">✓（推荐，低方差）</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>关键结论</strong>：</p>

<ol>
  <li><strong>On-policy 优化反向 KL</strong>：唯一正确选择是 $k_2$</li>
  <li><strong>Off-policy 优化反向 KL</strong>：$\frac{q}{\mu} k_1$ 和 $\frac{q}{\mu} k_3$ 都正确，但 $\frac{q}{\mu} k_3$ 方差更低</li>
  <li><strong>$k_2$ 在 off-policy 下失效</strong>：这是一个容易被忽视的陷阱</li>
</ol>

<h2 id="rl-实践指南">RL 实践指南</h2>

<h3 id="kl-作为-reward-惩罚不需要梯度">KL 作为 Reward 惩罚（不需要梯度）</h3>

<p>当 KL 仅作为标量惩罚加入 reward shaping 时，我们只需要准确的<strong>数值估计</strong>，不需要反传梯度。</p>

<p><strong>推荐</strong>：</p>
<ul>
  <li>使用 <strong>$k_1$</strong> 或 <strong>$k_3$</strong>（两者对反向 KL 数值均无偏）</li>
  <li>当策略已接近参考策略时，$k_3$ 往往更低方差</li>
  <li>覆盖不足或尾部错配明显时，$k_1$ 更稳健</li>
  <li>Off-policy 时加重要性权重 $\frac{q_\theta}{\mu}$ 即可</li>
</ul>

<blockquote>
  <p><strong>注</strong>：若想施加<strong>正向 KL 惩罚</strong>（偏向覆盖行为分布），数值上可用 $\mathbb{E}_q[r \log r]$ 或（若可从 $p$ 采样）$\mathbb{E}_p[\log r]$。</p>
</blockquote>

<h3 id="kl-作为-loss需要梯度回传">KL 作为 Loss（需要梯度回传）</h3>

<p>当 KL 作为 loss 的一部分参与反传时，必须考虑梯度的正确性。</p>

<h4 id="on-policy优化反向-kl最常见场景">On-policy：优化反向 KL（最常见场景）</h4>

<p>目标：控制 actor 不偏离 reference policy。</p>

<p><strong>正确做法</strong>：使用 <strong>$k_2$</strong> 作为 loss。</p>

\[\mathcal{L}_{k_2} = \frac{1}{2}(\log r)^2\]

<p>其梯度期望 $\mathbb{E}_q[\nabla k_2] = \nabla_\theta D_{\mathrm{KL}}(q | p)$ 正是反向 KL 的真梯度。</p>

<h4 id="on-policy优化正向-kl覆盖型场景">On-policy：优化正向 KL（覆盖型场景）</h4>

<p>目标：让策略覆盖参考分布的支撑集（如离线 RL、模仿学习等）。</p>

<p><strong>正确做法</strong>：使用 <strong>$k_3$</strong> 作为 loss。</p>

\[\mathbb{E}_q[\nabla k_3] = \mathbb{E}_q[(1-r) \cdot s_\theta] = \nabla_\theta D_{\mathrm{KL}}(p \| q)\]

<p>直接对 $k_3$ 的样本均值调用反传，自动微分计算的就是 $\mathbb{E}_q[\nabla_\theta k_3]$，即正向 KL 的梯度，无需额外处理。</p>

<h4 id="off-policy优化反向-kl">Off-policy：优化反向 KL</h4>

<p>目标：数据来自行为策略 $\mu$，仍希望优化反向 KL。</p>

<p><strong>正确做法</strong>：使用 <strong>$\frac{q_\theta}{\mu} k_3$</strong> 作为 loss。</p>

\[\mathcal{L} = \frac{q_\theta(x)}{\mu(x)} \cdot \left(\frac{p(x)}{q_\theta(x)} - 1 - \log \frac{p(x)}{q_\theta(x)}\right)\]

<ul>
  <li>梯度无偏</li>
  <li>当 $q_\theta \approx p$ 时方差显著更低</li>
</ul>

<p><strong>备选方案</strong>：使用 $\frac{q_\theta}{\mu} k_1$（梯度同样无偏，但方差更高）</p>

<p><strong>避免</strong>：使用 $\frac{q_\theta}{\mu} k_2$（梯度有偏，不是反向 KL 的正确方向）</p>

<h2 id="一份拿来就用的对照表">一份「拿来就用」的对照表</h2>

<div class="table-responsive">
<table class="table table-bordered" style="font-size: 0.95em;">
  <thead>
    <tr style="background-color: var(--global-bg-color);">
      <th style="text-align: center;">目标</th>
      <th style="text-align: center;">采样来源</th>
      <th style="text-align: center;">用于<strong>数值</strong></th>
      <th style="text-align: center;">用于<strong>梯度</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center;">反向 KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;">$q$（on-policy）</td>
      <td style="text-align: center;">$k_1$ 或 $k_3$（无偏）</td>
      <td style="text-align: center;">$k_2$</td>
    </tr>
    <tr>
      <td style="text-align: center;">反向 KL $D_{\mathrm{KL}}(q \| p)$</td>
      <td style="text-align: center;">$\mu$（off-policy）</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_1$ 或 $\frac{q}{\mu} k_3$（无偏）</td>
      <td style="text-align: center;">$\frac{q}{\mu} k_3$</td>
    </tr>
    <tr>
      <td style="text-align: center;">正向 KL $D_{\mathrm{KL}}(p \| q)$</td>
      <td style="text-align: center;">$q$</td>
      <td style="text-align: center;">$\mathbb{E}_q[r\log r]$</td>
      <td style="text-align: center;">$k_3$</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="常见实现陷阱">常见实现陷阱</h2>

<p><strong>陷阱 1：把 $k_1$ 直接当 loss 反传（on-policy）</strong></p>

<p>$k_1$ 的梯度期望恒为零（$\mathbb{E}_q[\nabla k_1] = \mathbb{E}_q[s_\theta] = 0$），作为 loss 完全无效。</p>

<blockquote>
  <p><strong>解决</strong>：reward shaping 用 $k_1$ 或 $k_3$（不需要梯度），loss 用 $k_2$ 或 $k_3$。</p>
</blockquote>

<p><strong>陷阱 2：混淆 $k_3$ 的「数值无偏性」与「梯度对应的目标」</strong></p>

<p>$k_3$ 对<strong>反向 KL 的数值</strong>是无偏估计，但它的<strong>梯度</strong>对应的是<strong>正向 KL</strong>。如果你的目标是优化反向 KL，却用 $k_3$ 作为 loss，实际上在优化正向 KL。</p>

<blockquote>
  <p><strong>解决</strong>：明确你的优化目标。优化反向 KL 用 $k_2$；优化正向 KL 才用 $k_3$。</p>
</blockquote>

<p><strong>陷阱 3：$r$ 重尾导致方差爆炸</strong></p>

<p>当策略与参考分布差异过大时，$r = p/q$ 可能出现极端值，导致 $k_3$ 的方差爆炸。</p>

<blockquote>
  <p><strong>解决</strong>：控制 KL 约束，或对 $r$ 进行 clipping。</p>
</blockquote>

<p><strong>陷阱 4：离策略下仍用 $k_2$ 或 $\frac{q_\theta}{\mu} k_2$</strong></p>

<p>在 on-policy 下，$k_2$ 是优化反向 KL 的正确选择。但如果数据来自 $\mu \neq q_\theta$：</p>
<ul>
  <li>直接用 $k_2$（不加权）：期望不是在 $q_\theta$ 下取的，估计器完全失效</li>
  <li>用 $\frac{q_\theta}{\mu} k_2$：梯度有偏，不是反向 KL 的真梯度</li>
</ul>

<blockquote>
  <p><strong>解决</strong>：离策略场景下，改用 $\frac{q_\theta}{\mu} k_3$（推荐）或 $\frac{q_\theta}{\mu} k_1$。</p>
</blockquote>

<p><strong>陷阱 5：把重要性权重 detach 掉</strong></p>

<p>代码实现中，$w = q_\theta / \mu$ 通常通过 <code class="language-plaintext highlighter-rouge">log_prob_q - log_prob_mu</code> 再取 <code class="language-plaintext highlighter-rouge">exp</code> 计算得到。如果把 $w$ 当作常数（detach），则丢失了 $\nabla_\theta w = w s_\theta$ 这一项，导致梯度错误。</p>

<blockquote>
  <p><strong>解决</strong>：确保 $w$ 参与计算图，让自动微分正确计算完整的 $\nabla_\theta(w k_i)$。</p>
</blockquote>

<h2 id="总结">总结</h2>

<p><strong>一句话记忆</strong>：</p>

<ul>
  <li><strong>只要数值（KL 作为 reward 惩罚）</strong>：选 $k_1$ 或 $k_3$（均对反向 KL 无偏）；off-policy 时加重要性权重即可</li>
  <li><strong>需要梯度（KL 作为 loss）</strong>：
    <ul>
      <li><strong>On-policy</strong>：优化反向 KL → 用 $k_2$；优化正向 KL → 用 $k_3$</li>
      <li><strong>Off-policy</strong>：优化反向 KL → 用 $\frac{q_\theta}{\mu} k_3$（无偏 + 低方差）</li>
    </ul>
  </li>
</ul>

<p>把「<strong>从谁采样</strong>」、「<strong>估计谁的值</strong>」、「<strong>对谁求梯度</strong>」这三个问题捋清楚，三种估计器就不再让人混淆了。特别注意：<strong>on-policy 和 off-policy 下，优化反向 KL 的正确选择是不同的</strong>——前者用 $k_2$，后者用 $\frac{q_\theta}{\mu} k_3$。</p>

<h2 id="参考文献">参考文献</h2>

<ol>
  <li>
    <p>Dibya Ghosh. “KL Divergence for Machine Learning”. https://dibyaghosh.com/blog/probability/kldivergence</p>
  </li>
  <li>
    <p>John Schulman. “Approximating KL Divergence”. https://joschu.net/blog/kl-approx.html</p>
  </li>
  <li>
    <p>Verl Documentation. “Proximal Policy Optimization (PPO)”. https://verl.readthedocs.io/en/latest/algo/ppo.html</p>
  </li>
  <li>
    <p>初七123334. RLHF/RLVR 训练中的 KL 近似方法浅析（k1 / k2 / k3）. https://zhuanlan.zhihu.com/p/1966872846212010437</p>
  </li>
  <li>
    <p>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. “Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization”. https://arxiv.org/abs/2510.01555</p>
  </li>
  <li>
    <p>Yifan Zhang, Yiping Ji, Gavin Brown, et al. “On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning”. https://arxiv.org/abs/2505.17508</p>
  </li>
</ol>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">WangZhang2025KLEstimators</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Understanding {KL} Divergence Estimators in {RL}: From Value Approximation to Gradient Estimation}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
  <span class="na">url</span>          <span class="p">=</span> <span class="s">{https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html}</span>
<span class="p">}</span>
</code></pre></div></div>

  </article></div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2025 Xihuai Leo Wang. Last updated: December 04, 2025.
      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    <!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2923RQZBXG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-2923RQZBXG');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
