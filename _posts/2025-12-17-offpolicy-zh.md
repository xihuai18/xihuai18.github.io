---
layout: post
title: "驯服陈旧数据：LLM 强化学习的异策略训练与单调改进保证"
date: 2025-12-17
description: "系统推导大模型强化学习中的异策略训练理论：从单策略采样的性能改进下界出发，扩展到多策略静态/动态混合采样，给出单调提升的充分条件，并通过三角不等式分解将约束拆分为更新增量偏移（优化侧可控）与采样陈旧性（采样侧可控）两部分，最终落地为可操作的裁剪机制与数据过滤策略。"
categories: reinforcement-learning
lang: zh
en_url: /reinforcement-learning/2025/12/17/offpolicy-en.html
---



## 引言：为什么我们需要关心"异策略"？

想象这样一个场景：你正在用强化学习训练一个大语言模型，让它学会更好地回答问题。理想情况下，每次模型生成一批回答后，你会立即用这些数据更新模型，然后用更新后的模型生成新数据，如此循环往复。这种"用谁的数据就更新谁"的方式叫做**同策略**（on-policy）训练。

但现实没这么简单。在大规模分布式训练中，数百个GPU并行生成数据，而模型更新需要时间。当新模型发布时，很多"旧版本"模型生成的数据还没用完——扔掉太浪费，用起来又担心"数据过时"会影响训练效果。

这就是**异策略**（off-policy）训练面临的核心问题：**用旧策略采集的数据来更新新策略，能保证性能持续提升吗？**

本文将系统回答这个问题。我们从基础理论出发，逐步推导出可操作的条件，告诉你：在什么情况下，混合使用多个版本策略的数据仍然能保证训练单调改进。

## 第一部分：理论基础

### 1.1 基本设定

我们考虑标准的马尔可夫决策过程（MDP），包含状态空间 $\mathcal{S}$、动作空间 $\mathcal{A}$、转移概率 $p(s'\|s,a)$、奖励函数 $r(s,a)$、初始分布 $\rho_0$ 和折扣因子 $\gamma \in (0,1)$。

策略 $\pi$ 的**期望累计折扣回报**为：

$$
J(\pi) := \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid \pi\right]
$$

**折扣状态访问分布**定义为策略长期运行中访问各状态的加权频率：

$$
d_\pi(s) := (1-\gamma) \sum_{t=0}^{\infty} \gamma^t \Pr(s_t = s \mid \pi)
$$

**优势函数**衡量动作 $a$ 相对于策略平均水平的优劣：

$$
A^\pi(s,a) := Q^\pi(s,a) - V^\pi(s)
$$

**全变差距离**（TV距离）衡量两个策略在状态 $s$ 上动作分布的差异：

$$
D_{\mathrm{TV}}(\pi, \pi'; s) := \frac{1}{2} \sum_{a \in \mathcal{A}} |\pi(a \mid s) - \pi'(a \mid s)|
$$

### 1.2 核心工具：策略性能差异引理

整个理论的基石是这个简洁的结论：

> **引理1.1（策略性能差异引理）**
> 
> 对任意策略 $\pi_k$（旧）和 $\pi$（新），性能差异可表示为：
> 
> $$
> J(\pi) - J(\pi_k) = \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_\pi}\left[ \mathbb{E}_{a \sim \pi(\cdot \mid s)}[A^{\pi_k}(s,a)] \right]
> $$

**直观理解**：新策略比旧策略好多少，等于在新策略访问的状态分布下，用新策略选动作能获得的"平均优势"。

## 第二部分：单策略采样的性能改进下界

### 2.1 分布不匹配问题

策略性能差异引理有个实际问题：右侧期望在 $d_\pi$（新策略的状态分布）下计算，而我们只能从 $d_{\pi_k}$（旧策略）采样。

解决思路是：把期望拆成"旧分布下的期望 + 偏差项"，然后控制偏差。关键问题是：**状态分布的差异与策略的差异有什么定量关系？**

### 2.2 状态分布差异的控制

> **引理1.2（状态分布差异与策略TV距离的关系）**
> 
> $$
> \|d_\pi - d_{\pi_k}\|_1 \leq \frac{2\gamma}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]
> $$

**物理意义**：策略在动作空间上的小差异，会通过环境动力学"放大"成状态访问分布的差异。系数 $\frac{\gamma}{1-\gamma}$ 反映了**时间累积效应**——长时域任务（$\gamma$ 接近1）中，放大效应更强。

**证明思路**：推导折扣访问分布的不动点方程，利用随机矩阵的 $\ell_1$ 非扩张性，可以证明状态分布差异被策略差异通过转移动力学放大，放大系数正是 $\frac{\gamma}{1-\gamma}$。

### 2.3 策略性能改进下界

> **定理1.1（策略性能改进下界）**
> 
> 定义期望优势上界常数 $C_{\pi,\pi_k} := \max_{s} \lvert \mathbb{E}_{a \sim \pi}[A^{\pi_k}(s,a)] \rvert$，则：
> 
> $$
> J(\pi) - J(\pi_k) \geq L_{\pi_k}(\pi) - \frac{2\gamma C_{\pi,\pi_k}}{(1-\gamma)^2} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]
> $$
> 
> 其中**代理目标**为：
> 
> $$
> L_{\pi_k}(\pi) := \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}, a \sim \pi_k} \left[ \frac{\pi(a \mid s)}{\pi_k(a \mid s)} A^{\pi_k}(s,a) \right]
> $$

这个下界由两部分组成：

1. **代理目标** $L_{\pi_k}(\pi)$：可用旧策略数据通过重要性采样直接估计，是TRPO/PPO的优化目标。

2. **策略偏移惩罚**：随新旧策略的TV距离增大而增大，这解释了为何PPO需要限制更新幅度。

**核心结论**：最大化代理目标的同时控制策略偏移，即可保证性能改进。

## 第三部分：多策略静态混合采样

### 3.1 实际场景

在实际训练中，一个batch的数据可能来自多个策略版本 $\{\pi^{(1)}, \ldots, \pi^{(M)}\}$，各版本占比为 $\alpha_1, \ldots, \alpha_M$。如何将定理1.1扩展到这种情形？

### 3.2 核心思想：扩展状态空间

解决方案是一个优雅的建模技巧：**把策略版本索引当作状态的一部分**。

定义扩展状态空间 $\tilde{\mathcal{S}} := \mathcal{S} \times \mathcal{I}$，其中 $\mathcal{I} = \{1, \ldots, M\}$ 是策略索引集合。在扩展状态 $(s, i)$ 下，**混合行为策略**定义为 $\beta(a \mid s, i) := \pi^{(i)}(a \mid s)$。

索引的演化由**索引转移核** $q(i' \mid i)$ 刻画。扩展MDP继承原始MDP的奖励和环境转移，索引按 $q(i'\|i)$ 独立演化。

这个技巧之所以有效，是因为新策略 $\pi$ 在扩展MDP上的回报等于原始MDP中的回报，从而可以直接应用定理1.1。

### 3.3 轨迹级混合的结构简化

最常见的情形是**每条轨迹只用一个旧策略**：轨迹开始时采样索引 $I_0 \sim \alpha$，整条轨迹使用 $\pi^{(I_0)}$。此时索引转移核为恒等转移：$q(i' \mid i) = \mathbf{1}_{i'=i}$。

从工程实现角度看，在很多 **actor-learner 的异步训练**里（如果采样与训练侧把数据按"整条轨迹/完整 episode 归属某个策略版本"来组织），这可以近似对应这里的**轨迹级混合**：actor 在一个采样单元内固定使用某个策略快照生成数据，learner 再混合使用来自不同版本的整轨迹数据做更新。这里用"近似"是因为不同系统对"轨迹/采样单元"的切分边界并不完全一致。

> **引理2.1（轨迹级混合的结构简化）**
> 
> (a) 扩展状态访问分布分解为：$d_{\beta}(s, i) = \alpha_i \cdot d_{\pi^{(i)}}(s)$
> 
> (b) 优势函数还原为：$A^{\beta}((s, i), a) = A^{\pi^{(i)}}(s, a)$

**(b)的直觉**：由于索引永不改变，从扩展状态 $(s,i)$ 出发的**所有未来轨迹**都由同一个策略 $\pi^{(i)}$ 生成。因此，未来的累计回报完全由 $\pi^{(i)}$ 决定，价值函数和优势函数自然还原为 $\pi^{(i)}$ 的对应量。

由此，混合策略的回报为各旧策略回报的加权平均：$J_{\mathrm{mix}} = \sum_{i=1}^{M} \alpha_i J(\pi^{(i)})$。

### 3.4 轨迹级混合的性能改进下界

> **推论2.1（轨迹级混合的性能改进下界）**
> 
> $$
> J(\pi) - \sum_{i=1}^{M} \alpha_i J(\pi^{(i)}) \geq \sum_{i=1}^{M} \alpha_i L_{\pi^{(i)}}(\pi) - \frac{2\gamma \max_i C_{\pi, \pi^{(i)}}}{(1-\gamma)^2} \sum_{i=1}^{M} \alpha_i \mathbb{E}_{s \sim d_{\pi^{(i)}}} \big[ D_{\mathrm{TV}}(\pi, \pi^{(i)}; s) \big]
> $$

该结论表明：将多个旧策略版本的轨迹混合训练时，若对每条轨迹用对应旧策略的重要性比率构造损失，同时控制新策略与各旧策略的偏移，则新策略性能有明确的改进下界。

## 第四部分：动态混合采样与单调提升条件

### 4.1 问题的核心挑战

第三部分讨论的是**静态混合**——混合权重 $\alpha_i$ 固定不变。本节考虑更一般的**动态混合**——新策略发布后，采样逐步由新策略接管。

前面的结论刻画了"新策略相对于混合行为策略"的改进。但在实际训练中，我们真正关心的是：**每轮更新后的最新策略 $\pi_{k+1}$ 相对于上一轮最新策略 $\pi_k$ 是否单调提升？**

$$
J(\pi_{k+1}) \geq J(\pi_k)
$$

### 4.2 统一建模框架

动态混合采样的两种典型形式都可以用索引转移核 $q(i'\|i)$ 统一刻画：

**轨迹级混合**（可类比为常规异步训练的一个抽象；索引恒等转移）：$q(i'\|i) = \mathbf{1}\{i'=i\}$

**步/段级混合**（partial rollout / 段式采样的一个抽象；允许切换）：$q(i'\|i) = (1-\sigma(i))\mathbf{1}\{i'=i\} + \sigma(i)\kappa(i'\|i)$

其中 $\sigma(i)$ 为切换概率，$\kappa(\cdot\|i)$ 为目标索引分布。

### 4.3 核心分解

通过引入混合回报 $J_{\mathrm{mix}}^{(k)}$ 作为中间桥梁，性能差异分解为：

$$
J(\pi_{k+1}) - J(\pi_k) = \underbrace{[J(\pi_{k+1}) - J_{\mathrm{mix}}^{(k)}]}_{\text{相对混合策略的改进}} + \underbrace{[J_{\mathrm{mix}}^{(k)} - J(\pi_k)]}_{\text{混合偏差项}}
$$

第一项可用定理1.1处理。第二项是**混合偏差项**，可以证明它满足：

$$
J_{\mathrm{mix}}^{(k)} - J(\pi_k) \geq -\frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
$$

### 4.4 单调提升下界

合并上述结果，得到核心定理：

> **定理3.1（动态混合采样下的单调提升下界）**
> 
> $$
> \begin{aligned}
> J(\pi_{k+1}) - J(\pi_k) \geq\;& L_{\beta^{(k)}}(\pi_{k+1}) \\
> &- \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \big] \\
> &- \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
> \end{aligned}
> $$

该下界揭示了**双重控制**的必要性：
- **更新偏移惩罚**：新策略 $\pi_{k+1}$ 相对采样来源策略 $\pi^{(i)}$ 的偏移
- **采样陈旧性惩罚**：采样来源策略 $\pi^{(i)}$ 相对当前策略 $\pi_k$ 的陈旧性

### 4.5 直接约束的不可行性

定理3.1中的更新偏移惩罚项看似可以通过约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s)$ 来控制，但这实际上**不可行**：

> **观察3.1（更新偏移约束的不可行性）**
> 
> 设混合采样包含两个旧策略 $\pi^{(1)}$ 和 $\pi^{(2)}$，若存在某状态 $s$ 使 $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) > 2\delta$，则不存在策略 $\pi_{k+1}$ 同时满足 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(1)}; s) \leq \delta$ 与 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(2)}; s) \leq \delta$。

**证明**：由三角不等式，若同时满足两约束，则 $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) \leq 2\delta$，矛盾。

**问题根源**：更新偏移惩罚项将 $\pi_{k+1}$ 与历史策略族 $\{\pi^{(i)}\}$ 直接耦合，而后者的内部结构是历史训练的产物，不受当前更新控制。

### 4.6 三角不等式分解

解决方案是利用TV距离的三角不等式：

$$
D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \leq D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s) + D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)
$$

这将耦合约束拆分为两个独立部分：

- **更新增量偏移** $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)$：新策略相对当前策略的偏离，**可由优化侧控制**
- **采样陈旧性** $D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)$：当前策略相对各旧策略的偏离，**需由采样侧控制**

定义：

$$
U_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)\big], \quad S_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\big]
$$

> **推论3.2（分解后的单调提升下界）**
> 
> $$
> J(\pi_{k+1}) - J(\pi_k) \geq L_{\beta^{(k)}}(\pi_{k+1}) - \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} U_k - \left( \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} + \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \right) S_k
> $$

**为何分解能解决问题？** 关键在于：分解后的 $U_k$ 只涉及新策略 $\pi_{k+1}$ 和当前策略 $\pi_k$，**与旧策略族 $\{\pi^{(i)}\}$ 的结构完全无关**。因此，无论旧策略之间差异多大，约束 $U_k$ 都是可行的——这正是观察3.1揭示的不可行性问题的解决之道。

这揭示了重要的工程原则——**职责分离**：

| 控制项                | 负责方   | 控制手段           |
| --------------------- | -------- | ------------------ |
| $U_k$（更新增量偏移） | 优化算法 | 策略裁剪           |
| $S_k$（采样陈旧性）   | 采样系统 | 数据过滤、版本窗口 |

## 第五部分：裁剪机制的理论基础

### 5.1 从TV距离到可计算量

推论3.2告诉我们，要保证单调提升，需要控制更新增量偏移 $U_k = \mathbb{E}[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)]$。但TV距离是分布层面的量，如何用样本来控制它？

关键桥梁是下面这个恒等式：

> **引理3.3（TV距离的比值差表示）**
> 
> 设策略 $\pi_1$ 的支撑覆盖 $\pi$ 和 $\pi_2$ 的支撑，则对任意状态分布 $\mu$：
> 
> $$
> \mathbb{E}_{s\sim \mu} \big[D_{\mathrm{TV}}(\pi, \pi_2; s)\big] = \frac{1}{2} \mathbb{E}_{s\sim \mu, a\sim\pi_1(\cdot|s)} \left| \frac{\pi(a|s)}{\pi_1(a|s)} - \frac{\pi_2(a|s)}{\pi_1(a|s)} \right|
> $$

**直观理解**：左边是两个分布的TV距离（需要遍历所有动作），右边是在 $\pi_1$ 下采样时两个重要性比值的差的绝对值。这使得我们可以用样本来估计和控制TV距离。

### 5.2 $U_k$ 的样本表示

利用引理3.3，取 $\pi = \pi_{k+1}$，$\pi_2 = \pi_k$，$\pi_1 = \pi^{(i)}$（采样来源策略），可得：

$$
U_k = \frac{1}{2} \mathbb{E}_{(s,i) \sim d_{\beta^{(k)}}, a \sim \pi^{(i)}(\cdot|s)} \left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right|
$$

记 $\rho_{k+1} := \frac{\pi_{k+1}(a\|s)}{\pi^{(i)}(a\|s)}$ 和 $\rho_k := \frac{\pi_k(a\|s)}{\pi^{(i)}(a\|s)}$，则：

$$
U_k = \frac{1}{2} \mathbb{E}_{(s,i,a) \sim \text{训练数据}} \big| \rho_{k+1} - \rho_k \big|
$$

这意味着：**如果我们能让每个样本上 $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$，就能保证 $U_k \leq \epsilon/2$**。

### 5.3 两种约束 $U_k$ 的方法

**方法一：直接约束比值差**

对每个样本 $(s, i, a)$，要求：

$$
\left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right| \leq \epsilon
$$

即裁剪区间为 $\left[\frac{\pi_k(a\|s)}{\pi^{(i)}(a\|s)} - \epsilon, \frac{\pi_k(a\|s)}{\pi^{(i)}(a\|s)} + \epsilon\right]$，**裁剪中心是 $\rho_k$ 而非 1**。

**方法二：约束增量比值**

注意到 $\rho_{k+1} - \rho_k = \rho_k \cdot \left(\frac{\pi_{k+1}}{\pi_k} - 1\right)$，因此：

$$
|\rho_{k+1} - \rho_k| = \rho_k \cdot \left|\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right|
$$

如果约束 $\left\lvert\frac{\pi_{k+1}(a\|s)}{\pi_k(a\|s)} - 1\right\rvert \leq \epsilon$，由于 $\mathbb{E}_{a\sim\pi^{(i)}}[\rho_k] = 1$，可证 $U_k \leq \epsilon/2$。

这种方法直接对 $\pi_{k+1}/\pi_k$ 以 1 为中心裁剪，**完全不涉及旧策略 $\pi^{(i)}$**，我们给出三种裁剪机制的完整目标函数。设当前样本来自旧策略 $\pi^{(i)}$，记：
- $\rho_{k+1} = \frac{\pi_{k+1}(a\|s)}{\pi^{(i)}(a\|s)}$（新策略相对采样策略的比值）
- $\rho_k = \frac{\pi_k(a\|s)}{\pi^{(i)}(a\|s)}$（当前策略相对采样策略的比值）
- $r = \frac{\pi_{k+1}(a\|s)}{\pi_k(a\|s)}$（新策略相对当前策略的增量比值）

**标准PPO**：以 1 为中心裁剪 $\rho_{k+1}$

$$
L^{\mathrm{PPO}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\pi^{(i)}}, \; \mathrm{clip}(\rho_{k+1}, 1-\epsilon, 1+\epsilon) \cdot A^{\pi^{(i)}} \right) \right]
$$

**方法一**：以 $\rho_k$ 为中心裁剪 $\rho_{k+1}$

$$
L^{\mathrm{M1}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\beta^{(k)}}, \; \mathrm{clip}(\rho_{k+1}, \rho_k-\epsilon, \rho_k+\epsilon) \cdot A^{\beta^{(k)}} \right) \right]
$$

**方法二**：以 1 为中心裁剪增量比值 $r$

$$
L^{\mathrm{M2}} = \mathbb{E} \left[ \min\left( r \cdot \hat{A}, \; \mathrm{clip}(r, 1-\epsilon, 1+\epsilon) \cdot \hat{A} \right) \right]
$$

其中 $\hat{A} = \rho_k \cdot A^{\beta^{(k)}}$ 是经过重要性加权的优势估计。

### 5.5 三种方法的对比

**表5.1　三种裁剪机制的对比**

| 方法    | 裁剪变量                           | 裁剪中心                   | 裁剪区间                             | 约束的TV距离                            |
| ------- | ---------------------------------- | -------------------------- | ------------------------------------ | --------------------------------------- |
| 标准PPO | $\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$ | $1$                        | $[1-\epsilon, 1+\epsilon]$           | $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$ |
| 方法一  | $\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$ | $\rho_k = \pi_k/\pi^{(i)}$ | $[\rho_k-\epsilon, \rho_k+\epsilon]$ | $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$     |
| 方法二  | $r = \pi_{k+1}/\pi_k$              | $1$                        | $[1-\epsilon, 1+\epsilon]$           | $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$     |

**标准PPO在多策略混合下的根本问题**

标准PPO约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$，要求新策略同时接近所有采样来源策略。由观察3.1，当各旧策略 $\pi^{(1)}, \pi^{(2)}, \ldots$ 之间差异显著时，**不存在能同时满足所有约束的 $\pi_{k+1}$**。这导致信赖域交集收缩甚至为空，更新被最陈旧的策略所限制。

**方法一与方法二的共同优势**

两者都约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$——新策略相对**当前策略**（而非采样策略）的偏离。由于 $\pi_k$ 是唯一确定的，这个约束对所有来源的样本一致，完全规避了不可行性问题。

**方法一 vs 方法二**

| 比较维度                   | 方法一（自适应裁剪）                      | 方法二（增量裁剪）                          |
| -------------------------- | ----------------------------------------- | ------------------------------------------- |
| 陈旧样本（$\rho_k \gg 1$） | 自动收紧约束，更保守                      | 可能产生大梯度方差                          |
| LLM大词表低概率token       | 允许较大绝对变化（加法型）                | 绝对变化受限（乘法型）                      |
| 实现复杂度                 | 需存储 $\pi^{(i)}(a\|s)$ 和 $\pi_k(a\|s)$ | 仅需 $\pi_k(a\|s)$                          |
| 优势函数                   | 使用 $A^{\beta^{(k)}}$                    | 使用加权优势 $\rho_k \cdot A^{\beta^{(k)}}$ |

**详细解释**：

**(一) 陈旧样本处理**

当样本来自很旧的策略时，$\rho_k = \pi_k/\pi^{(i)}$ 可能很大。

- 方法二的被积函数为 $\rho_k \cdot \lvert r - 1\rvert$，即便 $\lvert r-1\rvert \leq \epsilon$，被积函数仍可达 $\epsilon \cdot \rho_k$，产生尖峰。
- 方法一直接约束 $\lvert\rho_{k+1} - \rho_k\rvert \leq \epsilon$，被积函数上界恒为 $\epsilon$，不受 $\rho_k$ 放大。

**(二) LLM大词表问题**

大语言模型词表规模巨大，大量token概率极小。

- 方法二约束 $\pi_{k+1} \in [(1-\epsilon)\pi_k, (1+\epsilon)\pi_k]$，这是**乘法型约束**：若 $\pi_k(a\|s) = 10^{-6}$，允许的绝对变化仅为 $\epsilon \times 10^{-6}$。
- 方法一约束 $\lvert\pi_{k+1} - \pi_k\rvert \leq \epsilon \cdot \pi^{(i)}$，这是**加法型约束**：若该token在旧策略下概率较高（如 $\pi^{(i)}(a\|s) = 0.1$），即便当前概率很低，也允许较快提升。

### 5.6 采样陈旧性的控制

推论3.2表明，$S_k$ 同样影响单调提升下界，但它**无法通过优化侧裁剪控制**，需由采样系统实现：

**(一) 丢弃陈旧数据**

设定阈值 $\epsilon_{\mathrm{stale}}$，对每个样本计算 $\lvert\rho_k - 1\rvert = \lvert\pi_k(a\|s)/\pi^{(i)}(a\|s) - 1\rvert$，丢弃超过阈值者。

**(二) 控制策略版本窗口**

限制混合采样的旧策略版本数量，如仅用最近 $W$ 个版本的数据。

### 5.7 裁剪的操作含义

最后，需要澄清裁剪与理论下界的关系。

推论3.2中，$U_k$ 的系数 $C_{\pi_{k+1},\beta^{(k)}}$ 依赖于新策略 $\pi_{k+1}$，因此惩罚项**不能简单替换为常数**。正确的操作含义是：

> **在 $U_k \leq \epsilon/2$ 的约束下，最大化代理目标 $L_{\beta^{(k)}}(\pi_{k+1})$**

裁剪目标函数正是这一约束优化的实现——通过裁剪**硬性限制**更新幅度，确保 $U_k$ 可控；在此前提下，梯度上升提升代理目标，从而为策略单调改进提供保障。

### 5.8 本节小结

本节建立了裁剪机制的理论基础：

1. **引理3.3**将TV距离转化为样本层面的比值差，是连接理论与实现的桥梁
2. **两种约束方法**：方法一（自适应裁剪中心）和方法二（固定增量裁剪），均保证 $U_k \leq \epsilon/2$
3. **与标准PPO对比**：标准PPO约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$，在多策略混合下不可行；方法一/二约束 $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$，规避了该问题
4. **方法选择**：陈旧性高或LLM大词表场景推荐方法一；实现简洁优先推荐方法二
5. **$S_k$ 的控制**由采样侧负责，通过数据过滤和版本窗口实现
6. **裁剪是约束优化**：在 $U_k$ 约束下最大化代理目标

## 第六部分：轨迹级与步/段级混合的比较

### 6.1 两类机制的核心差异

两类混合机制的本质区别在于索引转移核的结构：

- **轨迹级混合**：$q(i'\|i) = \mathbf{1}\{i'=i\}$，索引永不改变
- **步/段级混合**：$\sigma(i) > 0$，允许轨迹内切换

与常见工程术语的对应关系是：

- 这里的**轨迹级混合**可以大致理解为"**常规异步训练**"的一个理想化抽象：数据按整条轨迹/episode 归属到某个策略版本；
- 这里的**步/段级混合**可以大致理解为"**partial rollout**"的一个抽象：由于 actor 与 learner 异步、且 segment 边界处可能刷新到新策略版本，用索引转移核允许"轨迹内部版本切换"可以更好地近似刻画这种现象。

关键分水岭是**引理2.1的结构简化是否成立**：轨迹级混合满足优势函数还原；步/段级混合一般不满足，因为未来回报受索引转移核影响。

### 6.2 采样陈旧性 $S_k$ 的差异

**轨迹级混合**的陈旧性来源于：混合权重 $\alpha_i^{(k)}$ 在新策略发布后仍对旧策略保留质量。

**步/段级混合**具有**指数压缩效应**：考虑从旧到新以概率 $\sigma$ 切换的简化模型，折扣访问分布下旧索引的边缘质量为 $\frac{1-\gamma}{1-\gamma(1-\sigma)}$。只要 $\sigma \gg 1-\gamma$，旧策略权重即可被显著压缩。

### 6.3 代理目标估计的差异

**轨迹级混合**：优势函数还原为 $A^{\pi^{(i)}}(s,a)$，估计路径清晰。

**步/段级混合的优势替代偏差**：若沿用单策略优势估计，将产生系统性偏差。原因是 $A^{\beta^{(k)}}((s,i),a)$ 需要对未来索引切换取期望，而 $A^{\pi^{(i)}}(s,a)$ 隐含"未来始终沿用 $\pi^{(i)}$"的假设。

**Bandit设定下的统一**：在单步episode的LLM训练中，无后续状态转移，两类机制的估计问题统一，无上述偏差。

### 6.4 方差放大风险

步/段级混合还有一个隐患：即便单步重要性比值被裁剪，长轨迹下多步噪声叠加仍会放大梯度估计方差。当每次更新的策略变化幅度较大时，轨迹内部的"行为突变"可能引发更重尾的比值分布。这也是下表中"策略变化幅度大"场景推荐轨迹级混合的原因。

### 6.5 适用场景

**表6.1　两类混合机制的适用场景**

| 场景特征                 | 推荐机制 | 理由             |
| ------------------------ | -------- | ---------------- |
| 长轨迹、高频更新、强异步 | 步/段级  | 可显著压缩 $S_k$ |
| 短轨迹（非Bandit）       | 轨迹级   | $S_k$ 自然较低   |
| 每次更新策略变化幅度大   | 轨迹级   | 避免方差放大     |
| 单步episode（Bandit）    | 均可     | 按实现便利选择   |
| 需要折中方案             | 段级     | 在自然边界切换   |

**核心权衡**：步/段级混合在采样侧更强（快速去陈旧），轨迹级混合在估计侧更稳（代理目标易估计）。

## 第七部分：训推不一致的处理

### 7.1 问题背景

在大规模分布式训练中，推理端和训练端的策略可能不一致：

- **数值实现差异**：softmax归一化、量化、核融合
- **解码规则差异**：温度缩放、top-p/top-k采样

设训练侧建模的行为策略为 $\pi^{(i)}$，而推理端实际采样的策略为 $\hat{\pi}^{(i)}$。

### 7.2 有效陈旧性

定义**有效陈旧性**：

$$
\hat{S}_k := \mathbb{E}_{(s,i) \sim d_{\hat{\beta}^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_k, \hat{\pi}^{(i)}; s) \big]
$$

该定义同时覆盖版本陈旧性与训推实现差异。

### 7.3 可操作控制

由引理3.3，$\hat{S}_k$ 可表示为样本级可计算形式。给定阈值 $\epsilon_{\mathrm{stale}}$，若训练仅使用满足 $\lvert\pi_k(a\|s)/\hat{\pi}^{(i)}(a\|s) - 1\rvert \leq \epsilon_{\mathrm{stale}}$ 的样本，则 $\hat{S}_k \leq \epsilon_{\mathrm{stale}}/2$。

**关键实现要点**：

1. **行为分母对齐**：损失中的行为概率应使用推理端记录的 $\hat{\pi}^{(i)}(a\|s)$
2. **概率平滑**：若推理端有截断（如top-k），需确保比值合法

## 总结：实践指南

### 核心理论框架

单调提升下界的结构为：

$$
J(\pi_{k+1}) - J(\pi_k) \geq \underbrace{L_{\beta^{(k)}}(\pi_{k+1})}_{\text{代理目标}} - \underbrace{C_1 \cdot U_k}_{\text{更新偏移惩罚}} - \underbrace{C_2 \cdot S_k}_{\text{采样陈旧性惩罚}}
$$

### 职责分离原则

| 控制项 | 负责方   | 控制手段 | 具体操作                  |
| ------ | -------- | -------- | ------------------------- |
| $U_k$  | 优化算法 | 策略裁剪 | 对 $\pi_{k+1}/\pi_k$ 裁剪 |
| $S_k$  | 采样系统 | 数据过滤 | 丢弃陈旧样本              |
| $S_k$  | 采样系统 | 版本窗口 | 仅用最近 $W$ 个版本       |

### 裁剪方法选择

| 场景         | 推荐方法         | 理由                    |
| ------------ | ---------------- | ----------------------- |
| 陈旧性较高   | 方法一（自适应） | 自动对陈旧样本收紧约束  |
| 实现简洁优先 | 方法二（增量）   | 无需存储旧策略信息      |
| LLM大词表    | 方法一           | 避免低概率token更新过慢 |

### 训推不一致处理

- 使用推理端记录的 $\hat{\pi}^{(i)}$ 作为行为分母
- 通过样本过滤压缩有效陈旧性

## 附录：关键符号速查表

| 符号                                              | 含义                               |
| ------------------------------------------------- | ---------------------------------- |
| $\pi_k$, $\pi^{(i)}$                              | 第 $k$ 轮最新策略，第 $i$ 个旧策略 |
| $d_\pi(s)$, $A^\pi(s,a)$                          | 折扣状态访问分布，优势函数         |
| $D_{\mathrm{TV}}(\pi, \pi'; s)$                   | 两策略在状态 $s$ 上的TV距离        |
| $\beta^{(k)}(a \mid s, i) := \pi^{(i)}(a \mid s)$ | 第 $k$ 轮混合行为策略              |
| $q(i' \mid i)$, $\alpha_i^{(k)}$                  | 索引转移核，索引初始分布           |
| $U_k$, $S_k$                                      | 更新增量偏移，采样陈旧性           |
| $\epsilon$, $\epsilon_{\mathrm{stale}}$, $W$      | 裁剪半径，陈旧性阈值，版本窗口     |
| $C_{\pi,\pi_k}$                                   | 期望优势上界常数                   |

## 参考文献

1. John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. "Trust Region Policy Optimization" (TRPO). arXiv:1502.05477. <https://arxiv.org/abs/1502.05477>

2. Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel. "Constrained Policy Optimization" (CPO). arXiv:1705.10528. <https://arxiv.org/abs/1705.10528>

3. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. "Proximal Policy Optimization Algorithms" (PPO). arXiv:1707.06347. <https://arxiv.org/abs/1707.06347>

4. James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras. "Generalized Proximal Policy Optimization with Sample Reuse" (GePPO). arXiv:2111.00072. <https://arxiv.org/abs/2111.00072>

5. Yuzhen Zhou, Jiajun Li, Yusheng Su, et al. "APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation" (APRIL; partial rollout). arXiv:2509.18521. <https://arxiv.org/abs/2509.18521>

6. Jacob Hilton, Karl Cobbe, John Schulman. "Batch size-invariance for policy optimization" (Decoupled PPO). arXiv:2110.00641. <https://arxiv.org/abs/2110.00641>

```bibtex
@misc{WangZhang2025OffPolicyLLMRL,
	author       = {Wang, Xihuai and Zhang, Shao},
	title        = {Off-Policy Training in LLM Reinforcement Learning: From Theory to Practice},
	year         = {2025},
	month        = dec,
	day          = {17},
	url          = {https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html},
	urldate      = {2025-12-17}
}
```
