---
layout: post
title: "Off-Policy Training in Large Language Model Reinforcement Learning: From Theory to Practice"
date: 2025-12-17
description: "A systematic derivation of off-policy training theory in LLM reinforcement learning: starting from the performance improvement lower bound with single-policy sampling, extending to multi-policy static/dynamic mixture sampling, providing sufficient conditions for monotonic improvement, and using triangle inequality decomposition to split constraints into update increment drift (controllable by optimization) and sampling staleness (controllable by sampling), ultimately landing on actionable clipping mechanisms and data filtering strategies."
categories: reinforcement-learning
lang: en
---

* TOC
{:toc}

[中文版本](/reinforcement-learning/2025/12/17/offpolicy-zh.html)

## Introduction: Why Should We Care About "Off-Policy"?

Imagine this scenario: you're using reinforcement learning to train a large language model to better answer questions. Ideally, after the model generates a batch of responses, you would immediately update the model with this data, then generate new data with the updated model, and so on. This approach of "updating with data from the same policy that generated it" is called **on-policy** training.

But reality isn't so simple. In large-scale distributed training, hundreds of GPUs generate data in parallel, while model updates take time. When a new model is released, much data generated by "old version" models hasn't been used yet—discarding it is wasteful, but using it raises concerns about "stale data" affecting training effectiveness.

This is the core problem facing **off-policy** training: **Can we guarantee continuous performance improvement when using data collected by old policies to update new policies?**

This article will systematically answer this question. Starting from foundational theory, we'll progressively derive actionable conditions that tell you: under what circumstances can mixing data from multiple policy versions still guarantee monotonic training improvement.


## Part One: Theoretical Foundations

### 1.1 Basic Setup

We consider a standard Markov Decision Process (MDP), consisting of state space $\mathcal{S}$, action space $\mathcal{A}$, transition probability $p(s'|s,a)$, reward function $r(s,a)$, initial distribution $\rho_0$, and discount factor $\gamma \in (0,1)$.

The **expected cumulative discounted return** of policy $\pi$ is:

$$
J(\pi) := \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid \pi\right]
$$

The **discounted state visitation distribution** is defined as the weighted frequency of visiting each state during long-term policy execution:

$$
d_\pi(s) := (1-\gamma) \sum_{t=0}^{\infty} \gamma^t \Pr(s_t = s \mid \pi)
$$

The **advantage function** measures the relative merit of action $a$ compared to the policy's average:

$$
A^\pi(s,a) := Q^\pi(s,a) - V^\pi(s)
$$

The **total variation distance** (TV distance) measures the difference in action distributions between two policies at state $s$:

$$
D_{\mathrm{TV}}(\pi, \pi'; s) := \frac{1}{2} \sum_{a \in \mathcal{A}} |\pi(a \mid s) - \pi'(a \mid s)|
$$

### 1.2 Core Tool: Policy Performance Difference Lemma

The cornerstone of the entire theory is this concise result:

> **Lemma 1.1 (Policy Performance Difference Lemma)**
> 
> For any policies $\pi_k$ (old) and $\pi$ (new), the performance difference can be expressed as:
> 
> $$
> J(\pi) - J(\pi_k) = \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_\pi}\left[ \mathbb{E}_{a \sim \pi(\cdot \mid s)}[A^{\pi_k}(s,a)] \right]
> $$

**Intuitive understanding**: How much better the new policy is than the old equals the "average advantage" obtained by selecting actions with the new policy under the state distribution visited by the new policy.


## Part Two: Performance Improvement Lower Bound with Single-Policy Sampling

### 2.1 Distribution Mismatch Problem

The policy performance difference lemma has a practical issue: the expectation on the right side is computed under $d_\pi$ (the new policy's state distribution), but we can only sample from $d_{\pi_k}$ (the old policy).

The solution is to decompose the expectation into "expectation under old distribution + deviation term," then control the deviation. The key question is: **What is the quantitative relationship between state distribution differences and policy differences?**

### 2.2 Controlling State Distribution Differences

> **Lemma 1.2 (Relationship Between State Distribution Difference and Policy TV Distance)**
> 
> $$
> \|d_\pi - d_{\pi_k}\|_1 \leq \frac{2\gamma}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]
> $$

**Physical meaning**: Small differences between policies in action space get "amplified" through environment dynamics into differences in state visitation distributions. The coefficient $\frac{\gamma}{1-\gamma}$ reflects the **temporal accumulation effect**—in long-horizon tasks ($\gamma$ close to 1), the amplification effect is stronger.

**Proof sketch**: By deriving the fixed-point equation for discounted visitation distribution and using the $\ell_1$ non-expansiveness of stochastic matrices, one can show that state distribution differences are amplified by policy differences through transition dynamics, with amplification factor exactly $\frac{\gamma}{1-\gamma}$.

### 2.3 Policy Performance Improvement Lower Bound

> **Theorem 1.1 (Policy Performance Improvement Lower Bound)**
> 
> Define the expected advantage upper bound constant $C_{\pi,\pi_k} := \max_{s} | \mathbb{E}_{a \sim \pi}[A^{\pi_k}(s,a)] |$, then:
> 
> $$
> J(\pi) - J(\pi_k) \geq L_{\pi_k}(\pi) - \frac{2\gamma C_{\pi,\pi_k}}{(1-\gamma)^2} \mathbb{E}_{s \sim d_{\pi_k}} \big[ D_{\mathrm{TV}}(\pi, \pi_k; s) \big]
> $$
> 
> where the **surrogate objective** is:
> 
> $$
> L_{\pi_k}(\pi) := \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_k}, a \sim \pi_k} \left[ \frac{\pi(a \mid s)}{\pi_k(a \mid s)} A^{\pi_k}(s,a) \right]
> $$

This lower bound consists of two parts:

1. **Surrogate objective** $L_{\pi_k}(\pi)$: Can be directly estimated using old policy data through importance sampling; this is the optimization objective of TRPO/PPO.

2. **Policy drift penalty**: Increases with the TV distance between new and old policies, explaining why PPO needs to limit update magnitude.

**Core conclusion**: Maximizing the surrogate objective while controlling policy drift guarantees performance improvement.


## Part Three: Multi-Policy Static Mixture Sampling

### 3.1 Practical Scenario

In practical training, a batch of data may come from multiple policy versions $\{\pi^{(1)}, \ldots, \pi^{(M)}\}$, with proportions $\alpha_1, \ldots, \alpha_M$. How do we extend Theorem 1.1 to this setting?

### 3.2 Core Idea: Extended State Space

The solution is an elegant modeling technique: **treat the policy version index as part of the state**.

Define the extended state space $\tilde{\mathcal{S}} := \mathcal{S} \times \mathcal{I}$, where $\mathcal{I} = \{1, \ldots, M\}$ is the policy index set. Under extended state $(s, i)$, the **mixture behavior policy** is defined as $\beta(a \mid s, i) := \pi^{(i)}(a \mid s)$.

Index evolution is characterized by the **index transition kernel** $q(i' \mid i)$. The extended MDP inherits the original MDP's rewards and environment transitions, with indices evolving independently according to $q(i'|i)$.

This technique works because the new policy $\pi$'s return in the extended MDP equals the return in the original MDP, allowing direct application of Theorem 1.1.

### 3.3 Structural Simplification for Trajectory-Level Mixing

The most common case is **using only one old policy per trajectory**: at trajectory start, sample index $I_0 \sim \alpha$, and use $\pi^{(I_0)}$ throughout the trajectory. Here the index transition kernel is identity transition: $q(i' \mid i) = \mathbf{1}_{i'=i}$.

From an engineering implementation perspective, in many **actor-learner asynchronous training** setups (if the sampling and training sides organize data by "whole trajectory/complete episode belonging to a certain policy version"), this can approximately correspond to the **trajectory-level mixing** here: an actor uses a fixed policy snapshot within a sampling unit to generate data, and the learner then mixes whole-trajectory data from different versions for updates. We say "approximately" because different systems don't have completely consistent definitions of "trajectory/sampling unit" boundaries.

> **Lemma 2.1 (Structural Simplification for Trajectory-Level Mixing)**
> 
> (a) Extended state visitation distribution decomposes as: $d_{\beta}(s, i) = \alpha_i \cdot d_{\pi^{(i)}}(s)$
> 
> (b) Advantage function reduces to: $A^{\beta}((s, i), a) = A^{\pi^{(i)}}(s, a)$

**Intuition for (b)**: Since the index never changes, **all future trajectories** starting from extended state $(s,i)$ are generated by the same policy $\pi^{(i)}$. Therefore, future cumulative returns are entirely determined by $\pi^{(i)}$, and value functions and advantage functions naturally reduce to those of $\pi^{(i)}$.

Thus, the mixture policy's return is the weighted average of each old policy's return: $J_{\mathrm{mix}} = \sum_{i=1}^{M} \alpha_i J(\pi^{(i)})$.

### 3.4 Performance Improvement Lower Bound for Trajectory-Level Mixing

> **Corollary 2.1 (Performance Improvement Lower Bound for Trajectory-Level Mixing)**
> 
> $$
> J(\pi) - \sum_{i=1}^{M} \alpha_i J(\pi^{(i)}) \geq \sum_{i=1}^{M} \alpha_i L_{\pi^{(i)}}(\pi) - \frac{2\gamma \max_i C_{\pi, \pi^{(i)}}}{(1-\gamma)^2} \sum_{i=1}^{M} \alpha_i \mathbb{E}_{s \sim d_{\pi^{(i)}}} \big[ D_{\mathrm{TV}}(\pi, \pi^{(i)}; s) \big]
> $$

This result shows: when mixing trajectories from multiple old policy versions for training, if we construct the loss using the importance ratio corresponding to each trajectory's old policy while controlling the drift between the new policy and each old policy, the new policy's performance has a clear improvement lower bound.


## Part Four: Dynamic Mixture Sampling and Monotonic Improvement Conditions

### 4.1 The Core Challenge

Part Three discussed **static mixing**—mixing weights $\alpha_i$ remain fixed. This section considers more general **dynamic mixing**—after a new policy is released, sampling gradually transitions to the new policy.

Previous results characterized "improvement of the new policy relative to the mixture behavior policy." But in actual training, what we really care about is: **Is the latest policy $\pi_{k+1}$ after each update monotonically better than the previous latest policy $\pi_k$?**

$$
J(\pi_{k+1}) \geq J(\pi_k)
$$

### 4.2 Unified Modeling Framework

Two typical forms of dynamic mixture sampling can both be characterized by index transition kernel $q(i'|i)$:

**Trajectory-level mixing** (can be loosely understood as an abstraction of typical asynchronous training; identity index transition): $q(i'|i) = \mathbf{1}\{i'=i\}$

**Step/segment-level mixing** (an abstraction of partial rollout / segment-wise sampling; allows switching): $q(i'|i) = (1-\sigma(i))\mathbf{1}\{i'=i\} + \sigma(i)\kappa(i'|i)$

where $\sigma(i)$ is the switching probability and $\kappa(\cdot|i)$ is the target index distribution.

### 4.3 Core Decomposition

By introducing mixture return $J_{\mathrm{mix}}^{(k)}$ as an intermediate bridge, the performance difference decomposes as:

$$
J(\pi_{k+1}) - J(\pi_k) = \underbrace{[J(\pi_{k+1}) - J_{\mathrm{mix}}^{(k)}]}_{\text{Improvement relative to mixture policy}} + \underbrace{[J_{\mathrm{mix}}^{(k)} - J(\pi_k)]}_{\text{Mixture bias term}}
$$

The first term can be handled using Theorem 1.1. The second term is the **mixture bias term**, which can be shown to satisfy:

$$
J_{\mathrm{mix}}^{(k)} - J(\pi_k) \geq -\frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
$$

### 4.4 Monotonic Improvement Lower Bound

Combining the above results yields the core theorem:

> **Theorem 3.1 (Monotonic Improvement Lower Bound Under Dynamic Mixture Sampling)**
> 
> $$
> \begin{aligned}
> J(\pi_{k+1}) - J(\pi_k) \geq\;& L_{\beta^{(k)}}(\pi_{k+1}) \\
> &- \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \big] \\
> &- \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[ D_{\mathrm{TV}}(\pi^{(i)}, \pi_k; s) \big]
> \end{aligned}
> $$

This lower bound reveals the necessity of **dual control**:
- **Update drift penalty**: Drift of new policy $\pi_{k+1}$ relative to sampling source policy $\pi^{(i)}$
- **Sampling staleness penalty**: Staleness of sampling source policy $\pi^{(i)}$ relative to current policy $\pi_k$

### 4.5 Infeasibility of Direct Constraints

The update drift penalty term in Theorem 3.1 might seem controllable by constraining $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s)$, but this is actually **infeasible**:

> **Observation 3.1 (Infeasibility of Update Drift Constraints)**
> 
> Suppose mixture sampling includes two old policies $\pi^{(1)}$ and $\pi^{(2)}$. If there exists some state $s$ such that $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) > 2\delta$, then no policy $\pi_{k+1}$ can simultaneously satisfy $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(1)}; s) \leq \delta$ and $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(2)}; s) \leq \delta$.

**Proof**: By the triangle inequality, if both constraints were satisfied, then $D_{\mathrm{TV}}(\pi^{(1)}, \pi^{(2)}; s) \leq 2\delta$, a contradiction.

**Root cause**: The update drift penalty term couples $\pi_{k+1}$ directly with the historical policy family $\{\pi^{(i)}\}$, whose internal structure is a product of historical training and not controllable by the current update.

### 4.6 Triangle Inequality Decomposition

The solution uses the triangle inequality for TV distance:

$$
D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)}; s) \leq D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s) + D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)
$$

This splits the coupled constraint into two independent parts:

- **Update increment drift** $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)$: Deviation of new policy from current policy, **controllable by optimization**
- **Sampling staleness** $D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)$: Deviation of current policy from each old policy, **needs to be controlled by sampling**

Define:

$$
U_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)\big], \quad S_k := \mathbb{E}_{(s,i)\sim d_{\beta^{(k)}}} \big[D_{\mathrm{TV}}(\pi_k, \pi^{(i)}; s)\big]
$$

> **Corollary 3.2 (Decomposed Monotonic Improvement Lower Bound)**
> 
> $$
> J(\pi_{k+1}) - J(\pi_k) \geq L_{\beta^{(k)}}(\pi_{k+1}) - \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} U_k - \left( \frac{2\gamma C_{\pi_{k+1},\beta^{(k)}}}{(1-\gamma)^2} + \frac{2\|A^{\pi_k}\|_\infty}{1-\gamma} \right) S_k
> $$

**Why does decomposition solve the problem?** The key is: after decomposition, $U_k$ only involves new policy $\pi_{k+1}$ and current policy $\pi_k$, **completely independent of the old policy family $\{\pi^{(i)}\}$'s structure**. Therefore, regardless of how different the old policies are from each other, constraining $U_k$ is always feasible—this is precisely the solution to the infeasibility problem revealed in Observation 3.1.

This reveals an important engineering principle—**separation of concerns**:

| Control Term                   | Responsible Party      | Control Method                 |
| :----------------------------- | :--------------------- | :----------------------------- |
| $U_k$ (Update increment drift) | Optimization algorithm | Policy clipping                |
| $S_k$ (Sampling staleness)     | Sampling system        | Data filtering, version window |


## Part Five: Theoretical Foundation of Clipping Mechanisms

### 5.1 From TV Distance to Computable Quantities

Corollary 3.2 tells us that to guarantee monotonic improvement, we need to control update increment drift $U_k = \mathbb{E}[D_{\mathrm{TV}}(\pi_{k+1}, \pi_k; s)]$. But TV distance is a distribution-level quantity—how do we control it using samples?

The key bridge is the following identity:

> **Lemma 3.3 (Ratio Difference Representation of TV Distance)**
> 
> Let policy $\pi_1$'s support cover the supports of $\pi$ and $\pi_2$, then for any state distribution $\mu$:
> 
> $$
> \mathbb{E}_{s\sim \mu} \big[D_{\mathrm{TV}}(\pi, \pi_2; s)\big] = \frac{1}{2} \mathbb{E}_{s\sim \mu, a\sim\pi_1(\cdot|s)} \left| \frac{\pi(a|s)}{\pi_1(a|s)} - \frac{\pi_2(a|s)}{\pi_1(a|s)} \right|
> $$

**Intuitive understanding**: The left side is the TV distance between two distributions (requiring enumeration over all actions), while the right side is the absolute difference of two importance ratios when sampling under $\pi_1$. This allows us to estimate and control TV distance using samples.

### 5.2 Sample Representation of $U_k$

Using Lemma 3.3, taking $\pi = \pi_{k+1}$, $\pi_2 = \pi_k$, $\pi_1 = \pi^{(i)}$ (the sampling source policy), we get:

$$
U_k = \frac{1}{2} \mathbb{E}_{(s,i) \sim d_{\beta^{(k)}}, a \sim \pi^{(i)}(\cdot|s)} \left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right|
$$

Let $\rho_{k+1} := \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)}$ and $\rho_k := \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)}$, then:

$$
U_k = \frac{1}{2} \mathbb{E}_{(s,i,a) \sim \text{training data}} \big| \rho_{k+1} - \rho_k \big|
$$

This means: **if we can ensure $|\rho_{k+1} - \rho_k| \leq \epsilon$ on each sample, we can guarantee $U_k \leq \epsilon/2$**.

### 5.3 Two Methods for Constraining $U_k$

**Method One: Direct Constraint on Ratio Difference**

For each sample $(s, i, a)$, require:

$$
\left| \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)} - \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} \right| \leq \epsilon
$$

That is, the clipping interval is $\left[\frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} - \epsilon, \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)} + \epsilon\right]$, with **clipping center at $\rho_k$ rather than 1**.

**Method Two: Constraint on Incremental Ratio**

Note that $\rho_{k+1} - \rho_k = \rho_k \cdot \left(\frac{\pi_{k+1}}{\pi_k} - 1\right)$, therefore:

$$
|\rho_{k+1} - \rho_k| = \rho_k \cdot \left|\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right|
$$

If we constrain $\left|\frac{\pi_{k+1}(a|s)}{\pi_k(a|s)} - 1\right| \leq \epsilon$, since $\mathbb{E}_{a\sim\pi^{(i)}}[\rho_k] = 1$, we can prove $U_k \leq \epsilon/2$.

This method directly clips $\pi_{k+1}/\pi_k$ centered at 1, **without involving the old policy $\pi^{(i)}$ at all**.

### 5.4 Objective Functions for Three Clipping Mechanisms

For comparison, we present the complete objective functions for three clipping mechanisms. Suppose the current sample comes from old policy $\pi^{(i)}$, and denote:
- $\rho_{k+1} = \frac{\pi_{k+1}(a|s)}{\pi^{(i)}(a|s)}$ (ratio of new policy to sampling policy)
- $\rho_k = \frac{\pi_k(a|s)}{\pi^{(i)}(a|s)}$ (ratio of current policy to sampling policy)
- $r = \frac{\pi_{k+1}(a|s)}{\pi_k(a|s)}$ (incremental ratio of new policy to current policy)

**Standard PPO**: Clip $\rho_{k+1}$ centered at 1

$$
L^{\mathrm{PPO}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\pi^{(i)}}, \; \mathrm{clip}(\rho_{k+1}, 1-\epsilon, 1+\epsilon) \cdot A^{\pi^{(i)}} \right) \right]
$$

**Method One**: Clip $\rho_{k+1}$ centered at $\rho_k$

$$
L^{\mathrm{M1}} = \mathbb{E} \left[ \min\left( \rho_{k+1} \cdot A^{\beta^{(k)}}, \; \mathrm{clip}(\rho_{k+1}, \rho_k-\epsilon, \rho_k+\epsilon) \cdot A^{\beta^{(k)}} \right) \right]
$$

**Method Two**: Clip incremental ratio $r$ centered at 1

$$
L^{\mathrm{M2}} = \mathbb{E} \left[ \min\left( r \cdot \hat{A}, \; \mathrm{clip}(r, 1-\epsilon, 1+\epsilon) \cdot \hat{A} \right) \right]
$$

where $\hat{A} = \rho_k \cdot A^{\beta^{(k)}}$ is the importance-weighted advantage estimate.

### 5.5 Comparison of Three Methods

**Table 5.1: Comparison of Three Clipping Mechanisms**

| Method       | Clipped Variable                   | Clipping Center            | Clipping Interval                    | Constrained TV Distance                 |
| :----------- | :--------------------------------- | :------------------------- | :----------------------------------- | :-------------------------------------- |
| Standard PPO | $\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$ | $1$                        | $[1-\epsilon, 1+\epsilon]$           | $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$ |
| Method One   | $\rho_{k+1} = \pi_{k+1}/\pi^{(i)}$ | $\rho_k = \pi_k/\pi^{(i)}$ | $[\rho_k-\epsilon, \rho_k+\epsilon]$ | $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$     |
| Method Two   | $r = \pi_{k+1}/\pi_k$              | $1$                        | $[1-\epsilon, 1+\epsilon]$           | $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$     |

**The Fundamental Problem of Standard PPO Under Multi-Policy Mixing**

Standard PPO constrains $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$, requiring the new policy to be close to all sampling source policies simultaneously. By Observation 3.1, when old policies $\pi^{(1)}, \pi^{(2)}, \ldots$ differ significantly from each other, **no $\pi_{k+1}$ can simultaneously satisfy all constraints**. This causes the trust region intersection to shrink or even become empty, with updates limited by the stalest policy.

**Common Advantage of Methods One and Two**

Both constrain $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$—the deviation of the new policy from the **current policy** (not the sampling policy). Since $\pi_k$ is uniquely determined, this constraint is consistent across samples from all sources, completely avoiding the infeasibility problem.

**Method One vs Method Two**

| Comparison Dimension                        | Method One (Adaptive Clipping)                       | Method Two (Incremental Clipping)                      |
| :------------------------------------------ | :--------------------------------------------------- | :----------------------------------------------------- |
| Stale samples ($\rho_k \gg 1$)              | Automatically tightens constraint, more conservative | May produce large gradient variance                    |
| LLM large vocabulary low-probability tokens | Allows larger absolute changes (additive)            | Absolute change is limited (multiplicative)            |
| Implementation complexity                   | Needs to store $\pi^{(i)}(a\|s)$ and $\pi_k(a\|s)$   | Only needs $\pi_k(a\|s)$                               |
| Advantage function                          | Uses $A^{\beta^{(k)}}$                               | Uses weighted advantage $\rho_k \cdot A^{\beta^{(k)}}$ |

**Detailed Explanation**:

**(1) Stale Sample Handling**

When samples come from very old policies, $\rho_k = \pi_k/\pi^{(i)}$ can be very large.

- Method Two's integrand is $\rho_k \cdot |r - 1|$; even if $|r-1| \leq \epsilon$, the integrand can reach $\epsilon \cdot \rho_k$, producing spikes.
- Method One directly constrains $|\rho_{k+1} - \rho_k| \leq \epsilon$, with integrand upper bound always $\epsilon$, not amplified by $\rho_k$.

**(2) LLM Large Vocabulary Problem**

Large language model vocabularies are enormous, with many tokens having extremely small probabilities.

- Method Two constrains $\pi_{k+1} \in [(1-\epsilon)\pi_k, (1+\epsilon)\pi_k]$, which is a **multiplicative constraint**: if $\pi_k(a|s) = 10^{-6}$, the allowed absolute change is only $\epsilon \times 10^{-6}$.
- Method One constrains $|\pi_{k+1} - \pi_k| \leq \epsilon \cdot \pi^{(i)}$, which is an **additive constraint**: if that token has higher probability under the old policy (e.g., $\pi^{(i)}(a|s) = 0.1$), even if current probability is very low, faster increases are allowed.

### 5.6 Controlling Sampling Staleness

Corollary 3 affects the monotonic improvement lower bound, but it **cannot be controlled by optimization-side clipping** and needs to be implemented by the sampling system:

**(1) Discard Stale Data**

Set threshold $\epsilon_{\mathrm{stale}}$, compute $|\rho_k - 1| = |\pi_k(a|s)/\pi^{(i)}(a|s) - 1|$ for each sample, and discard those exceeding the threshold.

**(2) Control Policy Version Window**

Limit the number of old policy versions in mixture sampling, e.g., only use data from the most recent $W$ versions.

### 5.7 Operational Meaning of Clipping

Finally, we need to clarify the relationship between clipping and the theoretical lower bound.

In Corollary 3.2, the coefficient $C_{\pi_{k+1},\beta^{(k)}}$ of $U_k$ depends on the new policy $\pi_{k+1}$, so the penalty term **cannot simply be replaced by a constant**. The correct operational meaning is:

> **Maximize the surrogate objective $L_{\beta^{(k)}}(\pi_{k+1})$ subject to the constraint $U_k \leq \epsilon/2$**

The clipping objective function is precisely the implementation of this constrained optimization—clipping **strictly limits** update magnitude to ensure $U_k$ is controlled; under this premise, gradient ascent improves the surrogate objective, thereby providing guarantees for policy monotonic improvement.

### 5.8 Section Summary

This section established the theoretical foundation of clipping mechanisms:

1. **Lemma 3.3** converts TV distance to sample-level ratio differences, serving as the bridge between theory and implementation
2. **Two constraint methods**: Method One (adaptive clipping center) and Method Two (fixed incremental clipping), both guarantee $U_k \leq \epsilon/2$
3. **Comparison with standard PPO**: Standard PPO constrains $D_{\mathrm{TV}}(\pi_{k+1}, \pi^{(i)})$, which is infeasible under multi-policy mixing; Methods One/Two constrain $D_{\mathrm{TV}}(\pi_{k+1}, \pi_k)$, avoiding this problem
4. **Method selection**: Method One recommended for high staleness or LLM large vocabulary scenarios; Method Two recommended when implementation simplicity is prioritized
5. **$S_k$ control** is the responsibility of the sampling side, implemented through data filtering and version windows
6. **Clipping is constrained optimization**: Maximize surrogate objective under $U_k$ constraint


## Part Six: Comparison of Trajectory-Level and Step/Segment-Level Mixing

### 6.1 Core Differences Between the Two Mechanisms

The essential difference between the two mixing mechanisms lies in the structure of the index transition kernel:

- **Trajectory-level mixing**: $q(i'|i) = \mathbf{1}\{i'=i\}$, index never changes
- **Step/segment-level mixing**: $\sigma(i) > 0$, allows within-trajectory switching

The correspondence with common engineering terminology is:

- **Trajectory-level mixing** here can be roughly understood as an idealized abstraction of "**typical asynchronous training**": data is attributed to a certain policy version by whole trajectory/episode;
- **Step/segment-level mixing** here can be roughly understood as an abstraction of "**partial rollout**": because actors and learners are asynchronous, and segment boundaries may refresh to new policy versions, using an index transition kernel that allows "within-trajectory version switching" can better approximate this phenomenon.

The key watershed is **whether the structural simplification of Lemma 2.1 holds**: trajectory-level mixing satisfies advantage function reduction; step/segment-level mixing generally does not, because future returns are affected by the index transition kernel.

### 6.2 Differences in Sampling Staleness $S_k$

**Trajectory-level mixing**'s staleness comes from: mixing weights $\alpha_i^{(k)}$ still retain mass on old policies after new policy release.

**Step/segment-level mixing** has an **exponential compression effect**: Consider a simplified model that switches from old to new with probability $\sigma$; under discounted visitation distribution, the marginal mass on old indices is $\frac{1-\gamma}{1-\gamma(1-\sigma)}$. As long as $\sigma \gg 1-\gamma$, old policy weights can be significantly compressed.

### 6.3 Differences in Surrogate Objective Estimation

**Trajectory-level mixing**: Advantage function reduces to $A^{\pi^{(i)}}(s,a)$, with a clear estimation path.

**Advantage substitution bias in step/segment-level mixing**: If single-policy advantage estimation is used, systematic bias will result. This is because $A^{\beta^{(k)}}((s,i),a)$ needs to take expectation over future index switches, while $A^{\pi^{(i)}}(s,a)$ implicitly assumes "future always follows $\pi^{(i)}$".

**Unification under Bandit setting**: In single-step episode LLM training, there are no subsequent state transitions, and the estimation problems of both mechanisms unify without the above bias.

### 6.4 Variance Amplification Risk

Step/segment-level mixing also has a hidden danger: even if single-step importance ratios are clipped, multi-step noise accumulation over long trajectories can still amplify gradient estimation variance. When policy changes per update are large, within-trajectory "behavior jumps" may cause heavier-tailed ratio distributions. This is also why the table below recommends trajectory-level mixing for "large policy change per update" scenarios.

### 6.5 Applicable Scenarios

**Table 6.1: Applicable Scenarios for Two Mixing Mechanisms**

| Scenario Characteristics                                     | Recommended Mechanism | Reason                                     |
| :----------------------------------------------------------- | :-------------------- | :----------------------------------------- |
| Long trajectories, high-frequency updates, strong asynchrony | Step/segment-level    | Can significantly compress $S_k$           |
| Short trajectories (non-Bandit)                              | Trajectory-level      | $S_k$ is naturally low                     |
| Large policy change per update                               | Trajectory-level      | Avoid variance amplification               |
| Single-step episode (Bandit)                                 | Either                | Choose based on implementation convenience |
| Need compromise                                              | Segment-level         | Switch at natural boundaries               |

**Core tradeoff**: Step/segment-level mixing is stronger on the sampling side (fast staleness removal), while trajectory-level mixing is more stable on the estimation side (surrogate objective easier to estimate).


## Part Seven: Handling Training-Inference Inconsistency

### 7.1 Problem Background

In large-scale distributed training, policies on the inference and training sides may be inconsistent:

- **Numerical implementation differences**: softmax normalization, quantization, kernel fusion
- **Decoding rule differences**: temperature scaling, top-p/top-k sampling

Let the behavior policy modeled on the training side be $\pi^{(i)}$, while the policy actually sampling on the inference side is $\hat{\pi}^{(i)}$.

### 7.2 Effective Staleness

Define **effective staleness**:

$$
\hat{S}_k := \mathbb{E}_{(s,i) \sim d_{\hat{\beta}^{(k)}}} \big[ D_{\mathrm{TV}}(\pi_k, \hat{\pi}^{(i)}; s) \big]
$$

This definition covers both version staleness and training-inference implementation differences.

### 7.3 Actionable Control

By Lemma 3.3, $\hat{S}_k$ can be expressed in sample-level computable form. Given threshold $\epsilon_{\mathrm{stale}}$, if training only uses samples satisfying $|\pi_k(a|s)/\hat{\pi}^{(i)}(a|s) - 1| \leq \epsilon_{\mathrm{stale}}$, then $\hat{S}_k \leq \epsilon_{\mathrm{stale}}/2$.

**Key implementation points**:

1. **Behavior denominator alignment**: The behavior probability in the loss should use the inference-side recorded $\hat{\pi}^{(i)}(a|s)$
2. **Probability smoothing**: If the inference side has truncation (e.g., top-k), ensure ratios are valid


## Summary: Practical Guidelines

### Core Theoretical Framework

The structure of the monotonic improvement lower bound is:

$$
J(\pi_{k+1}) - J(\pi_k) \geq \underbrace{L_{\beta^{(k)}}(\pi_{k+1})}_{\text{Surrogate objective}} - \underbrace{C_1 \cdot U_k}_{\text{Update drift penalty}} - \underbrace{C_2 \cdot S_k}_{\text{Sampling staleness penalty}}
$$

### Separation of Concerns Principle

| Control Term | Responsible Party      | Control Method  | Specific Operation                |
| :----------- | :--------------------- | :-------------- | :-------------------------------- |
| $U_k$        | Optimization algorithm | Policy clipping | Clip $\pi_{k+1}/\pi_k$            |
| $S_k$        | Sampling system        | Data filtering  | Discard stale samples             |
| $S_k$        | Sampling system        | Version window  | Only use most recent $W$ versions |

### Clipping Method Selection

| Scenario                           | Recommended Method       | Reason                                              |
| :--------------------------------- | :----------------------- | :-------------------------------------------------- |
| High staleness                     | Method One (Adaptive)    | Automatically tightens constraint for stale samples |
| Implementation simplicity priority | Method Two (Incremental) | No need to store old policy information             |
| LLM large vocabulary               | Method One               | Avoid slow updates for low-probability tokens       |

### Key Parameters

- **Clipping radius $\epsilon$**: Controls $U_k$, typical values 0.1–0.2
- **Staleness threshold $\epsilon_{\mathrm{stale}}$**: Controls $S_k$, typical values 0.3–0.5; can be relaxed to 0.8–1.0 for strongly asynchronous systems
- **Version window $W$**: Limits number of policy versions in mixture sampling, typical values 2–4

### Training-Inference Inconsistency Handling

- Use inference-side recorded $\hat{\pi}^{(i)}$ as behavior denominator
- Compress effective staleness through sample filtering
- Apply probability smoothing for truncated decoding rules like top-k


## Appendix: Key Symbol Quick Reference

| Symbol                                            | Meaning                                                      |
| :------------------------------------------------ | :----------------------------------------------------------- |
| $\pi_k$, $\pi^{(i)}$                              | Latest policy at round $k$, $i$-th old policy                |
| $d_\pi(s)$, $A^\pi(s,a)$                          | Discounted state visitation distribution, advantage function |
| $D_{\mathrm{TV}}(\pi, \pi'; s)$                   | TV distance between two policies at state $s$                |
| $\beta^{(k)}(a \mid s, i) := \pi^{(i)}(a \mid s)$ | Mixture behavior policy at round $k$                         |
| $q(i' \mid i)$, $\alpha_i^{(k)}$                  | Index transition kernel, initial index distribution          |
| $U_k$, $S_k$                                      | Update increment drift, sampling staleness                   |
| $\epsilon$, $\epsilon_{\mathrm{stale}}$, $W$      | Clipping radius, staleness threshold, version window         |
| $C_{\pi,\pi_k}$                                   | Expected advantage upper bound constant                      |


## References

1. John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. "Trust Region Policy Optimization" (TRPO). arXiv:1502.05477. <https://arxiv.org/abs/1502.05477>

2. Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel. "Constrained Policy Optimization" (CPO). arXiv:1705.10528. <https://arxiv.org/abs/1705.10528>

3. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. "Proximal Policy Optimization Algorithms" (PPO). arXiv:1707.06347. <https://arxiv.org/abs/1707.06347>

4. James Queeney, Ioannis Ch. Paschalidis, Christos G. Cassandras. "Generalized Proximal Policy Optimization with Sample Reuse" (GePPO). arXiv:2111.00072. <https://arxiv.org/abs/2111.00072>

5. Yuzhen Zhou, Jiajun Li, Yusheng Su, et al. "APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation" (APRIL; partial rollout). arXiv:2509.18521. <https://arxiv.org/abs/2509.18521>

6. Jacob Hilton, Karl Cobbe, John Schulman. "Batch size-invariance for policy optimization" (Decoupled PPO). arXiv:2110.00641. <https://arxiv.org/abs/2110.00641>


```bibtex
@misc{WangZhang2025OffPolicyLLMRL,
	author       = {Wang, Xihuai and Zhang, Shao},
	title        = {Off-Policy Training in LLM Reinforcement Learning: From Theory to Practice},
	year         = {2025},
	month        = dec,
	day          = {17},
	url          = {https://xihuai18.github.io/reinforcement-learning/2025/12/17/offpolicy-en.html},
	urldate      = {2025-12-17}
}
